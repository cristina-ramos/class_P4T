{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Py4Text_Lab4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkj5P4Ja415w"
      },
      "source": [
        "# Dictionaries and Tuples - Lab 4, Python for Text, Fall 2020\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "To be completed and submitted by end of the day on Monday, September 28.\n",
        "\n",
        "The questions you need to answer are marked with **QUESTION**. For each one, there's a space (under **ANSWER**) for you to add your answer, which might be text, might be code, or might be a mix of the two. \n",
        "\n",
        "\n",
        "\n",
        "### Have questions?\n",
        "\n",
        "1. Use the Canvas discussion boards.\n",
        "\n",
        "## Reminder\n",
        "\n",
        "You will need to submit your own notebook (or rather, a link to your own notebook). There are (at least) two ways to do this:\n",
        "\n",
        "1. Make a copy of this notebook, rename it, and add new code cells when you want to write your own code.\n",
        "1. Create a new Python 3 notebook using the `File` menu and type in all cells yourself. \n",
        "\n",
        "Either way, your file should be named using this format: `LastName_Py4Text_Lab4.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRphYoQ7jHw_"
      },
      "source": [
        "# **Overview: Analyzing Text with Word Frequencies**\n",
        "\n",
        "This week we will look at using word frequencies to analyze text. Frequency analysis is a cornerstone of text analytics, across fields from literary analysis to forensic linguistics to market research. For example, methods such as topic modeling, word embeddings, and sentiment analysis incorporate word frequencies as essential components. In addition, text analysis programs (e.g. tools like nVivo, MaxQDA, or others) generally feature word frequency analysis. This week you will learn how to use Python to code your own frequency analysis system. Building your own system of course takes more time than simply running an existing tool, but it comes with the great advantage of being able to customize your system to do exactly what you want it to do.\n",
        "\n",
        "Along the way to building this system, you'll get practice working with two new **data structures** in python: dictionaries and tuples. You'll also start working with literary data from Project Gutenberg.\n",
        "\n",
        "Things you should be able to do at the end of this lab:\n",
        "\n",
        "* Read in literary texts (from the Gutenberg Corpus) from the NLTK, using the corpus reader methods introduced in Lab 3\n",
        "* Download additional texts from the Project Gutenberg website and read those texts into Python\n",
        "* Understand the difference between **types** and **tokens** and what it means to count each of these\n",
        "* Understand why **case normalization** is important\n",
        "* Understand and be able to use two new data structures: **dictionaries** and **tuples**\n",
        "* Learn how to use NLTK's `FreqDist` class to do frequency analysis of texts\n",
        "* Print results to a file using the `print` function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wwr2k4UCsql"
      },
      "source": [
        "\n",
        "# 0. Preliminaries\n",
        "\n",
        "To get started, import the NLTK and download the book data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db7CwgVwC28x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe01027c-0bd9-413e-c26d-7c07c5849e95"
      },
      "source": [
        "import nltk\n",
        "nltk.download('book')  ## the 'book' download includes some texts from Project Gutenberg \n",
        "from nltk.book import *\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKuduFps5K-D"
      },
      "source": [
        "# 1. Literature as text: Project Gutenberg\n",
        "\n",
        "With this week's lab, we move into the use of literature as text. Our focus is on any works of fiction. Some people have strong opinions about what counts as *literature* - you can decide for yourselves whether you are one of those people. Despite having a B.A. in English literature, I am *not* one of those people - for the purposes of this class, we'll consider just about any fictional texts as relevant.\n",
        "\n",
        "So why do we care about analyzing fictional texts? The answer to this depends on the point of view of the analyst. Some people study literature for literary purposes. Some studies have used text analysis for **authorship attribution** - determining the author(s) of previously anonymous texts. This type of analysis may also take place in forensic linguistics. One thing is certain - fictional texts have different properties than news texts. Imagine that you're given a randomly-selected paragraph of text, one with at least 5-6 sentences. Chances are quite high that you will be able to accurately determine whether that paragraph comes from news text or fiction. How do we make this determination? \n",
        "\n",
        "Let's try it. I've nearly-randomly selected paragraphs from two different texts, one from the news genre and one from the literary genre.\n",
        "\n",
        "```\n",
        "Paragraph One:\n",
        "Children have the strangest adventures without being troubled by them.\n",
        "For instance, they may remember to mention, a week after the event\n",
        "happened, that when they were in the wood they had met their dead\n",
        "father and had a game with him. It was in this casual way that Wendy one\n",
        "morning made a disquieting revelation. Some leaves of a tree had been\n",
        "found on the nursery floor, which certainly were not there when the\n",
        "children went to bed, and Mrs. Darling was puzzling over them when Wendy\n",
        "said with a tolerant smile:\n",
        "\n",
        "“I do believe it is that Peter again!”\n",
        "```\n",
        "\n",
        "```\n",
        "Paragraph Two:\n",
        "Over the past two months, orcas have damaged about a dozen pleasure boats \n",
        "off the Iberian Peninsula from the Strait of Gibraltar to the coast of Galicia,\n",
        "the most northerly point in Spain, baffling marine biologists and sailors.\n",
        "Although there have been no reports of injuries — at least for humans — \n",
        "scientists and the Spanish authorities have struggled to interpret the \n",
        "interactions.\n",
        "Were they attacks? Or just friendly encounters from a highly playful mammal \n",
        "that went a little too far?\n",
        "```\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> a. Which paragraph is news, and which is fiction?\n",
        "\n",
        "> b. For each genre, describe at least 2-3 cues that helped you recognize the text type of the paragraph. Feel free to look at other examples of texts from these genres if you feel it would be helpful.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> I think paragraph one is fiction and paragraph two is a news text - paragraph two reads more informative while the first includes characteristics typical of a novel or work of fiction. The quotation marks around dialogue is a good clue for fiction although some news articles can introduce quotes in this manner as well - the use of descriptive storytelling through adjective and proper nouns is another clue for fiction. The news text reads strictly informative by first introducing the subject matter being reported on - orcas - then follows a brief background description on location, and the issue being reported on (boat damages). The presence of rhetorical questions can also be seen as a typical presence in most news articles as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HSsZtQLzOrO"
      },
      "source": [
        "Different text types use language differently, and this is important not only for manual analysis but also for automated analysis. For example, many tools for automated analysis of language are trained on large amounts of data - some examples are automatic part-of-speech taggers, text generation systems, and machine translation systems. Systems that are trained on only news data might do an excellent job of labeling other news data, but their performance gets noticeably worse when they are applied to text from other genres.\n",
        "\n",
        "In machine learning, we talk about this as a matter of **domain shift** or **domain effects**, and many researchers have worked on **domain adaptation** methods to address the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-TyM6iItPKR"
      },
      "source": [
        "For this lab, we will stay within the domain of fiction. One problem that arises with fiction is the question of copyright, since so many fictional works are sold for profit, and are not considered free for re-distribution in the way that many (but not all) news texts are. \n",
        "\n",
        "Enter **Project Gutenberg**. Project Gutenberg is a large online collection of free ebooks. Project Gutenberg focuses on texts that are no longer subject to copyright protection, which means there's a lot of old literature available.\n",
        "The [Project Gutenberg website](https://www.gutenberg.org/) hosts more than 60,000 freely-available books, in more than 60 different languages (mostly in English, though). We'll do more with the website in the next chunk of this lab. For now, we'll look at the Project Gutenberg texts available in the NLTK. (If you'd like to see more examples of how to use the `gutenberg` corpus within NLTK, read section 1.1 of Chapter 2 of the NLTK book: http://www.nltk.org/book/ch02.html.)\n",
        "\n",
        "Remember the corpus readers introduced in the previous lab? We'll make use of these here too. As a refresher:\n",
        "\n",
        "* `words()` -- returns the corpus text as a list of words\n",
        "* `sents()` -- returns the corpus text as a list of sentences. Sometimes each sentence is in the form of a list of words, and sometimes each sentence is a string.\n",
        "* `paras()` -- returns the corpus text as a list of paragraphs\n",
        "* `raw()` -- returns the corpus text as one long string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU7ZXS6cFJcK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "fc1b3afa-d613-4f22-9276-40de7fcf0179"
      },
      "source": [
        "### import the corpus directly\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "### see a list of fileids\n",
        "for text in gutenberg.fileids():\n",
        "    print(text)\n",
        "\n",
        "### how many texts?\n",
        "print(len(gutenberg.fileids()))\n",
        "\n",
        "### store the data in different ways, using the built-in corpus readers\n",
        "guten_words = gutenberg.words() \n",
        "guten_sents = gutenberg.sents()\n",
        "guten_paras = gutenberg.paras()\n",
        "guten_raw = gutenberg.raw()\n",
        "\n",
        "### how many words in these texts?\n",
        "print(len(guten_words))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "austen-emma.txt\n",
            "austen-persuasion.txt\n",
            "austen-sense.txt\n",
            "bible-kjv.txt\n",
            "blake-poems.txt\n",
            "bryant-stories.txt\n",
            "burgess-busterbrown.txt\n",
            "carroll-alice.txt\n",
            "chesterton-ball.txt\n",
            "chesterton-brown.txt\n",
            "chesterton-thursday.txt\n",
            "edgeworth-parents.txt\n",
            "melville-moby_dick.txt\n",
            "milton-paradise.txt\n",
            "shakespeare-caesar.txt\n",
            "shakespeare-hamlet.txt\n",
            "shakespeare-macbeth.txt\n",
            "whitman-leaves.txt\n",
            "18\n",
            "2621613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6ZECgr1sqC"
      },
      "source": [
        "\n",
        "Now we'll choose one text to work with. I'm going to use \"Alice in Wonderland\" - choose one of your own and try out each of the following steps on your own text. (Show these steps in new code blocks.)\n",
        "\n",
        "We'll first store different versions of the text by using the different corpus readers. Notice that I've created a variable to store the fileid for \"Alice in Wonderland\" - this helps our code in several ways:\n",
        "\n",
        "* it saves us having to type the filename over and over again\n",
        "* it reduces the likelihood of typos (and makes them easier to fix)\n",
        "* it makes the code easily reusable - we can just change the value stored to the variable and run the code again for a different text (instead of writing new code to do the same operations for our new text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZGQzaOR3p-I"
      },
      "source": [
        "### variable for the fileid\n",
        "alice = 'carroll-alice.txt'\n",
        "\n",
        "### different versions of the text\n",
        "alice_raw = gutenberg.raw(alice)\n",
        "alice_words = gutenberg.words(alice)\n",
        "alice_sents = gutenberg.sents(alice)\n",
        "alice_paras = gutenberg.paras(alice)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfvAGlPjrqel"
      },
      "source": [
        "macbeth = 'shakespeare-macbeth.txt'\n",
        "macbeth_raw = gutenberg.raw(macbeth)\n",
        "macbeth_words = gutenberg.words(macbeth)\n",
        "macbeth_sents = gutenberg.sents(macbeth)\n",
        "macbeth_paras = gutenberg.paras(macbeth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66o67usi3pIv"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Review of data types: For each of the variables listed below, look at the first one or two elements. Based on what you see, how is the data structured for that variable? For example, if I look at the first five elements of `alice_words`, using a slice (`alice_words[:5]`), I get the following output: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A44PZv1wvNS8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d81a7d7-d76e-4567-aaa9-3c2e98c7e4a2"
      },
      "source": [
        "print(alice_words[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[', 'Alice', \"'\", 's', 'Adventures']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIGmK_6asIg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0286e7e2-1590-4649-8e2f-56fd85225cec"
      },
      "source": [
        "print(alice_raw[:45])\n",
        "print(alice_sents[:5])\n",
        "print(alice_paras[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Alice's Adventures in Wonderland by Lewis Ca\n",
            "[['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']'], ['CHAPTER', 'I', '.'], ['Down', 'the', 'Rabbit', '-', 'Hole'], ['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\"], ['So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', '),', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisy', '-', 'chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.']]\n",
            "[[['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']']], [['CHAPTER', 'I', '.'], ['Down', 'the', 'Rabbit', '-', 'Hole']], [['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\"]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDTdrkZPvn41"
      },
      "source": [
        "> We can see by the fact that the output starts and ends with a square bracket that it's a **list**, and see that each element in the list is a **string**, so `alice_words` is a list of strings. (We also see that mostly the strings are words, though broken apart in some ways - we'll talk more about this process later in the semester.)\n",
        "\n",
        "> Your task: how is the data structured for:\n",
        "\n",
        "* a. `alice_raw`\n",
        "* b. `alice_sents`\n",
        "* c. `alice_paras`\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> a. 'alice_raw' returns the full text in the corpora in a list, this isn't tokenized at all\n",
        "\n",
        ">b. 'alice_sents' - a nested tokenized list with one sentence per [] list. \n",
        "\n",
        ">c. 'alice_paras' - similar to the sents function except the outpus is a list of the text section off by their paragraphs.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ikDji9pYR-"
      },
      "source": [
        "# 2. Counting words, and building your own corpus in NLTK\n",
        "\n",
        "\n",
        "## **Counting words in a corpus**\n",
        "\n",
        "> *How many words are in my corpus?*\n",
        "\n",
        "Although it seems like a simple question on the surface, there are many different ways to answer this question. In text analysis, we often distinguish between the **type** count and the **token** count.\n",
        "\n",
        "Types are distinct words - so *cat* and *lion* are different word types. When we count types, we're counting how many different words appear in a corpus. \n",
        "\n",
        "Tokens are individual occurrences of words, so in a sentence like *Cats like to chase cats* we would count 4 types and 5 tokens.\n",
        "\n",
        "**READING:** Read sections 2.2 and 2.3 of chapter 2 in Speech and Language Processing (3rd edition), available here: https://web.stanford.edu/~jurafsky/slp3/2.pdf\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> In your own words, what is a *lemma*? To illustrate, choose a lemma in English. Write the lemma and at least 3 other forms of that lemma.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> A lemma is a unit of meaning which can index similar forms: so run is a lemma of (or indexes) run, running, runs.\n",
        "\n",
        "-----\n",
        "\n",
        "Once we have a list of all the words in a text, it's straightforward to get both the type count and the token count. For the token count, we simply compute the length of the list of words, keeping in mind that, for now at least, that list also include punctuation and other characters we may or may not want to include in the token count.\n",
        "\n",
        "To get the type count, we can use a python function called `set()`. In fact, the set is yet another data structure in python. Just as in set theory, a set in python is a group (or set) of elements with no duplicates. When we run a command like the following:\n",
        "\n",
        "```\n",
        "alice_vocab = set(alice_words)\n",
        "```\n",
        "\n",
        "we are producing a new variable - a set of the unique word types (and punctuation types) in \"Alice and Wonderland\". We can again use the `len()` function to count the elements in the set.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKs2Zl_00r17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5741399c-73f9-4460-baf2-ce211b62b88c"
      },
      "source": [
        "### first let's create the set of word types\n",
        "alice_vocab = set(alice_words)\n",
        "\n",
        "### now we can get the counts\n",
        "alice_token_count = len(alice_words)\n",
        "alice_type_count = len(alice_vocab)\n",
        "print(\"Tokens in AiW:\", alice_token_count)\n",
        "print(\"Types in AiW:\", alice_type_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens in AiW: 34110\n",
            "Types in AiW: 3016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TaPb9Ab0quv"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> What are the token and type counts for the Gutenberg text you selected? Include the code you use to compute these figures.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> The token counts for Macbeth is 23140 and 4017 for the type count\n",
        "\n",
        ">macbeth_vocab = set(macbeth_words)\n",
        "macbeth_tks = len(macbeth_words)\n",
        "macbeth_typ = len(macbeth_vocab)\n",
        "print(\"Tokens in Macbeth: \", macbeth_tks)\n",
        "print(\"Types in Macbeth: \", macbeth_typ)\n",
        "\n",
        ">Tokens in Macbeth:  23140\n",
        "Types in Macbeth:  4017\n",
        "\n",
        "-----\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP4WfW-1Czni",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9e7da0e8-40bf-44aa-ba37-3e23c0feee71"
      },
      "source": [
        "macbeth_vocab = set(macbeth_words)\n",
        "macbeth_tks = len(macbeth_words)\n",
        "macbeth_typ = len(macbeth_vocab)\n",
        "print(\"Tokens in Macbeth: \", macbeth_tks)\n",
        "print(\"Types in Macbeth: \", macbeth_typ)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens in Macbeth:  23140\n",
            "Types in Macbeth:  4017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNPRhSpX1wAN"
      },
      "source": [
        "The relationship between the type count and the token count tells us how varied the vocabulary of a text is - this measure is often referred to as **lexical diversity** or **lexical richness**. \n",
        "\n",
        "You can read more about this, and see ways of computing this measure, in section 1.4 of Chapter 1 of the NLTK book: http://www.nltk.org/book/ch01.html\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> What is the lexical diversity score for your selected text? Include the code you use to compute these figures.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> The lexical diversity for Macbeth is around 60% - the number of distinct words in 60% of the total words\n",
        "\n",
        ">def lexical_diversity(text):\n",
        "  return(len(set(text))) / len(text)\n",
        "lexical_diversity(macbeth)\n",
        "\n",
        "-----\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmlfxVX2EMnv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bb3d647-488a-4587-ba1f-85d446db93c6"
      },
      "source": [
        "def lexical_diversity(text):\n",
        "  return(len(set(text))) / len(text)\n",
        "lexical_diversity(macbeth)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6086956521739131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HDcLcx010Dj"
      },
      "source": [
        "Finally, when we perform these counts, we need to decide how we want to handle case. Specifically, do we want to have separate counts for the upper- and lower-case forms of a word? Do we want to count *Cat* separately from *cat*? Usually, the answer to that question is NO, and this is where **case normalization** comes into play. \n",
        "\n",
        "Case normalization simply means that we flatten out the case distinctions in a corpus or a text. There are more and less complicated ways to perform case normalization, and the right one to choose depends on what we want to do with the data. If we're interested in finding company names in texts, for example, we probably want to keep words that start with a capital letter, at least when they occur somewhere other than at the start of a sentence. The most typical strategy is to convert every word to all lower case. \n",
        "\n",
        "We can do this using python string methods and a list comprehension.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6U0FPWG3f05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "199ff8b7-d249-4b7f-982e-7de7967c2f45"
      },
      "source": [
        "### list comprehension: applies lower() to every word in alice_words\n",
        "alice_words_lower = [w.lower() for w in alice_words]\n",
        "\n",
        "### token count hasn't changed\n",
        "### but the casing has\n",
        "print(len(alice_words_lower))\n",
        "print(alice_words_lower[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34110\n",
            "['[', 'alice', \"'\", 's', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGFji0pv32zb"
      },
      "source": [
        "Notice that this is a slightly different way to use list comprehensions. In Lab 3, we used list comprehensions to apply conditions to lists and create new lists with subsets of the previous lists. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JezwxX7q4Blq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "07236a4a-34dc-40f1-ef37-c6a976c273e7"
      },
      "source": [
        "### make a list of all words in Alice that start with 'a'\n",
        "alice_awords = [w for w in alice_words if w.startswith('a') or w.startswith('A')]\n",
        "\n",
        "print(len(alice_awords))\n",
        "print(alice_awords[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3402\n",
            "['Alice', 'Adventures', 'Alice', 'and', 'and', 'a', 'Alice', 'as', 'as', 'and']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-26cFfb4Tep"
      },
      "source": [
        "Now, instead of applying a condition, we're applying a string method to *every* word in the text. We can also combine the two (transforming the list items and applying a condition)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iEwVeuI4gTq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f2da6f95-6853-437b-af72-7b2c362acac8"
      },
      "source": [
        "### lower-case only words that begin with 'a'\n",
        "new_alice = [w.lower() for w in alice_words if w.startswith('A')]\n",
        "\n",
        "print(len(new_alice))\n",
        "print(new_alice[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "566\n",
            "['alice', 'adventures', 'alice', 'alice', 'alice', 'a', 'alice', 'alice', 'alice', 'alice']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h89BKvBd4-ny"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Create a case-normalized version of your text. Print out the first 10 words of the original version and the first 10 words of the case-normalized version. Include the code you used to do this.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> new_macbeth = [w.lower() for w in macbeth_words if w.startswith('M')]\n",
        "print(len(macbeth_words))\n",
        "print(macbeth_words[:10])\n",
        "print(len(new_macbeth))\n",
        "print(new_macbeth[:10])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ07iIwsFYfx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c710e96f-ad2a-4f47-c75f-a21cad6aa883"
      },
      "source": [
        "new_macbeth = [w.lower() for w in macbeth_words if w.startswith('M')]\n",
        "print(len(macbeth_words))\n",
        "print(macbeth_words[:10])\n",
        "print(len(new_macbeth))\n",
        "print(new_macbeth[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23140\n",
            "['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']']\n",
            "583\n",
            "['macbeth', 'macbeth', 'malkin', 'malcome', 'mal', 'macdonwald', 'macbeth', 'minion', 'marke', 'macbeth']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHiaxCVX3bKX"
      },
      "source": [
        "\n",
        "## **Building your own corpus in NLTK**\n",
        "\n",
        "Now we're going to see how to build our own corpus within NLTK. In the next lab we'll learn how to import our own texts into Python without using NLTK as an intermediary. Before proceeding, please read sections 1.8 and 1.9 in Chapter 2 of the NLTK book: http://www.nltk.org/book/ch02.html \n",
        "\n",
        "NLTK\tgives\tus\ta\tgeneric\tcorpus\treader\twhich\tcan\tread\tin\ta\tcollection\tof\ttext\tfiles\tand\tlet\tyou\twork\twith\tthem\tusing\tNLTK’s\tmany\ttext\tanalysis\tfunctions. The\tfunction\twe\twill\tuse\tis\tNLTK’s\t`PlaintextCorpusReader`, and for today we will download texts from the [Project Gutenberg website](https://www.gutenberg.org/).\n",
        "\n",
        "1. We'll be building our corpus within Colab - this does mean you'll need to upload your texts each time you sign in from a new location, unless you create your corpus within Google Drive and mount your Drive. For now, though, here's the simplest way. Create a directory by clicking the folder icon to the left of your screen, right-clicking, and picking \"New folder\". Give it a name that has to do with your corpus, and no spaces in the name of the folder. I've named mine `corpus`. \n",
        "\n",
        "1. Using Gutenberg's search and/or browsing functions, choose at least three texts to add to your corpus. Download the version of the text that comes in `.txt` format - this is the `Plain text UTF-8` version. Open the plain text version, then right-click and save on your computer. (You can use text files from other sources too.)\n",
        "\n",
        "1. Next, upload the text files to your new `corpus` directory on Colab. I've selected *Three Lives* by Gertrude Stein, *The Scarlet Letter* by Nathaniel Hawthorne, and *Jack the Giant Killer* by Percival Leigh.\n",
        "  \n",
        "1. Now\twe\tare\tready\tto\twork\twith\tour\ttexts\tin\tPython.\tFirst\twe\tneed\tto\timport\tthe\t`PlaintextCorpusReader`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIbyGsIS8app"
      },
      "source": [
        "from nltk.corpus import PlaintextCorpusReader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2BuxutL8eN4"
      },
      "source": [
        "5. The next step is to tell Python the name of the directory in which our text files are stored. This is what’s known as the **root directory** for the corpus. I’m showing the path for my root directory. In Colab, this should be simple: just the name of the folder that you have uploaded your text files to, followed by a forward slash."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbW-8ie58qKs"
      },
      "source": [
        "corpus_root = r\"corpus/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5nItZ6m968l"
      },
      "source": [
        "6. Now we will call the function `PlaintextCorpusReader`. This function has two arguments – (a) the root directory for the corpus; and (b) a list containing the names of the files we want to read into Python (each file name should occur as a string). I'm storing the output of the function to a variable which is essentially the name I've given to my corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP4Rt4jO-AVv"
      },
      "source": [
        "funtexts1 = PlaintextCorpusReader(corpus_root, ['jack.txt', 'scarlet.txt', 'threeLives.txt'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVw5DEHS-RPw"
      },
      "source": [
        "7. Of course, we may not always want to list every file name in the corpus root directory. Instead of explicitly providing a list of filenames, another option is to give a pattern as the second argument. So if I want to add every file that ends in `txt` to my corpus, I can do the following instead. The pattern here uses regular expression language. (NOTE:\tin\tthis\tcase,\t`funtexts1` and `funtexts2` are\ttwo\tlists\twith\tthe\tsame\tcontent):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9B12LlG-o6G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9669bef1-6da2-45e5-8c8c-bc661b6982e3"
      },
      "source": [
        "funtexts2 = PlaintextCorpusReader(corpus_root, '.*txt')\n",
        "print(funtexts2.fileids())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dracula.txt', 'frankie.txt', 'jekyll.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YdfSYMy_BmY"
      },
      "source": [
        "We can use `fileids()` to see a list of the file names in our newly-created corpus.\n",
        "\n",
        "Now try this with your own texts. From this point on, fill in the Python commands with words and variables relevant to your new corpus.\n",
        "\n",
        "We can now see the list of file IDs in our corpus, and we can look at the word lists for individual files, using the `fileids` and `words` methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzF-awE3_R-_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "784bbffa-e197-4923-b20b-8dc71d500d75"
      },
      "source": [
        "fun_words = funtexts2.words()\n",
        "print(len(fun_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "320241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIR0y2uy_Wqs"
      },
      "source": [
        "We can also pull the words for just one text at a time, using the same approach that we have used for NLTK's small chunk of the `gutenberg` corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGfi1poj_dwz"
      },
      "source": [
        "scarlet_words = funtexts2.words(fileids='scarlet.txt')\n",
        "print(len(scarlet_words))\n",
        "print(scarlet_words[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d-f0TtG_9nY"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Now comes the fun of working with your own texts! For the corpus that you created, please answer each of the following questions. Include the code you used to arrive at the answers.\n",
        "\n",
        "* a. What are the titles and authors of the texts you selected?\n",
        "* b. How many words are in each text? Give both the type count and the token count.\n",
        "* c. What is the lexical diversity score for each text?\n",
        "\n",
        "> Next, take a look at these results. Are there any surprises? What, if anything, can you learn from comparing the lexical diversity scores for the different texts?\n",
        "\n",
        "> Can you see any problems with doing the analysis in this way? What simplifications or assumptions are we making that might need lead to a good analysis?\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> a. The texts I've chosen are: Dracula by Bram Stoker, The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson and Frankenstein; Or The Modern Prometheus by Mary Shelley.\n",
        "\n",
        "> b. Total words in the corpus: 320241\n",
        "Total words in Dracula: 196321\n",
        "Total words in Frankenstein: 89381\n",
        "Total words in Dr. Jekyll and Mr. Hyde: 34539\n",
        "Total type counts in Dracula: 10727\n",
        "Total type counts in Frankenstein: 7906\n",
        "Total type counts in Dr. Jekyll and Mr. Hyde: 4722\n",
        "\n",
        "> c. Lexical diversity score for Dracula: 0.054640104726442915 Lexical diversity score for Frankenstein: 0.08845280316845862\n",
        "Lexical diversity score for Dr. Jekyll and Mr. Hyde: 0.1367150178059585\n",
        "\n",
        "> No real surprises here just some questions on the lexical diversity scores and which function to use that would be iterable with the PlaintextCorpusReader. I think there's probably a simpler way to calculate the lexical diversity score that doesn't feel so clunky "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d7DpCGxH3zO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d769ea60-0ea2-491f-8035-bfb630d8d147"
      },
      "source": [
        "from nltk.corpus import PlaintextCorpusReader\n",
        "corpus_root = r\"/content/corpus\"\n",
        "spooky_txts = PlaintextCorpusReader(corpus_root, '.*txt')\n",
        "print(spooky_txts.fileids())\n",
        "total_words = len(spooky_txts.words())\n",
        "print(\"Total words in the corpus:\", total_words)\n",
        "\n",
        "dracula_words = spooky_txts.words(fileids='dracula.txt')\n",
        "frankie_words = spooky_txts.words(fileids='frankie.txt')\n",
        "jekyll_words = spooky_txts.words(fileids='jekyll.txt')\n",
        "print(\"Total words in Dracula:\", len(dracula_words))\n",
        "print(\"Total words in Frankenstein:\", len(frankie_words))\n",
        "print(\"Total words in Dr. Jekyll and Mr. Hyde:\", len(jekyll_words))\n",
        "\n",
        "dracula_vocab = set(dracula_words)\n",
        "frankie_vocab = set(frankie_words)\n",
        "jekyll_vocab = set(jekyll_words)\n",
        "dracula_type = len(dracula_vocab)\n",
        "frankie_type = len(frankie_vocab)\n",
        "jekyll_type = len(jekyll_vocab)\n",
        "print(\"Total type counts in Dracula:\", dracula_type)\n",
        "print(\"Total type counts in Frankenstein:\", frankie_type)\n",
        "print(\"Total type counts in Dr. Jekyll and Mr. Hyde:\", jekyll_type)\n",
        "\n",
        "dracula = PlaintextCorpusReader(corpus_root, fileids='dracula.txt')\n",
        "frankie = PlaintextCorpusReader(corpus_root, fileids='frankie.txt')\n",
        "jekyll = PlaintextCorpusReader(corpus_root, fileids='jekyll.txt')\n",
        "#tried it this way first: lexical_diversity(dracula) but got a TypeError - PlaintextCorpusReader object is not iterable\n",
        "\n",
        "dracula_div = len(set(dracula.words())) / len(dracula.words())\n",
        "frankie_div = len(set(frankie.words())) / len(frankie.words())\n",
        "jekyll_div = len(set(jekyll.words())) / len(jekyll.words())\n",
        "#not sure if .raw() is the correct function for this...getting really low numbers, going to try .words() instead\n",
        "print(\"Lexical diversity score for Dracula:\", dracula_div)\n",
        "print(\"Lexical diversity score for Frankenstein:\", frankie_div)\n",
        "print(\"Lexical diversity score for Dr. Jekyll and Mr. Hyde:\", jekyll_div)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dracula.txt', 'frankie.txt', 'jekyll.txt']\n",
            "Total words in the corpus: 320241\n",
            "Total words in Dracula: 196321\n",
            "Total words in Frankenstein: 89381\n",
            "Total words in Dr. Jekyll and Mr. Hyde: 34539\n",
            "Total type counts in Dracula: 10727\n",
            "Total type counts in Frankenstein: 7906\n",
            "Total type counts in Dr. Jekyll and Mr. Hyde: 4722\n",
            "Lexical diversity score for Dracula: 0.054640104726442915\n",
            "Lexical diversity score for Frankenstein: 0.08845280316845862\n",
            "Lexical diversity score for Dr. Jekyll and Mr. Hyde: 0.1367150178059585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTAtqQ21IACB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABUMi_l0pift"
      },
      "source": [
        "# 3. Data structures: tuples and dictionaries\n",
        "\n",
        "So far we have worked primarily with the data structures of strings and lists, and the recently-introduced sets. Now we're going to look at some additional data structures in Python: **tuples** and **dictionaries**.\n",
        "\n",
        "**READINGS:**\n",
        "\n",
        "* tuples: Python Crash Course chapter 4, pages 69-74\n",
        "* dictionaries: Python Crash Course chapter 6, pages 95-115\n",
        "\n",
        "Now that you've read about our two new data structures, let's spend a little time working with them. \n",
        "\n",
        "## **Tuples**\n",
        "\n",
        "Tuples are very similar to lists, as both lists and tuples are ordered collections of values. Unlike lists, tuples are **immutable** - this means they (mostly) can't be changed once they've been established.\n",
        "\n",
        "Tuples usually are displayed between parentheses, but there are in fact several different ways to create a new tuple, as shown below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km9reuTnBzkz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3085317e-4b81-4e3e-92ee-f718b58796a9"
      },
      "source": [
        "# different ways to create a tuple\n",
        "empty_tuple = ()\n",
        "simple_tuple = 'a', 'b', 'c' \n",
        "the_same_simple_tuple = ('a', 'b', 'c')\n",
        "the_same_simple_tuple_again = tuple( ['a', 'b', 'c'] )\n",
        "print( simple_tuple, the_same_simple_tuple, the_same_simple_tuple_again )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('a', 'b', 'c') ('a', 'b', 'c') ('a', 'b', 'c')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZMZy0AiGI7O"
      },
      "source": [
        "Now let's look at a use for tuples in text analysis. We're going to use tuples to represent two-word sequences (aka **bigrams**) in our texts, and we'll build a list of those bigram tuples to collect all of the two-word sequences in the text. We'll do this by taking the following steps:\n",
        "\n",
        "1. Use a `for` loop to iterate through each word in the word list, up to the next-to-last word. We stop before the last word, because we are building two-word tuples, and there's no word coming after the last word.\n",
        "1. Instead of using the words to drive the `for` loop, we'll use a numerical sequence. This way, we can use the number as the index of the first word in the sequence, and the number + 1 as the index of the second word in the sequence. This is accomplished using `range()` - the `range` function is discussed in Python Crash Course, pages 61-63. \n",
        "1. For each word pair, we'll create a tuple and add that tuple to a list of all tuples.\n",
        "\n",
        "To illustrate, let's first do this for a short list of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnjyWR4VKdTJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3d365436-7575-40fd-c65a-762b965d7777"
      },
      "source": [
        "### create a short list of words\n",
        "mylist = ['the', 'cat', 'sat', 'on', 'the', 'mat', '.']\n",
        "\n",
        "### create an empty list to store our bigrams\n",
        "bigrams = []\n",
        "\n",
        "### iterate through the list of words,\n",
        "### creating a bigram tuple for each 2-word sequence,\n",
        "### and adding that tuple to the list of bigrams\n",
        "\n",
        "for i in range(len(mylist)-1):\n",
        "    first = mylist[i]\n",
        "    second = mylist[i+1]\n",
        "    bigram = (first, second)\n",
        "    print(bigram)\n",
        "    mylist.append(bigram)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('the', 'cat')\n",
            "('cat', 'sat')\n",
            "('sat', 'on')\n",
            "('on', 'the')\n",
            "('the', 'mat')\n",
            "('mat', '.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ECNxWBMLEEG"
      },
      "source": [
        "Now we can do this for a longer text, and using code that's a bit more compact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkHay87qLIgS"
      },
      "source": [
        "scarlet_bigrams = []\n",
        "\n",
        "### using The Scarlet Letter\n",
        "for i in range(len(scarlet_words)-1):\n",
        "    bigram = (scarlet_words[i], scarlet_words[i+1])  ### two word sequence\n",
        "    scarlet_bigrams.append(bigram)\n",
        "\n",
        "### how many bigrams?\n",
        "print(len(scarlet_bigrams))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR9mPW8fLoH9"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Build a list of bigrams for the gutenberg text you selected. Include your code. How many bigrams in your text?\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> There are 23130 bigrams in Macbeth, code shown below.\n",
        "\n",
        "-----\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psf8IlpvSlGK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6ea0b055-bd81-47b5-c1fb-a1d9f21f9907"
      },
      "source": [
        "macbeth_bigrams = []\n",
        "for i in range(len(macbeth_words)-1):\n",
        "  bgm = (macbeth_words[i], macbeth_words[i+1])\n",
        "  macbeth_bigrams.append(bgm)\n",
        "print(macbeth_bigrams[:10])\n",
        "len(macbeth_bigrams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('[', 'The'), ('The', 'Tragedie'), ('Tragedie', 'of'), ('of', 'Macbeth'), ('Macbeth', 'by'), ('by', 'William'), ('William', 'Shakespeare'), ('Shakespeare', '1603'), ('1603', ']'), (']', 'Actus')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23139"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM3KEwkcMkmq"
      },
      "source": [
        "\n",
        "## **Dictionaries**\n",
        "\n",
        "Now that you've read all about dictionaries, let's see how we can use them. A first use of dictionaries is to keep track of how often each word in a text appears. Remember that dictionaries let us map a **key** to a **value** - in this case, the word will be the key, and the number of times it occurs will be the value. We could build this dictionary by using the `count()` function over and over, but it's more efficient to do this by iterating over a text and adding one to the value for the word each time it occurs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVB3XoteMmjo"
      },
      "source": [
        "### First, initialize our dictionary\n",
        "mydict = {}\n",
        "\n",
        "### Next, we'll iterate through our text (as a list of words)\n",
        "### For each word, we'll first check whether it's already in the dictionary\n",
        "### If it is, we add one to the count for the word\n",
        "### If it's not, we add it as a new key to the dictionary, with a value of 1\n",
        "for w in scarlet_words:\n",
        "    if w in mydict:                    ## check whether w is already in the dict\n",
        "        mydict[w] = mydict[w] + 1      ## alternately: mydict[w] += 1\n",
        "    else:\n",
        "        mydict[w] = 1                  ## create a dictionary entry w/ value 1\n",
        "\n",
        "### Recall that we get the value for a dictionary key by \n",
        "### looking it up, as if it were an index\n",
        "\n",
        "print(\"The word 'scarlet' occurs\", mydict['scarlet'], \"times.\")\n",
        "print(\"The word 'letter' occurs\", mydict['letter'], \"times.\")\n",
        "\n",
        "### Compare to the counts we get using the count() function\n",
        "print(scarlet_words.count(\"scarlet\"))\n",
        "print(scarlet_words.count(\"letter\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8Ru6EvaNoHH"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Now you try it. Create a count dictionary for the text you selected. Choose several interesting words and display how often they appear. Compare these counts to the ones you get using `count()`. Are they the same? Include the code you use.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> The word 'honor' appears 2 times.\n",
        "The word 'blood' appears 15 times.\n",
        "The word 'Macbeth' appears 61 times.\n",
        "The word 'death' appears 9 times.\n",
        "I got the same results using the count function: 2, 15, 61, 9\n",
        "code shown below:\n",
        "\n",
        "-----\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZCklQ_JTdgB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "e7387312-2f9e-4547-b9dd-1edf62d05734"
      },
      "source": [
        "new_dict = {}\n",
        "for w in macbeth_words:\n",
        "  if w in new_dict:\n",
        "    new_dict[w] = new_dict[w] + 1\n",
        "  else:\n",
        "    new_dict[w] = 1\n",
        "\n",
        "print(\"The word 'honor' appears\", new_dict['honor'], \"times.\")\n",
        "print(\"The word 'blood' appears\", new_dict['blood'], \"times.\")\n",
        "print(\"The word 'Macbeth' appears\", new_dict['Macbeth'], \"times.\")\n",
        "print(\"The word 'death' appears\", new_dict['death'], \"times.\")\n",
        "print(macbeth_words.count('honor'))\n",
        "print(macbeth_words.count('blood'))\n",
        "print(macbeth_words.count('Macbeth'))\n",
        "print(macbeth_words.count('death'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The word 'honor' appears 2 times.\n",
            "The word 'blood' appears 15 times.\n",
            "The word 'Macbeth' appears 61 times.\n",
            "The word 'death' appears 9 times.\n",
            "2\n",
            "15\n",
            "61\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFOmOkNQOTZL"
      },
      "source": [
        "### **OPTIONAL BONUS:**\n",
        "\n",
        "> Create a case-normalized count dictionary. (Show your code.) How do the counts of your target words change after doing case normalization?\n",
        "\n",
        "### **BONUS ANSWER:**\n",
        "\n",
        "> (your answers here)\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be3G3vYgOVqO"
      },
      "source": [
        "## **Combining tuples and dictionaries for a bigram count dictionary**\n",
        "\n",
        "A nifty fact about tuples is that, unlike lists, they can be used as keys in a dictionary. This works precisely because they are immutable. Mutable lists are no good as dictionary keys, because we can't count on them staying the same.\n",
        "\n",
        "We'll now combine our previous two exercises in order to build a dictionary of bigram counts. Remember that we represented each bigram as a tuple. Now these will take the place of our single-word keys from the previous exercise.\n",
        "\n",
        "*Sidetrip: You may wonder why we care about two-word sequences. Bigrams, and in particular the frequencies with which bigrams occur, are in fact the foundation of language modeling technology. This is the technology that is used for predictive text (and many other applications). Bigram frequencies are simple but powerful!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVXf7tFjPWTS"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Put the things you learned from this section together and write code to create a dictionary of bigram frequencies. In this dictionary, each bigram should be a dictionary key, and the value of that should be the frequency of how often that bigram occurs in your selected text. Show your code!\n",
        "\n",
        "> Next, check your dictionary to see how frequent each of these bigrams are:\n",
        "\n",
        "* walk on\n",
        "* and the\n",
        "* Python rules\n",
        "* my love\n",
        "* three more bigrams of your choice\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> (your answers here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY8Jz1BSX8mT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8daf7b64-d811-41f7-efb0-0bdc18f3a2d0"
      },
      "source": [
        "bigramdict = {}\n",
        "bgm_list = ['walk on', 'and the', 'Python rules', 'my love', 'run on', 'out of', 'black coffee']\n",
        "for i in bgm_list:\n",
        "  if i in bigramdict:\n",
        "    bigrm = (bigm_list[i], bigm_list[i+1])\n",
        "    bigramdict.append(bigrm)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyrJlvieQCyD"
      },
      "source": [
        "## **Make it interactive!**\n",
        "\n",
        "You can use the following code to test new bigrams in your dictionary. Notice a few new bits of Python: `while` can be used to get input from the user (more on this next week). `format()` gives greater control over string formatting.\n",
        "\n",
        "To use the code below, you'll need to change `pairDict` to the name of the bigram dictionary you created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOpmM4cGPPrG"
      },
      "source": [
        "# code to test the dictionary\n",
        "word1 = input('First Word:')\n",
        "word2 = input('Second Word:')\n",
        "wordPair = (word1, word2)\n",
        "print('The pair {} occurs {} times in your text'.format(wordPair, pairDict[wordPair]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hSvivDOpoKD"
      },
      "source": [
        "# 4. The `FreqDist` function\n",
        "\n",
        "Now you know how to build frequency dictionaries for a text, both for single word types and for bigrams. If we look at the frequency counts for all of the words in the text, what we have is a **frequency distribution** over words in a text. A frequency distribution is a set of collected statistics about the frequencies of any kind of countable event. \n",
        "\n",
        "When we create a frequency distribution related to a text, we usually count the frequency of each word in the vocabulary within the text.\n",
        "Frequency distributions have some nice mathematical properties, and NLTK has a function designed specifically to create and work with frequency distributions. This is the `FreqDist` function. \n",
        "\n",
        "**READING:** Please read Section 3 of Chapter 1 of the NLTK book: http://www.nltk.org/book/ch01.html\n",
        "\n",
        "First, let's make a frequency distribution for `text7`, the collection of Wall Street Journal articles. The input to `FreqDist` is a list of words, so we can do this with any list of words (or any list of other items, for that matter).\n",
        "\n",
        "**NOTE:** If you get an error saying that `FreqDist` does not exist, uncomment the import statement on the first line of the code block below and try again. (Explicitly importing a function you want to use is one strategy to try, whenever you get this type of error.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ycW1R_tWrgf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b0e1d41-679d-4c95-c043-4ccda5967b99"
      },
      "source": [
        "# from nltk.probability import FreqDist\n",
        "### The function FreqDist creates a frequency distribution, which we store as the variable `fdist7`\n",
        "### I chose 7 because it is for text7\n",
        "\n",
        "fdist7 = FreqDist(text7)\n",
        "print(fdist7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<FreqDist with 12408 samples and 100676 outcomes>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VVuw4LMXmDz"
      },
      "source": [
        "The frequency distribution we produce is another special type within NLTK. When we print it out, we see some additional information - it is of type `FreqDist` and as 12408 samples and 100676 outcomes. The number of samples is the number of different event we're counting - in this case, it's the number of words in the vocabulary (should be the same as `len(set(text7))` -- test this!). The number of outcomes is the same as the token count (test this too!).\n",
        "\n",
        "The `FreqDist` data type is very similar to a dictionary, and we can (for the most part) use it like a dictionary. In this dictionary, the keys are words, and teh values are the frequencies of those words - the number of times they occur in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyftUKrzYnnR"
      },
      "source": [
        "### by simply typing in the variable to which we stored the frequency distribution,\n",
        "### we get a lot of information\n",
        "fdist7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzjVD7dNYSEi"
      },
      "source": [
        "### we can also inspect the frequency of individual words quite easily\n",
        "print(fdist7[\"company\"])\n",
        "print(fdist7[\"music\"])\n",
        "print(fdist7[\"musicians\"])\n",
        "print(fdist7[\"bankers\"])\n",
        "print(fdist7[\"banker\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXQV1cyFY2lj"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Create a frequency distribution of your selected gutenberg text. Check the counts of the words you looked up before, with the earlier version of your frequency dictionary. Do the counts match? Include your code.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> Yes the counts match with the earlier counts of the code. Code shown below.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myGQBPOPlvgY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a23ff44d-652a-4b5b-9a89-1bd6561c22c5"
      },
      "source": [
        "freq_m = FreqDist(macbeth_words)\n",
        "print(freq_m)\n",
        "print(freq_m['honor'])\n",
        "print(freq_m['blood'])\n",
        "print(freq_m['Macbeth'])\n",
        "print(freq_m['death'])\n",
        "print(macbeth_words.count('honor'))\n",
        "print(macbeth_words.count('blood'))\n",
        "print(macbeth_words.count('Macbeth'))\n",
        "print(macbeth_words.count('death'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<FreqDist with 4017 samples and 23140 outcomes>\n",
            "2\n",
            "15\n",
            "61\n",
            "9\n",
            "2\n",
            "15\n",
            "61\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbrCOVH3ZHqS"
      },
      "source": [
        "## **`FreqDist` methods**\n",
        "\n",
        "There are some built-in methods for frequency distributions that can be used to show informative words in texts:\n",
        "\n",
        "* `most_common()` - takes one argument (# of items X) and then shows the X most frequent items in the text, along with their counts\n",
        "* `hapaxes()` - returns a list of **hapax legomena** - words that occur only one time in the text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoI-MSnVZgcg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a04ec29d-bd02-428e-d715-e6cca0a547fb"
      },
      "source": [
        "print(fdist7.most_common(20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(',', 4885), ('the', 4045), ('.', 3828), ('of', 2319), ('to', 2164), ('a', 1878), ('in', 1572), ('and', 1511), ('*-1', 1123), ('0', 1099), ('*', 965), (\"'s\", 864), ('for', 817), ('that', 807), ('*T*-1', 806), ('*U*', 744), ('$', 718), ('The', 717), ('``', 702), (\"''\", 684)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nTDd_uCZk-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "637138bb-bc63-4922-a757-4cc0a164c5f1"
      },
      "source": [
        "print(fdist7.hapaxes())\n",
        "print(len(fdist7.hapaxes()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Pierre', 'Elsevier', 'Agnew', 'fiber', 'resilient', 'lungs', 'symptoms', 'Loews', 'Micronite', 'spokewoman', 'properties', 'Dana-Farber', 'filter', '1953', '1955', 'Four', 'diagnosed', 'malignant', 'mesothelioma', 'asbestosis', 'morbidity', 'Groton', 'stringently', 'smooth', 'needle-like', 'classified', 'amphobiles', 'Brooke', 'pathlogy', 'Vermont', 'curly', 'Environmental', 'Protection', 'gradual', '1997', 'cancer-causing', 'outlawed', '160', 'Areas', 'dusty', 'burlap', 'sacks', 'bin', 'poured', 'cotton', 'acetate', 'mechanically', 'clouds', 'dust', 'hung', 'ventilated', 'Darrell', 'Yields', 'tracked', 'IBC', 'fraction', 'Compound', 'reinvestment', 'lengthened', 'longest', 'Donoghue', 'Longer', 'Shorter', 'Brenda', 'Malizia', 'Negus', 'rises', 'pour', 'Assets', '352.7', 'money-fund', 'Dreyfus', 'World-Wide', 'top-yielding', '9.37', '9.45', 'invests', 'waiving', '8.12', '8.14', '8.19', '8.22', '8.53', '8.56', 'J.P.', 'Bolduc', '83.4', 'energy-services', 'Terrence', 'Daniels', 'Royal', 'Trustco', '212', 'McDermott', 'Babcock', 'Wilcox', 'S.p', '295', 'Wickliffe', 'computerized', '2,700', '370', '2.80', 'Legislation', 'ensnarled', 'earliest', 'Clark', 'oversee', 'biannual', 'powwow', 'titans', 'sunny', 'confines', 'Hoosier', 'royalty', 'rock', 'buckle', 'Rust', 'rusty', 'Maytag', 'lesser', 'knowns', 'Trojan', 'Queen', 'Cheese', 'starters', 'Hudnut', 'Symphony', 'Orchestra', 'guest', 'pianist-comedian', 'Borge', 'dessert', 'escort', 'busloads', 'wives', 'raced', 'Speedway', 'unimpeded', 'traffic', 'lieutenant', 'breakfast', 'drinks', 'everyday', 'speedway', 'hauled', 'crews', '10-lap', 'Fortune', 'drooled', 'schoolboys', 'dummies', 'Back', 'execs', 'boarding', 'dancing', 'moons', 'renovated', 'Roof', 'ballroom', 'hottest', 'duckling', 'mousseline', 'lobster', 'consomme', 'veal', 'mignon', 'chocolate', 'terrine', 'raspberry', 'sauce', 'Knowing', 'tasty', 'eat', 'ovation', 'red-carpet', 'tempts', 'heartland', 'winter', 'sluggishness', 'Preliminary', 'tallies', 'cloud', 'export-oriented', 'Exports', '5.29', '0.7', '5.39', 'gloomy', 'accumulated', '50.45', '50.38', 'Post', 'discounts', 'fixtures', 'underscore', 'Mortimer', 'Zuckerman', 'four-color', '100,980', 'mid-October', 'guaranteed', 'subscriber', '120,000', 'Circulation', 'awards', 'renewal', '325,000', '340,000', 'shore', '1,620', 'Publishers', 'giveaways', 'subscribers', 'telephones', 'Audit', 'Circulations', 'newsweekly', '4,393,237', '3,288,453', '2,303,328', 'payoff', 'Westborough', '2.29', '2.25', 'Manchester', 'N.H.', 'efficiencies', 'evaluated', 'emerges', 'attracts', 'Wilbur', 'Rothschild', 'cluttered', 'Regulatory', 'refile', 'expedited', 'FERC', 'Ricken', 'Toys', 'R', 'Us', 'Deane', 'Signet', 'Rexinger', 'Glauber', '12-member', 'ratepayers', 'overruns', 'entertain', 'pool', 'hostage', 'slash', '737.5', '3.01', 'tracking', 'addresses', 'nightmare', '38.375', 'disputed', 'collecting', 'refunded', 'calculations', 'court-ordered', 'summer\\\\/winter', 'differential', 'Appellate', '245', '72.7', 'LaSalle', '500,004', 'Automobile', 'year-to-year', 'setting', '0.4', '361,376', 'arising', 'consumption', '30,841', '13,056', 'Chinchon', 'diversifying', 'fledgling', 'depend', 'longevity', 'Documents', 'product-design', 'scrapped', 'operational', 'prototype', 'Minneapolis-based', 'needing', 'worst-case', '2.875', '98.3', 'promissory', 'complicate', 'tricky', 'unproven', 'gallium', 'arsenide', 'robotic', 'twice', 'C-90', 'Hitachi', '4.75', 'Pro-forma', '19.3', '5.9', 'existed', '20.5', 'Regarded', '240,000', 'Davenport', 'Blanchard', 'Hammerton', 'Wheeland', '241', 'Pardus', 'electric-utility', 'Carney', 'Tassinari', 'diplomacy', 'watching', 'copyrights', 'India', 'unfair-trade', 'investigations', 'genuine', 'touchy', 'denial', 'harms', 'inventiveness', 'offending', 'citizens', 'protections', 'discouraging', 'deterring', 'high-technology', 'lauded', 'pirates', 'search-and-seizure', 'initialing', 'amending', 'trademark', 'unauthorized', 'showings', 'compel', 'video-viewing', 'parlors', 'enact', 'compatible', 'literary', 'lower-priority', 'Greece', 'less-serious', 'Hoffman', 'specializing', 'retaliation', 'improvements', 'craft', 'developments', 'piracy', 'disregard', '301', 'halve', 'third-highest', 'declaration', 'Economy', 'Nestor', 'Latin', 'aspires', 'external', 'Miguel', 'Alurralde', 'Assistant', 'Mulford', 'negotiator', 'Carballo', 'Menem', 'centennial', 'milestones', 'THREE', 'COMPUTERS', 'THAT', 'CHANGED', 'Commodore', 'Pet', 'Tandy', 'TRS-80', 'audiocassettes', 'garage', 'Wozniak', 'Jobs', 'hobbyists', 'Homebrew', 'Club', 'affordable', '1,298', 'explosive', 'desktop', 'mainframe', 'built-from-kit', 'Altair', 'Sol', 'IMSAI', 'keyboards', 'contributors', 'language-housekeeper', 'billionaire', 'adapted', 'Shugart', 'Seagate', 'Hayes', 'Dale', 'Heatherington', 'co-developers', 'modems', '38.3', 'F.H.', 'Jersey-based', 'Purepac', 'label', 'Strait', '321,000', 'Broken', 'Pty.', 'Output', 'gradually', 'reaches', 'Perch', 'Dolphin', 'Seahorse', 'Tarwhine', 'Sloan', 'Following', 'gelatin', 'capsules', 'divest', 'non-encapsulating', 'possessions', 'Generalized', 'Preferences', 'classifications', 'categories', 'injury', 'low-priced', 'battery-operated', 'preferences', 'beneficiaries', '37.3', 'automotive-parts', 'achieved', 'Akerfeldt', 'wallowing', '52-week', '16.125', '13.73', '9.625', 'resume', 'personally', 'assisted', 'Manfred', 'Gingl', 'Chilver', '63-year-old', 'Clays', 'securities-based', '701', 'mortgage-based', 'O.', '570', 'Blackstone', 'seven-year', 'redeploy', 'channel', 'addressing', 'tenth', 'Pate', '54-year-old', 'LTV', 'S.p.A.', 'indirect', '37-a-share', 'incorporated', 'editions', '1.82', '84.29', 'yen-support', 'intervention', '150.00', 'sharper', '86.12', 'Pick', 'rash', 'triple', 'Washington-based', 'turf', 'Austria', 'Portugal', 'Next', 'Corazon', 'Aquino', 'province', 'Anything', 'quips', 'Northampton', 'mirrors', 'mania', 'narrowly', 'wildly', 'oblivion', 'open-end', 'one-country', 'issuing', 'Behind', 'hoopla', 'heavy-duty', 'stretching', 'nets', 'urge', 'smattering', 'emerging', 'outpaced', 'Country', 'taste', 'burned', 'Political', 'whipsaw', 'clobbered', 'frenzy', 'Share', 'alarmed', 'valuations', 'fattened', 'foreign-stock', 'resistant', 'Nonetheless', 'aghast', 'lofty', 'jumping', 'advice', 'ready', 'repayment', 'pre-Communist', 'Russian', 'Coincident', 'short-lived', 'Kerensky', 'Communists', 'seized', '1917', '1934', 'Debt', 'Default', 'amended', 'U.S.S.R.', 'pre-1917', 'pre-1933', 'satisfying', 'lend-lease', 'Factories', 'booked', '236.74', '236.79', '59.6', 'contractors', '415.6', '415.8', 'seasonal', 'Mayland', 'taper', 'blank', 'industrial-production', 'slip', 'Inventories', 'watched', 'buildup', 'conforms', 'Elliott', 'eases', 'nondurable', '109.73', 'Orders', 'durable', '0.2', '127.03', 'durable-goods', '234.4', 'Shipments', 'unfilled', '0.5', '497.34', '191.9', 'Berson', 'Spending', 'nonresidential', '99.1', 'F.W.', 'goverment', 'signaling', 'Pitney', 'Bowes', 'purhasing', 'vendors', 'delivering', 'inflationary', 'abating', 'lengthen', 'gauges', 'worsening', 'acknowledging', 'Items', 'numbered', 'newcomer', 'powder', 'nonfat', 'dairy', 'Sebastian', 'Judging', 'Haruki', '320', '18.95', 'baby', 'boomers', 'texture', 'Characters', 'Salty', 'Dogs', 'whistle', 'Johnny', 'Goode', 'Bugs', 'Bunny', 'Mickey', 'Spillane', 'Groucho', 'Harpo', 'desultory', 'charm', 'recognizing', 'buttoned-down', 'lore', 'refreshing', 'self-aggrandizing', 'we-Japanese', 'perpetuate', 'unique', 'unfathomable', 'outsiders', 'implicit', 'nutty', 'plot', 'rooted', 'imaginative', 'disaffected', 'hard-drinking', 'nearly-30', 'snow', 'search', 'elusive', 'sheep', 'behest', 'sinister', 'erudite', 'mobster', 'tow', 'prescient', 'girlfriend', 'sassy', 'retorts', 'docile', 'butterfly', 'meets', 'solicitous', 'chauffeur', 'God', 'roughhewn', 'wears', 'sheepskin', '40-year-old', 'sensation', 'Norwegian', 'fluent', 'lyrics', 'published', 'youthful', 'brat', 'pack', 'dominating', 'best-seller', 'charts', 'idiomatic', 'dashes', '339', 'fabled', 'virtues', 'player', 'batting', 'Polls', 'Tatsunori', 'Hara', 'humble', 'uncomplaining', 'obedient', 'soul', 'besuboru', 'ball', 'bat', 'Fans', 'politely', 'balls', 'ushers', 'expands', 'depending', 'hitter', 'honorably', 'abide', 'wear', 'chronicle', 'rationed', 'Soho', 'petulant', 'impudent', 'hosted', 'Luce', 'Fellowship', 'supercilious', 'vicious', 'passages', 'invades', 'mundane', 'aspects', 'regimented', 'assigned', 'lunch', 'austere', 'dormitory', 'prying', 'caretaker', 'observations', 'salarymen', 'unproductive', 'overtime', 'sake', 'solidarity', 'hierarchical', 'chary', 'enormously', 'frustrating', 'science', 'raring', 'invent', 'Walkman', 'Kirkpatrick', 'corners', 'tobacco', 'smoke', 'Discos', 'exempt', 'bars', 'bans', 'theaters', 'hospitals', 'Siti', 'Zaharah', 'Sulaiman', 'No-Smoking', 'Week', 'Mara', 'Kuala', 'Lumpur', 'on-campus', '26,000', 'stalls', 'posters', 'signboards', 'restricts', 'designated', 'Backer', 'Spielvogel', 'Bates', 'colony', 'surveyed', 'espouse', 'stress', 'Thai', 'cabinet', 'Pramual', 'Sabhavasu', 'Bangkok', 'Plaza', 'undertaking', 'Yasser', 'Arafat', 'Palestine', 'Liberation', 'WAFA', 'PLO', 'Tourism', 'food-shop', 'mainland', 'hospital', 'chaotic', 'PAP', 'unrealistically', 'happier', 'establishing', 'diplomatic', 'strapped', 'Warsaw', 'multibillion-dollar', 'Danube', 'Austrian', 'twinned', 'upstream', 'authorized', 'Miklos', 'twin', 'dams', 'twindam', 'Czech', 'solely', 'painting', 'Strindberg', 'Scandinavian', '2.44', 'Lighthouse', 'painted', 'oils', 'playwright', '1901', 'upturn', 'couples', 'exchanging', '271,124', '400,000', 'BRAMALEA', '85.1', '10.5', 'accrued', 'swapped', 'Lead', 'Scotia', 'McLeod', 'RBC', 'Dominion', 'Bramalea', 'actor', 'inheritor', 'Charlie', 'Steve', 'laid', 'obsessed', 'refitting', 'Purchase', 'campus', 'Place', '36-minute', 'black-and-white', 'full-length', 'poignant', 'modern-day', 'tramp', 'Composer', 'Marc', 'Marder', 'bass', 'classical', 'ensembles', 'exciting', 'eclectic', 'characters', 'intertitles', 'good-hearted', 'Filmed', 'lovely', 'Dill', 'benign', 'noticing', 'jostle', 'cabs', 'hangs', 'Greenwich', 'Village', 'Sixth', 'populated', 'jugglers', 'magicians', 'good-natured', 'hustlers', 'dead-eyed', 'four-year-old', 'Cosmopolitan', 'curled', 'sketching', 'passers-by', 'skirmishes', 'carefree', 'cure', 'two-year-old', 'waif', 'Nicole', 'Alysia', 'thugs', 'cute', 'curse', 'alerts', 'inadequacy', 'vagrant', 'beds', 'Bowery', 'Mission', 'drearier', 'tuck', 'dreamed', 'improbable', 'shop', 'high-rise', 'resonate', 'camera', 'glamorize', 'vagabond', 'existence', 'whimsical', 'enviable', 'Claude', 'weird', 'Story', 'Women', 'captivating', 'disagreeable', 'Giraud', 'significance', 'hypocrisy', 'collaborated', 'Resistance', 'fighters', 'Jews', 'diversionary', 'symbolic', 'traitor', 'small-time', 'accidentally', 'enabled', 'jam', 'cocoa', 'war-rationed', 'goodies', 'untrained', 'botched', 'remorse', 'shallow', 'playful', 'dreadful', 'war-damaged', 'lover', 'thin-lipped', 'Isabelle', 'Huppert', 'Marie', 'chopped', 'Gringo', 'rendering', 'Fuentes', 'Mexican', 'Revolution', 'endless', 'eating', 'drinking', 'celebrate', 'movies', 'Peck', 'marvelously', 'loose', 'portrayal', 'Video', 'Tip', 'finest', 'twin-jet', 'Seattle', 'Kawasaki', 'Fuji', 'accounting', 'contribution', 'plane', 'mid-1990s', 'irony', 'off-off', 'scattered', 'hostility', 'mudslinging', 'empty', 'ushering', 'Napolitan', 'stirrings', 'dawn', 'sometimes-tawdry', 'entertaining', 'confrontational', 'truthful', 'fights', 'commercials', 'facial', 'disembodied', 'contributions', 'accurately', 'hid', 'kidnapper', 'phony', 'Nasty', 'innuendoes', 'Siegal', 'prosecute', 'Shrum', 'Doak', 'unleashed', 'distorted', 'photos', 'Compare', 'Everybody', 'oversight', 'secret', 'Campaign', '95,142', 'matching', 'errors', 'get-out-the-vote', 'kidnapping', 'Powers', 'deceptive', 'argues', 'Lt.', 'gubernatorial', 'Greer', 'persuasion', 'tour', 'Against', 'Monticello', 'superimposed', 'liberty', 'Virginians', 'nurtured', 'generations', 'statue', 'Jefferson', 'dissolves', 'incest', 'transforming', 'Goodman', 'shake', 'counterattack', 'close-up', 'shadows', 'recalling', 'unpleasant', 'ordeal', \"C'mon\", 'boyfriends', 'interjects', 'interrogated', 'rapists', 'constituent', 'technique', 'unfounded', 'interrogation', 'tired', 'stigma', 'campaigner', 'Rozell', 'onus', 'lasted', 'carrying', 'sensitivity', 'aired', 'photograph', 'Remember', 'Consider', 'Squier', 'dirty', 'rusted', 'drums', 'swim', 'purrs', 'neighbors', 'cry', 'cleaned', 'pollution', 'Eagleton', 'Barrels', '1966', 'route', 'rout', 'Seats', '331,000', '550,000', 'propelling', 'Interviews', 'fits', 'sparking', 'posing', 'whereby', 'knitted', '5.57', '705.6', 'cash-rich', 'sites', 'labor-intensive', 'tigers', 'subordinate', 'fearful', 'hegemony', 'encourages', 'burdens', 'resists', 'U.S.-Japanese', 'behemoth', 'swelling', 'multinationals', 'offend', 'lender', 'Drobnick', 'cohesive', 'sectors', 'Calder', 'Woodrow', 'Wilson', 'Internatonal', 'tubes', 'assemble', 'Countries', 'framework', 'ministers', 'Participants', 'Zealand', 'Brunei', 'rim', 'Hawke', 'reasserts', 'designing', 'dominance', 'outstrips', 'outranks', 'enlarged', 'convey', 'undertone', 'Farren', 'benevolent', 'altruistic', 'apprehensive', 'troop', 'Asians', 'counterweight', 'marbles', 'juggernaut', 'monopolize', 'sew', 'Chong-sik', 'ninth', 'spotted', 'uncanny', 'stockbroker', 'profession', 'Senate-House', 'Virtually', 'OK', 'Confronted', 'display', 'projector', 'underline', 'prosecuted', 'downfall', 'unstinting', 'laurels', 'bitterness', 'anger', 'betrayer', '*T*-82', '*T*-83', 'school-district', 'stunned', 'bald-faced', 'martyr', 'dark', 'high-stakes', 'enhanced', 'cheat', 'school-improvement', 'bolster', 'depended', 'student-test', 'incredible', 'Walt', 'Haney', '*T*-84', '50-state', 'school-research', 'inflated', 'Evidence', 'erasures', '*T*-85', 'occurrences', 'revising', 'beforehand', 'precise', 'Use', 'Experts', 'wrenching', 'state-supervised', 'interventions', '*T*-86', 'firings', 'lab', 'grants', 'superintendent', 'Scholastic', 'Aptitude', '*T*-87', 'SAT', 'entrance', 'prosecuting', 'administrators', 'Sandifer', 'purely', 'foundation', 'inferences', 'track', 'achievement-test', 'Standing', 'shaded', 'hill', 'educated', 'brightest', 'Nobel', 'Prize', 'Townes', 'actress', 'Joanne', 'Woodward', 'glory', 'faded', 'yellow', 'bricks', 'facade', 'gangs', 'Crime', 'awful', 'enrollment', 'honors', 'seventh', 'breakdown', 'Prior', 'bled', 'halls', 'stabbed', 'Academically', 'disparate', 'privileged', 'elite', 'monied', 'inner', 'resolved', 'clean', 'deadwood', '*T*-88', 'ushered', 'betterment', '*T*-89', 'Being', '37-year-old', 'dismissal', 'dreamt', 'struggled', '14-hour', '1986-87', '1987-88', 'Encouraged', 'cheerleaders', 'pep', 'Cultural', 'Literacy', 'PTA', 'Teacher', 'inspirational', 'Laura', 'Dobson', 'freshman', '*T*-90', 'teacher-cadet', '11th', 'grader', 'Kelli', 'Green', 'distinguished', 'herself', 'approaches', 'pair', 'college-bowl', 'competitions', 'weekends', 'prepare', 'polish', 'correcting', 'homework', 'cocky', '*T*-91', 'grandstander', 'Pressures', 'deteriorating', 'incentive-bonus', 'Huge', '23,000', 'Winning', 'pride', 'Ariail', 'attending', 'adequately', 'copied', 'motives', 'sociology', 'rankings', 'Mostly', 'self-esteem', 'broke', 'desperately', '*T*-93', 'cared', '*T*-94', 'drag-down', '*T*-95', 'defeats', 'inkling', 'underprivileged', 'prosecutor', '*T*-96', 'alumni', 'concedes', 'sympathy', '*T*-97', 'morale-damaging', 'dumbfounded', 'recalls', 'knife', 'astonishment', 'dismay', 'superiors', 'unpopularity', 'Mrs', 'school-board', 'crowded', '*T*-98', 'testify', 'Supportive', 'decried', 'particulars', 'offense', 'talk-show', 'Editorials', 'overused', 'enraged', 'first-time', '*T*-99', 'expunged', 'conviction', 'cranked', 'worthy', 'witnesses', 'cheerleading', 'squad', 'crushed', '17-year-old', 'T-shirts', 'corridors', 'red-and-white', 'GHS', 'logo', 'shirts', '*T*-100', '*T*-101', 'aspersions', 'incident', 'evaluating', 'Gayle', 'worms', 'relieved', 'chalk', 'touched', 'slate', 'schoolchildren', 'workbooks', 'Roman', 'numeral', 'IX', 'two-sevenths', 'three-sevenths', 'Worksheets', 'test-practice', 'kit', '*T*-102', 'Communication', 'Close', 'Test-preparation', 'subindustry', 'school-sponsored', 'justifying', 'Traverse', '*T*-103', 'aids', '*T*-104', 'learning', 'everybody', 'aces', 'psychiatrist', '*T*-105', 'Standardized', 'kindergarten', 'eighth', 'Houghton', 'Mifflin', 'Harcourt', 'Brace', 'Jovanovich', 'Metropolitan', 'test-prep', 'Arizona', 'Louisiana', 'tools', 'test-preparation', 'binders', 'best-selling', 'CTB', '*T*-106', 'replicated', '*T*-107', 'coincidental', 'schoolteacher', 'similarity', 'devised', '69-point', 'awarding', 'subskill', 'closeness', 'preparatives', 'symmetry', 'geometrical', 'measurement', 'pie', 'graphs', 'kits', 'replicate', 'familiarization', '66.5', '64.5', 'two-letter', 'consonant', 'exclusion', 'contains', 'examples', 'matches', 'scrupulously', 'replicating', 'publication', 'outraged', 'advisory', 'CTBS', '*T*-108', '*T*-109', 'unaware', 'discontinue', 'H.N.', 'Frances', 'Berger', 'Sacramento-based', '*T*-110', 'north', 'ancillary', 'transplantation', 'humans', 'juvenile', 'diabetes', 'degenerative', 'Huntington', 'therapies', 'abortions', 'tissue-transplant', 'federally', '*T*-111', '*T*-112', 'acting', 'implant', 'brain', 'patient', 'NIH-appointed', 'recommended', 'carefully', 'embroiled', 'anti-abortion', 'recruit', 'doctors', 'helm', 'Centers', 'Disease', 'Control', '*T*-113', 'surgeon', 'reportedly', 'opposes', 'Child', 'imposing', 'defuse', 'CDC', 'judged', 'excellence', 'disturbs', 'judgments', 'Myron', 'dean', 'polarized', 'exists', 'warns', 'discourage', 'unavailability', 'foundations', 'fronts', 'regenerate', '*T*-114', '*T*-115', 'Down', 'syndrome', 'retardation', 'Rekindled', 'lackluster', '1.01', '456.64', '118.6', 'inauspicious', '133.8', 'busiest', 'averaged', 'nonfinancial', '1.39', '446.62', '1.28', '449.04', '*T*-116', '3.23', '436.01', 'unattractive', 'anticipating', 'permitting', '*T*-117', 'beneficiary', '*T*-118', '*T*-119', 'expires', 'Macheski', 'Wilton', '*T*-120', 'WFRR', 'L.P.', 'GHKM', '273.5', '*T*-121', 'Centerbank', 'NESB', 'Pennview', 'leapt', 'Univest', '25.50', 'Nelms', 'near-record', 'definitive', '78', '*T*-122', 'restaurant', '858,000', 'Huntsville', 'Ala.', '225.6', 'Internal', '*T*-123', '*T*-124', 'outcry', '*T*-125', 'protected', '*T*-126', 'receives', 'document', 'Form', 'Social', 'passport', 'Failure', '*T*-127', 'identities', 'rarely', 'acted', 'witness', 'red-flag', '*T*-128', '*T*-129', 'computer-generated', 'certified', 'mail', '*T*-130', '*T*-131', 'Filling', 'tip', 'Lefcourt', 'Delegates', 'condemning', '*T*-132', 'grand', 'prohibited', 'ethics', 'disclosing', 'committing', '*T*-133', 'notice', 'Lezovich', 'summons', 'initiated', 'dating', 'correspondence', 'mailed', '8300s', 'Individuals', 'assertions', 'relating', 'WAR', 'OVER', 'JUDICIAL', 'SALARIES', 'Often', 'fanfare', 'Raul', 'quitting', 'clerks', 'quipped', '89,500', 'Judges', 'accountants', 'blinks', 'sum', 'Orrick', 'Herrington', 'Sutcliffe', 'detail', 'DOONESBURY', \"CREATOR'S\", 'UNION', 'TROUBLES', 'Cartoonist', 'Garry', 'punish', 'screenwriters', 'Productions', '*T*-134', 'collective-bargaining', 'cartoonist', 'reviewing', 'harassment', 'consists', 'year-long', '*T*-135', 'punishment', 'punishing', 'retaliating', 'ABORTION', 'RULING', 'UPHELD', 'abortion-related', '*T*-136', 'obtaining', 'referrals', '*T*-137', 'advocate', 'Appeals', 'providers', 'pregnant', 'INQUIRY', 'CLEARS', 'TEXAS', 'JUDGE', 'remarks', 'sentencing', '18-year-old', 'referring', 'queers', 'cruising', 'picking', 'teenage', 'appointed', 'commenting', 'Observing', 'exhibited', 'prejudice', 'impartial', 'prostitute', 'discredit', 'judiciary', 'Judicial', 'Conduct', '*T*-138', 'empowered', 'TRIAL', 'stock-manipulation', 'Lowe', 'eight-count', 'indictment', 'Vice', 'Sherwin', 'manipulate', 'Carbide', 'trials', 'mistrials', 'SWITCHING', 'TO', 'DEFENSE', 'Iran\\\\/Contra', 'affair', 'Mayer', 'three-lawyer', '520-lawyer', 'specialize', 'white-collar', 'narcotics', 'Tire', 'Rubber', 'Albany', 'warehousing', '353', 'apologizing', 'indulging', 'rebuked', 'fretted', 'penny', '*T*-139', 'tie-breaking', 'computer-system-design', 'summoned', 'ministry', 'antitrust-law', 'apologize', 'packed', 'sorry', 'embarrassing', 'sacrificing', '*T*-140', 'gifts', 'businessmen', 'extramarital', 'behavior', 'municipality', 'low-ball', 'Foreigners', 'complain', 'procurement', 'undercut', 'excessively', 'slashing', 'semiconductors', 'U.S.-Japan', 'one-yen', 'map', 'waterworks', '77,000', 'Saitama', 'prefectural', 'Wakayama', 'emerge', 'Michio', 'Sasaki', 'Keidanren', 'Federation', 'Organizations', 'PAPERS', '*T*-141', 'papers', 'Osborn', 'Desai', 'Michaels', '*T*-142', '14.6', '32.8', '28.6', '29.3', '28.4', 'locally', 'hydraulically', 'wheel-loader', 'Heidelberg', '280', 'Dynamics', '*T*-143', 'computer-aided', 'automation', '*T*-144', 'productivity', '@', 'Deposits-a', '6.21', 'metropolitan', 'Rate', 'Monitor', 'b', 'LSI', 'Logic', '*T*-145', 'industry-wide', 'semiconductor', 'custom-chip', 'lagging', 'billings', 'economical', 'Wilfred', 'Corrigan', 'midyear', 'phase', 'appropriate', 'equals', '86', 'counting', '133.7', '94', 'five-inch', 'more-efficient', 'silicon', 'wafers', 'fabricate', 'Related', 'converting', 'Clara', 'speculate', '*T*-146', 'Stark', 'Robertson', 'Stephens', 'Part', 'jitters', 'INGERSOLL-RAND', 'Woodcliff', 'Kuhns', 'buoyed', 'cautiously', 'bearish', 'underpin', 'narrow', 'Goldinger', 'Insight', '77.70', '77.56', 'Dollar-yen', 'Trettien', 'Banque', 'Paribas', 'convinced', 'erode', '1.5755', '1.5805', '143.93', '143.08', 'traced', 'wave', 'vitriolic', 'mollified', 'knight', 'undisclosed', 'forthcoming', 'yen-denominated', 'redeeming', 'unclear', 'unabated', 'recede', 'cues', '*T*-147', '45.3', '*T*-148', 'minimal', '374.20', '374.19', 'Show', 'Huxtable', 'viewers', '*T*-149', '187', '*T*-150', 'fuming', 'ultimatum', 'Either', 'Different', 'spin-off', '*T*-151', 'tactics', 'tell', 'flooded', 'comedies', '*T*-152', 'networks', 'pre-emptive', '2-8', 'A.C.', 'Nielsen', 'R.I.', 'Raleigh', 'Ky.', 'Dick', 'Lobo', 'WTVJ', 'NBC-owned', 'Kuvin', 'WHAS', 'uncomfortable', 'frankly', 'Wu', 'Atlanta-based', 'life-insurance', 'Nationale', 'Nederlanden', 'frantic', 'revenue-desperate', 'magazines', 'cozy', 'fawning', 'articles', 'advertorial', 'downright', 'thumbing', 'billed', 'Practical', 'Environment', 'entrepreneur', 'Patricia', 'how-to', 'backyard', 'composting', 'explanatory', 'essays', 'happens', 'flush', 'toilet', 'hard-hitting', 'whirling', 'rampage', 'supermarket', 'aisles', 'guys', 'feature', 'deem', 'standpoint', '*T*-153', 'belt', 'alienated', 'would-be', 'ire', 'furious', 'microwave', 'column', 'diagram', 'arrows', 'polystyrene', 'foam', 'polyproplene', 'polyester', 'non-biodegradable', 'landfill', 'monster', 'practicing', 'journalistic', 'garbage', 'fumes', 'Modifications', 'portrayed', 'recyclability', 'Soups', 'Mike', 'DDB', 'big-time', 'handful', 'Adolph', 'Coors', 'Bumkins', '*T*-154', 'relied', 'subscription', 'revenues', 'Individual', '2.95', 'yearly', 'recycled', 'Old-House', '126,000', 'newsstands', '93,000', 'Ad', 'Notes', 'INTERPUBLIC', 'ON', 'E.C.', 'Fremantle', 'supplier', 'NEW', 'ACCOUNT', 'CoreStates', 'Earle', 'Palmer', 'Spiro', '*-86', 'VanSant', 'Dugdale', 'FAX', 'Fax', 'Ogilvy', 'Mather', 'WPP', 'Billings', '*T*-155', 'serviced', 'FIRST', 'CAMPAIGN', 'Rent-A-Car', 'replacement-car', 'rentals', 'accidents', 'Developed', 'Avrett', 'Ginsberg', 'pitches', 'consumer-driven', 'pick-up', 'drop-off', 'LANDOR', 'ASSOCIATES', 'Landor', 'identity-management', 'ACQUISITION', 'Ketchum', 'Braun', 'investor-relations', 'marketing-communications', '70-a-share', '*T*-156', 'Hamilton', 'Bermuda-based', '777', '963', 'asset-sale', '620', '490', 'flexibility', 'leeway', 'characterizing', 'entrench', 'confuse', 'materialize', '36', 'converted', '*-88', 'Bermuda', '62.625', 'equip', 'lap-shoulder', 'rear', '*-89', 'Samuel', 'Skinner', 'milestone', 'occupant', 'equipped', '*-90', 'front-seat', 'headrests', '*-91', 'urging', 'car-safety', '*T*-157', 'classed', 'therefore', 'luck', 'Chuck', 'Hurley', 'communications', 'Mo', 'rollover', 'crashes', 'bags', 'side-crash', 'weighing', 'unloaded', 'weight', 'roof', 'depressed', '*-92', 'inches', 'lap', 'engineer', 'auto-safety', 'installing', 'F-series', 'Crew', 'Cab', 'pickups', 'Explorer', 'sport-utility', 'Rail', 'enclosed', 'transporting', 'autos', 'multilevel', 'Thrall', 'Duchossois', 'Elmhurst', '850', 'Walters', '58-year-old', 'cement', 'Milne', '*T*-158', 'retires', 'Blue', 'Circle', '*-93', 'longstanding', 'Later', 'senate', 'Certainly', '*T*-159', 'prospectively', '*T*-160', '*-94', 'merger-related', '*-95', '121.6', 'lay', 'altogether', 'exclusively', 'leasing', 'GOODY', 'PRODUCTS', '11.5', 'payable', 'Jan.', 'Kearny', 'N.J.-based', 'hair', '992,000', '1.9', 'anti-takeover', 'Henderson', '51-year-old', 'Ian', '*T*-161', 'retiring', '1\\\\/10th', 'redemption', '*-96', 'proprietor', 'Cellars', 'Napa', 'tag', 'wine-making', 'estimation', '700', 'Sauvignon', 'sticker', 'fastest', 'exceptional', 'exceedingly', 'Lafite-Rothschild', 'Haut-Brion', 'Grand', 'Cru', 'deluxe', 'Champagnes', 'Dom', 'Perignon', 'rarefied', 'Trockenbeerenauslesen', 'Rieslings', 'Riserva', 'flashy', 'zoomed', 'priciest', 'boast', 'lion', 'bottles', 'smallest', '*T*-162', 'Sauternes', 'lighter', 'spectacularly', 'prestige', 'cuvees', 'inching', \"'82\", 'Taittinger', 'Comtes', 'encroaching', 'reds', 'Rhone', 'Guigal', 'Cote', 'Rotie', 'Landonne', 'steal', 'Domaine', 'la', 'vineyard', 'anywhere', 'commanded', 'three-digit', 'tags', 'Coche-Dury', 'Corton-Charlemagne', 'Angelo', 'Gaja', 'Barbaresco', 'Piero', 'Antinori', 'Solaia', 'Vega', 'Secilia', 'Unico', '10th', 'Grange', 'cult', '*T*-163', '*T*-164', 'happening', 'scarce', 'exhausted', \"'40s\", \"'50s\", '*T*-165', 'newer', 'bargain', 'ripen', 'acre', '*T*-166', 'yielded', 'Owner', 'Al', 'Brownstein', 'retailer', 'Is', 're-thought', 'Offering', 'yes', '*-97', 'six-bottle', 'retailers', 'awfully', 'Schaefer', 'Skokie', 'suburban', 'opinions', '*T*-167', 'wins', 'sticker-shock', 'talked', 'excited', 'astronomical', 'collection', 'one-upsmanship', 'Rock', 'Terrace', 'Dunn', '*T*-168', '*T*-169', 'knowledgeable', 'Cedric', 'Cellar', 'overpriced', '*-99', 'Grgich', 'Chardonnay', 'Chardonnays', 'Image', '*T*-170', '*T*-171', '*-100', 'le', 'walking', \"'86\", 'Opus', 'Dominus', 'wine-buying', 'holidays', '*T*-172', 'Ensrud', 'free-lance', 'Signs', 'upward', 'achieving', '8.75', 'shown', 'petroleum', 'Retail', 'big-ticket', '*-101', 'resisting', 'excesses', 'tilt', 'Integra-A', 'Restaurant', '*-103', '105', '13.5', '*T*-173', '*-104', '*T*-174', 'hotels', '*T*-175', '445', 'Shelby', 'Steelworkers', 'Local', '3057', 'Tube', 'expired', 'pact', '230-215', 'stoppage', 'postponed', '*-105', 'autions', 'rescheduled', '*-106', 'Unless', '*T*-176', 'partisan', 'bickering', '*T*-177', 'entangled', 'disruption', 'schedule', 'taxpayer', 'Nicholas', 'Brady', 'imperative', '*-107', 'maturing', '*-108', '*-109', '*-110', '36-day', 'when-issued', 'approves', 'clearing', '47.5', 'Auctions', '25.6', '21.9', '*-111', 'decides', 'Colony', 'noodles', 'pre-cooked', 'pasta', 'Clive', 'tidily', 'Long-term', 'Continuing', '*-112', 'evaporated', 'sight', '*T*-178', 'Beige', 'Book', '154.2', 'Smelting', '5.276', '36.9', '3.253', '4.898', '1.457', '*T*-179', 'rebuffed', 'reviewed', 'combinations', 'proof', '*-113', 'clamped', 'ankle', '1.75', '51.25', '22.75', 'Landis', '*T*-180', 'lessening', 'likelihood', 'agreeing', 'Satrum', 'spurns', '*T*-181', 'solicitation', 'replacing', '*-114', 'nominal', '*-115', '46.1', '251.2', '278.7', 'licensing', 'challenging', 'Westport', 'punitive', '*T*-182', 'combat', 'patented', 'Brunswick', 'Criticism', 'skittishness', 'unfettered', 'removal', 'perceives', '*T*-183', 'stirred', '*-116', 'Impediments', 'Initiative', '*T*-184', 'informally', 'direct-investment', 'fret', 'rancor', 'nervousness', 'devoted', 'briefing', 'journalists', 'vitally', 'emotions', 'Taizo', 'Watanabe', 'Fears', 'escalated', 'Coca-Cola', 'midtown', 'fires', 'discontent', 'stoked', 'Jr', 'oilman', '26.2', 'automotive-lighting', 'asserting', 'greenmailer', 'Texan', 'Lloyd', 'Bentsen', 'highlight', '*T*-185', 'disproportionate', 'table', 'litany', '*T*-186', 'clarified', '*T*-187', 'retort', 'concessions', 'exactly', '*T*-188', 'sorting', 'specifics', 'crossed', 'gauging', 'sheaf', 'Elisabeth', 'Rubinfien', 'improves', 'Laser', 'Wayland', 'sights', 'myriad', 'penetrate', 'tiny', 'joint-venture', 'guided', 'bureaucratic', 'maze', 'laser', '*T*-189', 'kidney', 'stones', '*T*-190', 'treats', 'lesions', 'count', 'Olsen', 'milked', '*-117', 'bankroll', 'promising', '*T*-191', '214', 'Heightened', '*-118', '*T*-192', 'penetration', 'low-tech', 'fancy', 'strategic', 'Glass', 'Warrenton', 'fabricator', 'architectural', 'foundering', 'chiefly', 'Ichiro', 'inside', 'Nissho-Iwai', '*T*-193', 'counterpart', 'vertically', 'feudal', 'globally', 'sogo-shosha', 'Takeshi', 'Kondo', 'Silicon', '*-119', 'trading-company', 'Strategic', 'logjam', 'small-company', 'Davies', 'Alliance', 'queuing', 'unsympathetic', '*T*-194', 'relation', 'generate', 'ai', '*-120', '*T*-195', 'fueling', 'airports', '*T*-196', '*-121', 'Langner', 'Genie', 'high-balance', 'pine', 'safe', 'competed', 'bundling', 'segmenting', 'NCNB', 'Connections', 'adults', 'pre-approved', 'saving', 'Planters', 'Memphis', 'Tenn.', 'Edge', 'thirtysomething', 'crowd', '*T*-197', 'borrowed', 'aiming', 'elderly', 'Judie', 'Jacksonville', 'sub-segments', 'tailoring', 'styles', 'Varying', 'life-style', 'sub-markets', 'athletic', '55-year-old', 'Senior', '75-year-old', '1973', 'Wells', 'Fargo', '*T*-198', 'safe-deposit', 'travelers', 'begot', 'slew', 'copycats', 'computerize', 'niches', '*T*-199', '*-122', 'mid-1970s', 'analyze', 'deregulation', '*T*-200', 'Deregulation', '*T*-201', 'high-rate', 'CDs', 'passbook', 'certificates', 'interest-bearing', 'staggering', 'Norwest', 'battles', 'worrying', 'money-center', '*T*-202', 'cultivated', 'savvier', '*T*-203', 'fragmentation', 'attracting', 'rate-sensitive', 'Packages', 'rewarding', 'captive', 'audience', '*T*-204', '*-123', 'loyal', '*T*-205', 'borrowers', 'savers\\\\/investors', 'Packaging', 'drawbacks', 'personnel', 'promotional', 'ChemPlus', 'comprehensive', 'flourish', 'tailored', '*-124', 'boutique', 'IRAs', 'SHAREDATA', 'amend', 'delete', 'resubmit', 'develops', 'low-cost', 'Five', 'Hawaiian', 'Send', 'diamond', 'necklace', 'Make', 'lasting', 'Parents', 'pocket', 'teetering', 'insolvency', 'needy', 'solvent', 'Performing', '*T*-206', 'doors', 'joy', '*T*-207', 'builds', 'self', 'sufficiency', 'critical', 'mid-size', 'semesters', '*T*-208', 'rap', 'sagging', 'morale', 'Baris', 'emergencies', 'reinstating', '150-point', '20-point', '*-125', '1:30', '3:15', '*-126', 'tumultuous', 'skidded', '*T*-210', '30-minute', '*-127', 'Melamed', 'lessen', 'reopened', 'subsequent', 'flood', '*T*-211', 'knocked', 'intermediate', 'synchronized', 'circuit-breaker', 'aggravated', 'directing', '*-129', '*T*-212', 'respite', 'sell-offs', 'maximum', 'modification', '*-130', 'lapses', '*T*-213', 'post-hearing', '*T*-214', 'comfortable', 'legislators', 'Breeden', 'breakers', '*T*-215', 'vague', 'mushy', 'viewpoints', 'angered', '*-131', 'sensitive', '*-132', 'happy', '*-133', 'annoyed', 'congressman', 'Fifteen', 'attended', '*T*-216', 'jurisdictional', '*T*-217', '*-134', '*T*-218', 'committees', 'change-ringing', 'peculiar', 'peculiarities', 'unintelligible', 'Nine', 'Tailors', 'ASLACTON', '*T*-219', 'evoke', 'loveliest', 'cascading', 'calling', 'faithful', 'evensong', 'parishioners', 'Angels', 'chat', 'rhythmically', '*T*-220', '1614', 'discordant', '*-135', 'church-goers', 'enjoying', 'cool', '*T*-221', 'herald', 'Derek', 'octogenarians', '*T*-222', 'sometimes-exhausting', 'sounding', 'belfries', 'Anglia', 'scrape', 'water-authority', 'discos', 'dances', 'drift', 'flightiness', 'diminish', 'Anglian', '*T*-223', 'pealing', 'History', 'nationwide', 'Sundays', 'tunes', 'carillons', 'continental', '*-136', 'childish', 'foreigners', 'Change-ringing', 'mind-boggling', '380', 'dexterity', 'concentration', 'Proper', 'rounds', 'highest-pitched', 'descending', 'altering', 'variation', 'memorize', '*T*-224', 'odd-sounding', 'Treble', 'Grandsire', 'Caters', 'Kensington', 'Ten', 'shirt-sleeved', 'circle', 'foot', 'prize-fighter', 'pulling', 'rope', '*T*-225', 'disappears', 'hole', 'chamber', 'snaking', 'muffled', 'Totally', 'stare', 'vision', 'rope-sight', 'pulls', 'Far', 'bronze', 'wheels', 'madly', '360', 'inverted', 'mouth-up', 'Skilled', 'wrists', 'retard', 'detective-story', 'novelist', 'finds', 'satisfaction', 'mathematical', 'completeness', 'perfection', 'filled', 'solemn', 'intoxication', '*T*-226', 'intricate', 'ritual', 'faultlessly', 'obsession', 'Pattenden', '*T*-227', 'stays', 'stuck', '*-137', 'sweat', 'skip', 'pub', 'sit', 'clerics', 'steadily', 'dwindling', 'pressing', 'non-religious', 'Rev.', 'Jeremy', '*T*-228', 'sacked', 'self-perpetuating', '*T*-229', 'premises', 'Ilminster', 'Somerset', 'dust-up', 'attendance', 'W.D.', 'refuses', 'C.J.B.', 'stairs', '*-138', 'altar', 'obvious', 'exit', 'prayer', 'feelings', 'bell-ringer', '*-139', 'fuller', 'aims', 'speak', 'theological', 'colleges', 'joys', 'booklet', 'entitled', 'Bells', 'attacking', 'bellringers', '40,000', '*-140', 'parishes', 'inner-city', 'lucky', 'male-dominated', 'bell-ringing', 'Ancient', 'Youths', '1637', 'male-only', '*T*-230', 'galling', 'sole', 'Cathedral', 'Westminster', 'Abbey', 'equal-opportunity', 'Red-blooded', 'balanced', 'frequency', 'fainting', 'tea', 'torrent', 'Solihull', '*-141', 'dressed', 'decorated', 'beer-belly', 'unwashed', 'unbearably', 'flatulent', 'Sheffield', 'faint', 'bless', 'unsettled', 'comfort', 'predictable', 'arrival', 'breathe', 'warn', 'trap', 'unwary', 'quantitative', 'Strong', 'widow', 'spiders', '*T*-231', 'males', 'mating', 'Income', 'robustly', 'Invariably', 'hospitable', 'cyclical', 'sliding', 'Payouts', 'smartly', '*T*-232', 'unenticing', 'exits', 'pushes', 'tanked', 'Always', 'Stockholders', '*T*-233', 'hint', 'escaped', 'debacle', '1933', '1961', '1968', 'troublesome', 'behaving', 'Philadelphia-based', 'co-chairman', 'tad', '*T*-234', 'single-digit', 'weakening', 'forecasting', 'slowdowns', 'Doerflinger', 'wherewithal', 'declare', 'Dividend', '*-142', 'bulls', 'expenditures', '139', '138', 'slippage', 'harbinger', '*T*-235', 'Having', 'supportive', 'element', 'upside', 'Alexander', 'Graham', 'invention', 'communication', 'father-in-law', 'Gardner', 'Hubbard', 'wealthy', 'well-connected', 'Emile', '*-144', 'princely', 'infringed', 'established', 'caveat', '*-145', 'enter', 'Troubled', 'discontinuing', '*T*-236', 'Altogether', '266', '176', 'structurally', 'radically', 'word-processing', 'Legend', '*T*-237', 'lacked', '93', '*T*-238', 'rapprochement', 'spoke', 'length', '*T*-239', 'afflicted', 'bloody', 'suppression', 'harped', 'outrage', 'massacre', 'Legal', '*T*-240', 'proponent', 'peaceful', 'seduce', 'socialist', 'capitalist', 'tension', 'evident', 'banquet', 'reciting', 'platitudes', 'eternal', 'friendship', 'reminded', 'Shangkun', '3-4', '*T*-241', '*T*-242', 'ordering', 'undiplomatic', 'fashion', 'killings', 'prominently', 'demonstrations', 'supreme', 'Deng', 'Xiaoping', 'Frankly', 'speaking', '*-146', 'deeply', 'counterrevolutionary', 'rebellion', '*T*-243', 'reprove', 'mend', '*T*-244', 'deteriorated', 'Chinese-American', 'dissident', 'Lizhi', 'Shuxian', 'refuge', 'Shortly', 'afterwards', 'anti-China', 'high-level', 'codified', '*-147', 'unofficial', 'envoy', 'Brent', 'Scowcroft', 'Saturday', 'top-level', 'participation', 'Fulbright', 'government-funded', 'pulled', 'acknowledge', 'infusion', 'Ideas', 'borders', 'SDI', 'weapon', '*T*-245', 'shoot', 'minor', 'Premier', 'Peng', 'hoped', 'encounter', 'guns', 'Sure', 'arrived', 'embassy', 'machine-gun-toting', 'ambassador', 'residence', 'encircling', 'discarded', 'Uzi-model', 'pistols', 'plainclothes', 'unmarked', 'soldiers', '*T*-246', 'diplomats', 'clicked', 'Firms', 'Lure', 'Science', 'Graduates', 'accusing', 'jeopardizing', 'barking', 'buck', '*T*-247', 'volunteer', 'unethical', 'higher-salaried', 'Unfortunately', 'impression', 'visited', '*-149', 'images', 'perceptions', 'salaries', 'Hiroshi', 'Asada', 'Barbara', 'Earns', 'Ratings', 'Than', 'regrettable', 'negatives', 'liberals', 'progressive', 'Prof', 'Ethel', 'Klein', '76', '*T*-248', 'disapprove', 'spouse', 'imply', 'three-quarters', 'distasteful', 'newsworthy', 'perpetuates', 'insidious', 'stereotyped', 'perspective', 'defined', 'denominator', 'Preston', 'Birmingham', 'Ala', 'self-regulatory', 'disciplined', '*-150', '*-151', 'Marina', 'del', 'Rey', '*-152', '*-153', 'telephone-information', 'I.', 'improper', '*-154', '*T*-249', '*-155', 'Lauderhill', 'Plantation', '*T*-250', '*-156', 'Delwin', '*-157', 'Mount', 'Clemens', 'inaccurately', 'Weatherly', 'Keehn', 'Northy', 'Prater', 'Mercer', 'Wash.', 'Reached', 'implication', 'timing', 'rectified', 'W.N.', 'N.', 'differ', 'markup', '*T*-251', 'timely', 'requests', 'Except', 'Derel', 'Adams', 'Killeen', 'Angier', 'Reddington', 'Shores', 'Stirlen', 'Bonnell', 'Boorse', 'Horsham', 'Chiodo', 'Camille', 'Chafic', 'Cotran', 'Colonsville', 'Dompierre', 'Valrico', '16,072', 'Marion', 'Stewart', 'Spitler', '18,444', '*T*-252', 'complaining', 'anybody', 'Fishman', 'Longwood', 'Floyd', 'Amin', 'Jalaalwalikraam', 'Glenham', 'Knapp', 'Deborah', 'Renee', 'Muscolina', 'Palisades', 'Najarian', 'Minn.', 'Norwick', 'Nesconset', 'Phipps', 'Sr.', 'Rankin', 'Mo.', 'Leigh', 'Sanderoff', 'Gaithersburg', 'Md.', '12,252', 'Sandra', 'Ridgefield', 'Spence', 'Aloha', 'Mona', 'Estates', 'Swearingen', 'Bew', 'Wong', 'Rabia', 'Zayed', 'Veselich', 'Enright', 'Rolling', '11,762', 'Stuart', 'Russel', 'Glendale', '14,821', 'Nilson', 'Fountain', '82,389', 'screwed', 'breaking', '*-158', 'reps', 'security-type', 'mistakes', 'Cole', 'Jackson', 'Rita', 'Rae', 'Cross', 'Denver', 'Meinders', 'five-day', 'eight-month', 'La.', 'Karl', 'Grant', 'Hale', 'Midvale', 'Utah', 'Clinton', 'Hayne', 'one-week', 'Coconut', '250,000', 'Merrick', 'Pace', '90-day', 'Brian', 'Pitcher', 'Russo', 'Bridgeville', '15-day', 'Orville', 'Leroy', 'Sandberg', 'Marchese', 'Eric', 'Monchecourt', 'Gerhard', 'Carson', 'fond', 'blocked', '*-159', 'emigrate', 'hurdles', 'loom', 'onslaught', 'shrug', 'overlap', 'American-style', '*T*-253', 'exploit', '11.6', 'imagine', 'racing', 'Chicago-style', '*-160', 'Makato', 'Utsumi', 'home-market', 'Osaka', 'forgotten', 'leap', 'bout', 'foreign-led', 'skyward', '*T*-254', 'mechanisms', 'tightened', 'index-related', 'catch-up', 'reaped', '*T*-255', 'Deryck', '*T*-256', 'Wadsworth', '*T*-257', 'ascribe', 'futures-related', 'disruptive', 'liquid', 'Index-arbitrage', 'serves', 'conduit', 'counter', 'tapes', 'unwind', '*-161', 'Barfield', 'Assurance', '*T*-258', 'manages', '23.72', '*-162', 'Traded', 'tenfold', '9,118', '4,645', '917', 'index-options', 'derivatives', '382-37', 'replete', 'Advocates', '90-cent-an-hour', 'Republicans', 'bend', '*T*-259', 'lifting', 'four-year', '2.65', '3.80', 'smiles', 'Mont', '*-164', 'Marge', 'administrations', 'Adopting', 'training-wage', 'diming', 'Policy', 'lately', 'touted', 'impart', 'skills', 'entrants', 'fought', 'acceded', 'insistence', '3.61', 'Employers', '*NOT*', 'government-certified', '*T*-260', 'unrestricted', '*-165', 'mininum-wage', 'employers', 'elimination', '534', 'microcomputers', '84-month', '130.7', 'AC-130U', 'Marietta', '*-166', '29.9', 'low-altitude', 'navigation', '29.4', 'mode', 'gentle', 'Can', 'hard-charging', 'Teddy', 'Roosevelt', '62-year-old', 'forest-product', 'unsolicited', '3.19', 'entice', 'negotiating', 'surrender', 'opens', 'dilemma', 'Given', 'overpaying', 'courage', 'A.D.', 'long-time', 'Griffin', 'WTD', 'picked', 'Polytechnic', 'universities', 'Strother', 'researching', 'willingness', 'arrest', 'occupying', 'impressed', 'fundraising', 'enticed', 'befuddled', 'demonstrating', 'raw', 'possessed', 'skipped', 'classmates', 'Moving', 'graduated', 'Phi', 'Beta', 'Kappa', 'Kentucky', 'doctorate', 'retentive', 'understatement', 'photographic', 'engineered', 'inherited', 'recession-inspired', 'building-products', 'non-core', 'vinyl', 'checkbook', 'refocusing', 'remodeling', 'cycles', 'formula', 'reins', '467', 'attributes', 'philosophy', 'concentrating', 'impressive', 'diversification', 'high-quality', 'Kathryn', 'McAuley', 'Overseas', 'contrasts', 'provoked', 'shaping', 'authorizing', 'decisive', 'reallocate', 'Pentagon', 'separately', '220', 'Population', 'warming', 'elephant', 'draws', 'airplane', 'intriguing', 'stripped', 'Noriega', 'regime', '30,537', '21-month', 'reallocated', '23,403', 'growers', '9.3', 'Majority', 'Whip', 'Pa', 'English-speaking', 'Barbados', 'Californian', 'Bolivia', 'broadened', 'initiate', 'upsetting', 'allies', 'instructed', 'lobbyist', 'drafted', 'insert', 'waived', 'supplemental', 'anti-drug', '27.1', 'bounce', 'departments', 'Beauty', 'Takes', 'Backseat', 'Bridges', 'EVERYONE', 'AGREES', 'repaired', 'ornamental', 'crashing', 'scenic', 'planner', 'prefer', 'four-foot-high', 'slab', 'Ind.', 'arched', 'G', 'Garret', 'teaches', 'Earlham', 'ugly', 'Charter', 'Oak', 'cast-iron', 'medallions', 'Compromises', 'Citizens', 'Peninsula', 'floral', 'highway', 'sidewalk', 'Tray', 'Bon', 'Drink', 'Carrier', 'Competes', 'Cartons', 'PORTING', 'POTABLES', 'Scypher', 'Cup-Tote', 'beverage', 'resembles', 'beer', 'web', 'tote', 'cups', 'Inventor', 'Claire', 'spilling', 'Lids', 'carriers', 'acknowledges', 'driver', 'sunlight', 'recyclable', 'Spirit', 'Perestroika', 'Touches', 'Design', 'AN', 'EXCHANGE', 'gamut', 'blender', 'chairs', 'Leningrad', 'Mutchin', 'learn', 'corkscrews', 'maybe', 'Seed', 'Jail', 'Solution', 'Fails', 'Root', 'IT', \"'S\", 'TWO', 'BIRDS', 'architects', 'propose', 'prisoners', 'overcrowding', 'solutions', 'Grain', \"'30s\", 'walls', 'semicircular', 'cells', 'muster', 'visits', 'aesthetic', 'famed', 'altered', 'inmates', 'upstate', 'workplace', 'mill', 'crane-safety', 'citation', 'coke', 'electrical-safety', 'indifference', 'counteract', 'Particularly', 'flagrant', 'properly', 'spite', 'corporate-wide', 'evaluation', 'cooperating', 'corrected', 'promised', 'stiffer', 'unwilling', 'manpower', 'removing', 'safeguarding', 'Anku', 'contest', 'Review', 'Morrell', 'meatpacking', 'Brands', 'contesting', 'editing', 'error', 'Hallett', 'mistakenly', 'NRDC', 'Natural', 'implied', 'substance-abusing', 'alcoholics', 'quoting', 'emphasized', 'prevalance', 'alcoholism', 'multitude', 'malnutrition', 'chest', 'cardiovascular', 'infectious', 'aftereffects', 'assaults', 'elementary', 'necessities', 'nutrition', 'cleanliness', 'predispose', 'consequence', 'composed', 'adequate', 'interactions', 'defying', 'generalizations', 'possess', 'Breakey', 'Fischer', 'Psychiatry', 'Johns', 'Hopkins', 'Tulane', 'array', 'thread', 'exhibits', 'simultaneously', 'housing', 'disaffiliation', 'welfare', 'decay', 'intimately', 'Leighton', 'Cluff', 'quote', 'drop-in', 'robbed', 'deprivation', 'scarcely', 'fend', 'pre-existing', 'addiction', 'cracks', 'grim', 'brutal', 'escape', 'insanity', 'R.D.', 'Vos', 'N.Y', 'dismiss', 'sentimental', 'housing-assistance', 'sleeping', 'Reagan-Bush', 'bothered', 'inverse', 'Jenkins', 'sponsors', 'chose', 'Builders', 'Bricklayers', 'Craftsmen', 'insinuating', 'self-serving', 'crusade', 'desire', 'YMCA', 'YWCA', 'Catholic', 'USA', 'participated', 'examinations', 'Choose', 'deprived', 'families', 'substitute', 'Chivas', 'Regal', 'phobias', 'depressions', 'Ruth', 'Cullowhee', 'ROGERS', 'COMMUNICATIONS', '148.9', '153.3', 'perpetual', 'Perpetual', 'retractable', 'Rogers', 'B', 'cable', 'undesirable', 'intrusion', '300-113', 'override', 'brakes', 'impede', 'Ark', 'Works', 'impaired', 'airline-related', 'overriding', 'DeFazio', 'criteria', 'Traficant', '271-147', 'dubbed', 'labor-backed', 'two-time-losers', 'Lorenzo', 'Follow-up', 'broadcasting', 'scholar', 'ordinary', 'cousins', 'hither', 'yon', 'outrageous', 'propagandize', 'neat', 'propagandizes', 'speeches', 'briefings', 'sorts', 'Propaganda', 'viewing', 'absurd', 'inform', 'columns', 'clipped', 'refrigerator', 'languages', 'listeners', 'first-rate', '184', '*-30', 'copying', 'photocopying', 'short-wave', 'transcribe', 'Nor', 'reprint', 'happened', 'absolute', 'disseminate', 'scholarly', 'memo', 'preclude', 'disseminating', 'domestically', 'mentioned', 'notwithstanding', 'statutory', 'designations', 'credentials', 'appearing', 'requesting', 'examine', 'verbatim', 'disagreed', 'proscribes', 'abridging', 'prescribe', 'assure', 'laboriously', 'surreptitiously', 'public-relations', 'sends', 'stuff', '501', 'Educational', 'thanks', 'photocopy', 'trivial', 'Z.', 'Wick', 'Gartner', 'Tribune', 'Ames', 'wield', 'Herbert', '53-year-old', 'Edwin', 'dividing', 'searching', 'food-industry', 'reacted', 'favorably', 'distant', 'foods', 'Freshbake', 'biscuit', 'Lazzaroni', 'overproduction', 'skill', 'seasoned', 'rapport', 'mediocre', 'heirs', '343', '3,600', 'impatient', 'CEO', 'succession', 'repeatedly', '877,663', '244,000', 'fringe', 'nine-year', '5.7', '274', 'advocated', 'convince', 'worthiness', 'tremendous', 'duo', 'bottom-line', 'applaud', 'exuded', 'champions', 'sitting', 'guide', 'FUNDS', 'overnight', 'Fulton', 'Prebon', 'U.S.A', 'DISCOUNT', 'depository', 'CALL', 'MONEY', 'collateral', '119', '149', '7.80', '7.55', 'High-grade', 'multiples', '8.65', '8.575', 'CERTIFICATES', 'DEPOSIT', '8.06', 'negotiable', 'C.D.s', 'Typical', '8.60', '8.35', 'BANKERS', 'ACCEPTANCES', '8.48', '8.30', '8.15', 'Negotiable', 'bank-backed', 'LATE', 'EURODOLLARS', '13\\\\/16', '11\\\\/16', 'INTERBANK', 'OFFERED', 'LIBOR', 'quotations', 'FOREIGN', '13.50', '4.875', 'indications', 'TREASURY', 'BILLS', 'Results', '7.78', '7.62', 'HOME', 'LOAN', 'CORP', 'Freddie', 'Mac', '9.82', 'NATIONAL', 'ASSOCIATION', '9.75', '8.70', '6\\\\/2', 'MERRILL', 'LYNCH', 'READY', 'ASSETS', 'TRUST', '8.64', 'Annualized', 'clashed', 'S.I.', 'Advance', 'Abrupt', 'departures', 'unheard', 'evolved', 'Si', 'gut', 'brilliantly', 'enjoyed', 'spectacular', 'smoothly', 'Bennett', 'Cerf', '1925', 'Possible', 'Ballantine\\\\/Del', 'Rey\\\\/Fawcett', 'paperback', 'Cheetham', 'Century', 'Hutchinson', 'powerhouse', 'Gottlieb', 'Yorker', 'most-likely-successor', 'Joni', 'recruited', 'Sonny', 'less-than-brilliant', 'tall', 'intimidate', 'uttering', 'Provided', 'guarding', 'species', 'predicated', 'erroneous', 'Consequence', 'omnipresent', 'conceivable', 'Iran-Contra', 'broadly', 'construed', 'emasculate', 'swallow', 'Constitutional', 'Convention', '1787', 'ensure', 'accountability', 'unitary', 'Articles', 'technically', 'limitation', 'leash', 'deliberating', 'breathtaking', 'containing', 'alternatively', 'intrusions', 'void', 'Advice', 'Consent', 'appoint', 'ambassadors', 'empowers', 'Power', 'Vacancies', 'Recess', 'granting', 'Commissions', 'End', 'Session', 'appropriation', 'repeals', 'imposes', 'nominee', 'Anti-Deficiency', 'voluntary', 'muzzling', 'recommendation', 'Muzzling', 'blindfold', 'recommending', 'discretion', 'select', 'inquiring', 'market-oriented', 'Probably', 'egregious', 'proviso', 'Office', 'Budget', 'subjecting', 'cost-benefit', 'inherently', 'prohibiting', 'wasted', 'illustrates', 'usurp', '609', 'executive-office', 'administer', 'disapproved', '*-58', 'disapproval', 'accordance', 'applicable', 'one-house', 'bicameral', 'presentation', 'signature', 'INS', 'Chadha', 'vetoes', 'invite', 'purposes', 'custom', 'undo', 'then-Speaker', 'Mikhail', 'ratified', 'SALT', 'unworkable', 'unfunded', 'assert', 'restricting', 'expressly', 'Excision', 'riders', 'trespass', 'prerogative', 'context', 'characterized', 'objectionable', 'conflict', 'applicability', 'item', 'exerting', 'downside', 'asserts', 'sue', 'loses', 'Morrison', 'Olson', 'electorate', 'valuable', 'civics', 'presumes', 'Federalist', 'legislature', 'impetuous', 'vortex', 'Sidak', '57.7', 'deducting', 'gross', 'saved', 'reclaim', 'mortgaged', 'price-support', 'soaring', 'reclaimed', '240-page', 'parched', 'profited', 'hardest-hit', 'Dakotas', 'disaster-assistance', 'confirms', 'depression', 'helps', 'reluctance', 'lobbies', 'curtailed', 'land-idling', 'price-depressing', 'surpluses', 'strengthened', 'Keith', 'livestock', 'cattle', 'inventory', 'agriculture', '3.4', 'log', 'disaster', 'farms', 'cultivation', 'soybeans', 'cushioned', '14.5', 'Payments', '238,000-circulation', '700,000', 'one-newspaper', 'senses', 'dominates', '300,000', '170,000', 'Nearby', 'Pasadena', 'McCabe', 'materialized', 'stream', 'tire-kickers', 'lookee-loos', 'newsstand', 'freeway', 'inevitable', 'sprawling', 'balkanized', 'mammoth', 'limbo', 'torn', 'old-time', 'readership', 'blue-collar', 'sports-oriented', 'sprightly', 'staid', 'flirted', 'News-American', 'folded', 'Herald-American', 'cornerstones', 'fanciful', 'Julia', 'castle', 'Simeon', 'Spanish', 'Renaissance-style', 'survivor', 'bygone', 'Kendrick', 'Noble', 'Actually', 'looks', '1903', 'decade-long', '1967', 'Financially', 'moments', 'Bellows', 'brightened', 'Dolan', 'restored', 'limping', 'accomplishments', 'notable', 'much-larger', 'disclosures', 'coverage', 'arts', 'criticism', 'Danzig', 'Newspapers', '730', 'long-tenured', 'Hours', 'representatives', 'recruiting', 'emotional', 'crying', 'L.A.', 'headline', 'beers', 'drunk', 'Andy', 'Furillo', 'Outside', 'pressman', 'headlined', 'Closes', 'Forget', 'handed', 'Olympia', 'Broadcasting', '23.4', 'radio-station', 'programmer', 'lenders', 'slogan', 'beleaguered', 'boosting', 'four-day', 'explaining', 'disclose', 'approached', 'doctor', 'broaden', 'Traditionally', '*-66', 'entertainment', 'Phillip', 'car-care', 'truth-in-lending', 'billing', 'marketers', 'capability', 'screened', 'card-member', 'incomes', 'missed', 'preapproved', 'Visa', 'cards', 'tie-in', 'Express-Buick', 'accommodations', 'meals', 'Destinations', 'Honolulu', 'Orlando', 'destination', 'companion', 'lieu', 'clock', 'stereo', 'recorder', 'Card', 'sweepstakes', 'test-drive', 'calculator', 'travel-related', 'round-trip', 'Trans', 'borough', 'Borough', 'gilt', 'recovering', 'Gilts', 'retraced', 'soured', 'accounted', 'sterling', 'betting', 'Hazell', 'auditor', 'vested', 'persuasive', 'solicitor', 'Already', 'Barclays', 'Midland', 'Citibank', 'avenues', 'illegality', 'arrangements', 'gut-wrenching', '190-point', '1,400', 'letter-writing', 'quashing', 'wrath', 'minicrash', 'shaken', 'resentment', 'lightning-fast', 'reeling', 'reap', 'academics', 'exacerbated', 'ascendency', 'consisting', 'wizards', 'immense', 'pools', 'Defending', 'ramparts', 'stock-picking', 'tens', 'clannish', 'successfully', 'mobilizing', 'bludgeon', 'tormentors', 'layer', 'gigantic', 'crapshoot', 'broad-based', 'livelihood', 'palace', 'revolt', 'Wohlstetter', 'rallying', 'anti-program', 'countless', 'universally', 'Kill', 'consistent', 'wedded', 'waited', 'sneaked', 'conceding', 'headed', 'curbed', 'civil', 'contradictions', 'pitting', 'floors', 'premier', 'entrenched', 'tooth', 'nail', 'facilitate', 'Again', 'theme', 'greedy', 'manipulators', 'shambles', 'free-enterprise', 'gambling', 'den', 'odds', 'stacked', 'Off-Track', 'Betting', 'portray', 'old-fashioned', 'Neanderthals', 'witches', 'boogieman', 'unknown', 'beg', 'momentary', 'divergence', 'constitute', 'whichever', 'seconds', 'razor-thin', 'profess', 'despise', 'frightened', 'lotter', 'Zicklin', 'iota', 'Hans', 'unraveling', 'Unable', 'unload', 'takeover-stock', 'arbitragers', 'Del', 'Signore', 'speculator', 'arbs', 'overleveraged', 'apart', 'traditionalists', 'bundles', 'derisively', 'jockeys', 'Traditional', 'manage', 'benchmarks', 'old-style', 'juggle', 'pennies', 'pension-fund', 'automated', 'threatens', 'dinosaurs', 'stock-specialist', 'monopoly', 'knell', 'nonetheless', 'striving', 'printers', 'spooked', 'dismayed', 'stacking', 'deck', 'scaring', 'Raymond', 'Stockbrokers', '71,309', 'resent', '*-75', 'nameless', 'sweatshirts', 'sparkplugs', 'Oh', 'bloods', 'publicity', 'orchestrated', 'hunker', 'lynch-mob', 'cow', 'gored', 'proven', 'fastest-growing', 'minted', 'millionaires', '20s', '30s', 'thunder', 'Sit', 'jeopardy', 'unlikely', 'genie', '*-77', 'Short', 'middle-ground', 'enjoy', 'good-faith', 'potentially', 'chase', 'Crash', 'protects', 'relentlessly', 'destroy', 'efficiency', 'Fundamentalists', 'stock-price', 'band-wagon', 'picks', 'impetus', 'practiced', 'locations', 'leveraging', 'owning', 'cheapest', 'vast', 'hysteria', 'arise', 'cumbersome', 'desires', 'evolve', 'creature', 'evoking', 'curses', 'implemented', 'unneeded', 'harmful', 'Reducing', 'sufficient', 'responds', 'initiating', 'functioning', 'fundamentally', 'hypothetical', 'sacrifice', 'subtraction', 'finite', 'Eliminate', 'loathsome', 'tablets', 'Encouraging', 'labeling', 'Policies', 'akin', 'Buyers', 'please', 'sufficiently', 'annualized', 'deviation', 'Periods', 'advent', 'undergoing', '1973-75', '1937-40', '1928-33', 'scream', 'hailing', 'abounding', 'Could', 'deeds', 'goblins', 'fixes', 'whipping', 'boy', 'wooing', 'stock-selection', 'bringing', 'damaging', 'Getting', 'abolishing', 'merits', 'championing', 'loudest', 'cater', 'advise', 'amass', 'somehow', 'noble', 'studiously', 'devouring', 'clippings', 'Almost', 'guy', 'invariably', 'spreads', 'mutual-fund', 'sad', 'performers', 'grapple', 'Cost-effective', 'sexy', 'roadblock', '*-82', 'high-volume', 'Legislating', 'Left', 'Spreads', 'temporary', 'watchdogs', 'friction', 'two-tiered', 'taxation', 'etc.', 'loser', 'inviting', 'transfer', 'executes', 'Brooks', 'augment', 'Rill', 'Antitrust', 'Division', 'appropriated', 'Hart-Scott-Rodino', 'notify', 'completing', 'Proponents', 'Don', 'stifle', 'staffs', 'drastically', 'dismal', 'inhibit', 'Though', 'acquirers', 'noticed', 'FALL', 'BALLOT', 'ISSUES', 'off-year', 'Odd-year', 'ratcheting', 'referenda', 'Democracy', 'citizen-sparked', 'ballots', 'odd-year', 'Maine', 'initiative', 'missiles', 'Dakota', 'Schmidt', 'Citizen', 'cue', 'Magleby', 'Brigham', 'PHOTOGRAPH', 'COLLECTING', 'Christie', 'folio', 'Equivalents', '396,000', 'single-lot', 'variables', 'documented', 'anecdotal', 'Gates-Warren', 'Sotheby', 'museums', 'Persky', 'Photograph', 'Collector', 'masters', 'fetching', 'Miles', 'Barth', 'Photography', 'DIALING', 'million-a-year', 'Joel', 'Gross', 'caller', 'celebrity', 'chatter', 'horoscopes', 'Andrea', 'tutorials', 'Colleges', 'eyeing', 'merchandising', 'migrate', 'predicts', 'Lawless', 'US', 'Sprint', 'FAMILY', 'PETS', 'recovery', 'Milwaukee', 'Patients', 'canine', 'feline', 'appetite', 'receptive', 'therapy', \"O'Loughlin\", 'coordinator', 'TIRED', 'TRIMMING', 'Hammacher', 'Schlemmer', 'fiber-optic', 'Christmas', 'string', '6,500', 'continuously', 'colored', 'fiber-end', 'bunches', 'MEDICINE', 'TRANSPLANT', 'prompts', 'bilingual', 'Funded', 'Tokio', 'protocols', 'preventative', 'DIAPER', 'SERVICES', 'comeback', 'Concerned', 'shrinking', 'landfills', 'super-absorbent', 'disposables', 'Tiny', 'Tots', '1,200', 'Mogavero', 'Piscataway', 'Syracuse', 'DyDee', 'stresses', 'awareness', 'day-care', 'spurned', '672', 'inquiries', 'Elisa', 'Hollis', 'shortages', 'Stork', 'Springfield', 'spurring', 'Velcro', 'BRIEFS', '57.6', 'Yorkers', 'viewership', 'Impact', 'Columbus', 'FreudToy', 'pillow', 'likeness', 'Sigmund', 'Freud', '24.95', 'tool', 'do-it-yourself', 'doubts', 'echoed', 'heading', 'blames', 'overvalued', 'interior', 'decorator', 'spook', 'deviant', 'curbing', 'darned', 'Schwab', 'Buckhead', 'skepticism', 'Citing', 'Anderson', '59-year-old', 'fluctuations', 'heebie-jeebies', 'outlawing', 'Wamre', '31-year-old', 'disappear', 'dealing', 'decries', 'strictly', 'capitalism', 'adapting', 'Britta', '25-year-old', 'factoring', 'rigors', 'Silverman', 'insurance-company', 'culprit', 'Arbitraging', 'leery', 'Enzor', 'defends', 'accountant', 'recouped', 'flim-flammery', 'storm', 'Lucille', '84-year-old', 'housewife', 'amazingly', 'jolts', 'hunted', 'bargains', 'sky', 'wholesaler', 'spirits', 'Underwoods', 'Researchers', 'amps', 'centimeter', 'yttrium-containing', 'liquid-nitrogen', 'temperature', '321', 'Fahrenheit', 'journal', 'Nature', 'wires', 'motors', 'magnets', 'generators', 'Scientists', 'Superconductors', 'ceramic', 'technologies', 'generation', 'aspect', 'practical', 'bombarding', 'neutrons', 'radioactivity', 'large-scale', 'breathed', 'collective', 'sigh', 'demonstrates', 'flux', 'pinning', 'undercutting', 'Determining', 'enable', 'combine', 'melt-textured', 'Walbrecher', 'Francisco-based', '1st', 'Nationwide', 'Fidelity', 'versus', '5.3', '1.61', 'write-downs', '4.9', '45.75', 'offerings', 'pricings', 'non-U.S.', 'syndicate', '8.467', 'bellwether', 'Rated', '1991-2000', '81.8', '7.20', 'insured', 'triple-A-rated', 'A-D', '1991-1999', '7.422', '7.15', 'single-A', 'Housing', '80.8', '30.9', '1992-1999', 'Fourth', 'serial', '7.45', '7.50', '49.9', '7.65', 'double-A', 'Heiwado', 'equity-purchase', 'Intecknings', 'Garanti', 'Aktiebolaget', 'Sweden', '6.03', 'Handelsbanken', 'Takashima', 'Yamaichi', '3.43', 'Pencil', '106', '3.42', 'Koizumi', 'Sangyo', '1996', 'Schweiz', 'Guarantee', 'Lurie', 'avid', 'fan', 'digs', 'Pretty', 'afford', 'major-league', 'Something', 'ranging', 'Petersburg', 'complexes', 'moneymakers', 'Pepperdine', 'Dean', 'Baim', 'scoffs', 'looked', 'Dodger', 'Stadiums', 'redistribute', 'Voters', 'mega-stadium', 'Phoenix', 'thumbs', 'fielded', 'concede', 'polls', 'endorse', 'Dolphins', 'disagrees', 'city-owned', 'Bowl', 'coliseum', 'Moon', 'Landrieu', 'cavernous', 'money-losing', 'relevance', 'faith', 'Egyptian', 'Pharaoh', 'justified', 'civilization', 'schemes', 'pharaohs', 'erect', 'playgrounds', 'passions', '89.7', '141.9', '94.8', '149.9', '130.6', 'Pretax', '128', '133', '135', '388', '722', 'Risks', 'erodes', 'downgrading', 'shivers', 'shudders', 'issuers', 'IOUs', 'borrowings', 'difficulty', 'shoring', 'credit-rating', 'Whereas', 'structured', 'penchant', 'stretch', 'disarray', 'downgraded', 'Catch-22', 'Merchant', 'riskier', 'acquires', 'expectation', 'generated', 'overcapacity', 'managements', 'reliance', 'arranged', 'high-risk', 'Mattress', 'bedding', 'Suisse', 'participant', 'rebounding', 'overstated', 'disadvantage', 'rivals', 'toll', 'settlements', 'plea', 'felonies', 'insider-trading', 'workforce', 'Recently', 'circulated', 'moreover', 'confident', 'creditworthiness', 'vicissitudes', 'Sable', 'experiencing', 'creator', 'car-development', 'eclipse', 'top-selling', 'Team', 'involvement', 'assembly-line', 'cycle', 'responsive', 'demands', 'new-car', 'Cougar', 'American-made', 'parts-engineering', 'absurdity', 'stretched', 'notch', 'compassion', 'Solomonic', 'stark', 'policy-making', 'innovation', 'compelling', '1940s', 'hormone', 'diethylstilbestrol', 'miscarriages', 'sickness', 'generic', 'labels', 'mothers', 'thousand', 'recall', 'brand', 'Beginning', '1980', 'common-law', 'Courts', 'pills', 'assessed', 'proportion', 'duck', 'identical', 'slippery', 'slope', 'alike', 'differently', 'warranties', 'Lilly', 'Mindy', 'Alstyne', 'apple', 'apples', 'uncompensated', 'takings', 'twisting', 'doctrine', 'reversed', 'chilled', 'prescription', 'hidden', 'favors', 'accompany', 'pain', 'unanimous', 'anti-morning-sickness', 'Bendectin', 'Huber', 'introduces', 'anti-miscarriage', 'tort', 'lousy', 'anyway', 'contingency-fee', 'vaccines', 'predictably', 'Everyone', 'understands', 'utterly', 'incapable', 'deserving', 'billion-dollar', 'morass', 'trash', 'Odyssey', 'Norwalk', 'N.V', '235', '113.2', '6.70', '144', '4.10', 'outlay', '2.46', '2.42', 'PhacoFlex', 'intraocular', 'foldable', 'silicone', 'len', 'foldability', 'inserted', 'incisions', 'Cataracts', 'refer', 'clouding', 'EC', 'harmed', 'Salvador', 'intentioned', 'wrecking', 'incentives', 'smothering', 'kindness', '7.8', '13.625', 'Wis.', '4.5', 'truck', '1.92', '352.9', 'softer', 'motor-home', 'Deere', '132', '14.99', '35564.43', '63.79', '35500.64', '909', 'Declining', 'outnumbered', '454', '451', 'incentive-backed', 'untrue', 'directionless', 'Institutions', 'Topix', '16.05', '1.46', '0.05', '2691.19', '6.84', '5.92', '0.16', '3648.82', 'Nomura', 'clearer', 'comfortably', 'Tokyu', 'dominant', 'Yasuda', '56', '1,880', '13.15', 'continuingly', 'directed', 'mining', 'Teikoku', 'stimulated', '1,460', 'Showa', 'Shell', '1,570', 'Sumitomo', 'Metal', '692', '960', 'winners', 'Shokubai', '2,410', 'Marubeni', '890', 'affecting', '17.5', '2160.1', 'intraday', '2141.7', '2163.2', 'tempted', 'overdone', 'reversal', 'FT', '30-share', '1738.1', '372.9', '334.5', 'hugging', '879', '13.90', 'shed', 'waive', 'protective', 'golden', 'waiver', 'Goldsmith', 'Investments', '753', 'sweeten', '397', 'shopping', 'Carlton', '778', 'notched', 'searched', 'qualities', 'Wellcome', '666', 'Glaxo', '14.13', 'Amsterdam', 'Zurich', 'Paris', 'Brussels', 'Milan', 'Wellington', 'Sydney', 'Manila', 'calculated', 'Perspective', 'Geneva', '1969', 'equaling', 'Intermec', '1,050,000', 'Piper', 'Jaffray', 'Hopwood', 'Middlesex', 'Water', '150,000', 'Walker', 'Howard', 'Weil', 'Labouisse', 'Friedrichs', 'Midwesco', 'Filter', '830,000', 'Nylev', 'Occidental', 'shelf', 'Inns', 'zero', 'Montgomery', 'Fracturing', 'Lovett', 'Mitchell', 'Webb', 'Garrison', 'Blunt', 'Ellis', 'Loewi', '3,250,000', '3,040,000', '210,000', 'Funding', 'Hanifen', 'Imhoff', 'Hold', 'Putty', 'lipsticks', 'liners', 'lotions', 'creams', 'tackle', 'paint', 'spackle', \"D'Amico\", 'Diceon', 'Chatsworth', 'charging', 'Calif.-based', 'circuit-board', 'disposed', 'caustic', 'sewer', 'leaky', 'unlabeled', 'open-top', 'containers', 'Named', 'Roland', 'Matthews', 'Jonas', 'inspection', 'Arraignments', 'stayed', 'unsettling', 'Fabian', 'Survey', 'industry-supported', 'Confidence', '116.4', 'barely', 'revised', '116.3', '116.9', '112.9', '120.7', '18.3', 'worsen', 'Fewer', '21.1', '16.9', '17.4', '18.6', '28.5', 'day-to-day', 'Unemployment', 'paycheck', 'reasonably', 'conducted', 'Opinion', '6.7', '30.6', '27.4', '26.5', 'deluge', 'abuzz', 'Kathleen', 'Camilli', 'fixed-income', 'rare', 'Ednie', 'ranked', 'infrequent', 'Syndicate', 'scant', 'Whether', 'Hanover', 'blurred', 'Open', '14.', 'varied', 'industries', 'diverse', 'enhances', 'importance', 'non-farm', '152,000', '29year', 'nine-month', '12\\\\/32', '16\\\\/32', '7.79', '7.52', '7.60', 'compilation', 'compare', 'topped', '16.5', 'Junk', 'risk-free', 'outdistanced', 'Lonski', 'wanting', 'Mortgage-Backed', '9.32', '12-year', 'Activity', 'derivative', '114', 'Facilities', 'Financing', 'Authority', 'relaunched', '107.03', 'repriced', 'Bucking', 'drew', 'unenthusiastic', 'Rumors', 'Bund', '1998', '95.09', '5.435', '2003\\\\/2007', '14\\\\/32', '10.19', '1995', '9\\\\/32', '103', '11.10', 'double-C', 'triple-C', 'clothing', 'expense', 'exceeds', '94.2', '83', 'pre-tax', '306', '415', 'underperforming', 'Monopolies', 'Mergers', 'tires', '221.4', 'S.A', 'referral', '420', 'centerpiece', 'chaos', 'single-handed', 'seven-million-ton', 'cap', 'hailed', 'despised', 'innovative', 'market-based', 'polluters', 'subsidize', 'clean-up', 'coal-fired', 'sparing', 'exorbitant', 'jumps', 'sticking', 'vow', 'avoiding', 'staunchly', 'appease', 'high-polluting', 'cleanup', 'burn', 'cleaner-burning', 'fuels', 'quietly', 'tinker', 'Journals', 'resign', 'relocate', 'co-founded', 'Mo.-based', 'E.W.', 'Scripps', 'nickname', 'coordinate', 'deliberately', 'disconnect', 'heightened', 'routes', 'scans', 'resumes', 'DOT', 'Designated', 'Order', 'Turnaround', 'high-speed', 'Used', 'zip', 'handles', 'Obligations', 'instrument', 'agreed-upon', 'nullified', 'opposite', 'Indexing', 'barometer', 'swapping', 'simultaneous', 'Quant', 'Generally', 'quantitive', 'newest', 'breed', 'rocket', 'backgrounds', 'hedging', 'popularly', 'fleeting', 'arbitrager', 'multiplying', '20-stock', 'mimics', 'chicago', 'certin', 'indexes', 'Uptick', 'expression', 'signifying', 'Saul', 'puzzled', 'definitely', 'Airways', '110', '282', 'twist', '1.50', 'notified', 'tapping', 'Tiger', 'acquirer', 'pilots', 'machinists', 'cleared', '472', 'delisted', '23.25', '28.25', 'Duluth', 'liabilities', 'Trace', 'unaffiliated', '3.625', 'Asher', '16.2', 'computer-services', 'oust', 'Datapoint', 'explore', '2.75', 'above-market', '18-a-share', 'p.m.', 'EST', '576', '95', '24,000', '19.50', 'Barron', 'printing-press', 'trail', 'heavy-truck', 'passenger-car', '630.9', '126.1', '132.9', 'prior-year', 'adjustment', 'bomber', 'sewing-machine', '185.9', '3.28', '3.16', 'Overall', '107.9', '96.4', 'emerged', 'muscling', 'singled', '79', '42.1', 'Colorliner', 'newspaper-printing', 'Aerospace', 'sagged', 'bombers', 'resumption', 'shuttle', 'expendable', 'launch-vehicle', 'engines', 'hits', 'weapons-modernization', 'C-130', '734.9', '811.9', '3.04', '2.47', '7.4', '2.30', '12.52', '11.95', 'Austin', 'Texas-based', '210', '512', 'kilobytes', '40-megabyte', 'monitor', '2,099', 'more-advanced', 'microprocessor', 'megabytes', '100-megabyte', '5,699', '6,799', 'Personal', '286', 'microprocessors', 'grower', 'life-of-contract', '14.54', '14.28', '14.53', '0.56', 'restraints', '0.54', '14.26', 'permissible', '0.50', '14.00', 'near-limit', '1990-91', 'automobile', 'regardless', 'third-largest', 'fifth-largest', 'drastic', '1988-89', 'Judith', 'granted', 'licenses', 'planting', 'orange', 'trees', 'cane', 'Hackensack', 'am', 'Above', 'atmosphere', 'ethanol', 'fleet', 'importer', '60.36', '20.07', 'colder', 'GRAINS', 'AND', 'SOYBEANS', 'muted', 'observance', 'Saints', 'Day', 'Continued', 'buys', 'grains', 'verge', 'designation', 'generous', '1.1650', 'ignored', 'Disputado', 'Reuter', 'emergency', 'copper-rich', 'inoperative', 'native', 'landowners', 'secede', 'Cos.', '36-store', 'fair', 'rang', '313', 'Hubbell', 'all-cash', 'liquidation', 'Paso', 'boots', 'leather', 'accrue', 'unspecified', 'termed', 'amicable', '27-year', 'information-services', 'several-year', 'stemmed', 'directorship', 'eight-person', 'Shepperd', 'UBS', 'Drew', 'irrelevant', '913', '14.43', '43.875', 'Nigel', 'Judah', 'Holland', 'Mannix', 'Kingsbridge', '45-a-share', 'Donuts', '50.1', 'Delaware', 'Dunkin', 'deter', '38.5', 'receipt', '35.2', 'Conn.based', 'magnified', 'nonrecurring', '8.2', 'asset-valuation', 'adjustments', '85.7', '93.3', 'natural-gas', 'reserve', 'reimbursed', 'amortization', '169.9', 'blue-chips', 'brunt', '41.60', '500-Stock', '0.84', '341.20', 'Equity', '0.99', '319.75', 'Composite', '0.60', '188.84', 'Advancing', 'decliners', '176.1', 'arrive', 'firmed', 'faltered', 'pains', 'Uncertainty', 'Arbitrage-related', 'awaits', 'reluctant', 'stick', 'necks', 'Woolworth', 'Products', 'Paramount', 'Ferro', 'early-retirement', 'AMR', 'developer', 'mount', 'withdrew', '120-a-share', 'Derchin', '5.1', 'Mead', 'Louisiana-Pacific', '5.6', 'ex-dividend', 'Value', '1.56', '372.14', '11,390,000', 'Spaghetti', 'Warehouse', 'convert', 'adverse', 'otherwise', 'Orleans-based', 'yet-to-be-formed', 'distributed', 'unitholders', 'Unitholders', 'cents-a-unit', '108', '88.32', '618.1', '77.6', 'backdrop', 'continuous', '40.21', '16.09', '28.36', '11.72', '1.916', '1.637', 'seven-yen', 'Atsushi', 'Muramatsu', 'experienced', 'remarkable', 'difficulties', 'firmly', 'Depending', '2.35', '14.75', 'customized', 'simulates', 'unexpected', 'Fairlawn', 'full-year', '148', '2.19', 'Harry', 'Millis', 'McDonald', 'unanticipated', 'government-owned', 'subcontractor', 'cluster', 'bombs', 'Aerojet', 'Ordnance', '93.9', '1.19', '92.9', '1.18', '6.4', 'Hasbrouk', 'waivers', 'distributes', 'produces', 'literature', 'displays', '158,666', '26,956', '608,413', '967,809', '1.35', 'multinational', 'Haden', 'MacLellan', 'Surrey', 'Feniger', 'acquisition-minded', 'Seattle-based', '62.1', 'outbid', 'Ratner', 'mid-afternoon', '260', 'acceptances', '87-store', 'derived', 'averted', 'Fantasy', '2,050-passenger', 'slated', '12.09', 'Nuys', '132,000', 'write-off', 'realestate', '361.8', 'Peoria', 'adopt', 'Kalamazoo', 'Mich.-based', 'severance', 'staying', 'high-flying', 'toy', 'peaked', '430', '92', 'reorganized', 'Ranger', '225,000', 'Adam', 'plagued', 'glitches', 'fortunes', 'bounced', 'Cabbage', 'Patch', 'dolls', 'winner', 'bankruptcy-law', 'ORTEGA', 'ENDED', 'Nicaraguan', 'U.S.-backed', '19-month-old', 'cease-fire', 'reaffirmed', 'Ortega', 'thwart', 'demobilize', 'deplorable', 'brushed', 'renewing', 'insurgents', 'Contra', 'Honduras', 'Sandinista', 'offensive', 'rebel', 'Krenz', 'freedoms', 'socialism', 'Party', 'Thousands', 'fled', 'cross-border', 'emigres', 'conferees', 'Africa', 'armed', 'Namibian', 'nationalist', 'guerrillas', 'neighboring', 'Angola', 'violating', 'U.N.-supervised', 'peace', 'territory', 'alert', 'Guerrilla', 'sabotage', 'Namibia', 'Gunmen', 'Lebanon', 'assassinated', 'Arabian', 'pro-Iranian', 'Islamic', 'slaying', 'avenge', 'beheading', 'terrorists', 'Riyadh', 'Beirut', 'Moslem', 'implements', 'rulers', 'pledged', 'modernization', 'impeding', 'Pakistan', 'Bhutto', 'defeated', 'no-confidence', '42-year', '11-month-old', 'Islamabad', '237-seat', 'rigged', 'shipboard', 'Malta', '2-3', 'tete-a-tete', 'trafficking', 'Andean', 'coffee', 'Pan', 'Am', 'subpoenaed', 'CIA', 'FBI', 'bomb', 'planted', 'aboard', 'exploded', 'Scotland', 'Died', 'Attwood', 'Mutual', 'acute', 'anemic', 'tendering', '99.3', '3.55', '1.4', 'unresolved', 'Jon', 'Peters', 'Guber', 'laying', 'centralized', '492', '4.55', '12.97', 'Property\\\\/casualty', 'disasters', 'unconsolidated', 'Sept.30', '16.68', '116.7', '12.68', '292.32', '263.07', '7.63', '5.82', 'Per-share', '7.84', '6.53', 'Brisk', 'Domestic', 'bulldozers', '142.84', '126.15', 'Demand', 'climb', '566.54', '28.53', '12.82', 'ECONOMIC', 'GROWTH', 'APPEARS', 'outlays', 'apparent', 'drops', 'crises', '55-a-share', 'minimum-wage', 'resigning', 'depends', 'Light', '1206.26', '220.45', '3436.58', 'Commodities', '129.91', '0.28', '131.01', '1.17', '0.95', '0.0085', 'Junk-bond', 'fizzled', 'pall', 'high-rolling', '43-year-old', '12.7', 'Bears', 'shaky', 'flies', 'planes', 'salable', '458', 'cushion', 'Lindner', 'Irwin', 'Jacobs', 'latter', 'mulling', 'mitigate', 'distinct', 'reserved', '227', 'till', 'issuer', 'bust', 'restructures', 'longtime', 'battered', 'Gillett', 'Possibly', 'unrealized', 'restructurings', 'balloon', 'floated', 'Mellon', 'shopped', 'spun', 'workable', 'capitalize', 'bread-and-butter', 'Sanford', 'Pauline', 'Yoshihashi', 'NYSE', 'Symbol', 'CSV', '1.49', 'Third', '11.57', '83,206', 'Note', 'per-share', 'protein', 'recombinant', 'DNA', 'Sandoz', 'preclinical', 'marrow', 'blood-cell', 'protein-1', 'induce', 'formation', 'compositions', 'defects', 'fracture', 'healing', 'periodontal', 'cancers', 'Clarence', 'Judiciary', 'Equal', 'Employment', 'Opportunity', 'Groups', 'advocating', 'discrimination', 'Fourteen', 'jurisdiction', 'EEOC', 'judgment', 'Runkel', 'nominees', 'satisfactorily', 'bench', 'vacancy', 'Ferdinand', 'Germany-based', 'boilers', 'pipes', 'Lurgi', 'G.m.b', 'shedding', '434.4', 'injuring', 'investigate', 'sweater', 'defines', 'assess', 'apparel', 'agreements', 'manmade-fiber', '405', 'Wilmington', 'neoprene', 'rubber', 'implementation', 'Theodore', 'headcount-control', 'questioned', 'Gelles', 'Wertheim', 'Schroder', 'trim', '275', '350', '21,000', '87.5', '38.875', 'bidder', 'shareholder-rights', 'unwanted', 'suitors', 'cost-control', 'staff-reduction', 'trimmed', 'INTER-TEL', 'Chapman', 'WayMar', 'Holt', '326', '19.95', 'reassuring', 'preface', 'integrity', 'trip', 'mindful', 'plaintive', 'high-minded', 'assurance', 'sticky', 'fingers', 'sweaty', 'Bribe', 'bribe', 'Matthew', 'Harrison', 'path', 'traveled', 'inception', 'full-fledged', 'vital', 'revolves', 'rolling', 'Johnson-era', 'mandates', 'noncompetitively', 'ancestry', 'born', 'Puerto', 'Rico', 'falsify', '50\\\\/50', 'races', 'minority-owned', 'blighted', 'famous', 'Jimmy', 'plugged', 'rebuilding', \"'80s\", 'Starting', 'Congressman', 'Mario', 'Biaggi', 'sentence', 'bribing', 'Wallach', 'Attorney', 'Meese', 'fashioned', 'bribery', 'peddling', 'politically', 'respectable', 'confidant', 'Lyn', 'Nofzinger', 'corrupt', 'scheme', 'bag', 'crook', 'befell', 'semiliterate', 'sensational', 'revelations', 'breezy', 'easy-to-read', 'gripping', 'scams', 'ingenuity', 'Auditors', 'crookery', 'garden-variety', 'lifes', 'Mercedes', 'clothes', 'wrestling', 'intelligent', 'insane', 'Irving', 'Lobsenz', 'pediatrician', 'Rusty', 'gambler', 'blackjack', 'arrested', 'Absorbed', 'doling', 'tidbits', 'gloss', 'root', 'auspices', 'rigid', 'affirmative', 'Programs', 'expressing', 'thieves', 'Numerous', 'scandals', 'HUD', 'characteristics', 'tailor-made', 'insider', 'whenever', 'redistributing', 'influencing', 'brokering', 'bloc', 'Nomenklatura', 'Stern', 'Urban', '1983-85', 'bankrupt', 'Everything', 'Andersson', 'finalized', 'state-appointed', 'receivers', 'lease', 'Subcontractors', 'swift', 'avert', 'skilled', 'shipyards', 'injecting', 'Customers', 'undelivered', '170', 'Helsinki', 'dashed', 'repay', 'Concurrent', 'Norfolk', 'sputtered', 'Purina', '45.2', '84.9', '1.24', '422.5', '6.44', '387.8', '5.63', '70.2', 'seafood', '5.8', 'phase-out', 'Hostess', 'bakery', 'rechargeable', 'cadmium', 'carbon', 'zinc', 'ingredients', 'cereal', 'Continental', 'Baking', 'bread', 'Eveready', '80.50', 'Companies', 'five-cent', 'percent', 'Otherwise', '300-day', '55.1', 'cash-and-stock', 'Ravenswood', 'corn-buying', 'binge', 'bottlenecks', 'pipeline', 'trains', 'harvested', 'loading', 'reaping', 'windfall', 'gyrate', 'scrounge', 'strain', 'Dunton', 'upper', 'trimming', 'tows', 'south', 'feeds', 'reservoirs', 'Barge', 'sank', 'alleviate', 'slowed', 'Similar', 'hamstrung', 'budding', 'logistical', '2.375', 'rebuild', 'depleted', 'winding', 'speculating', 'gather', 'permits', '2.15', 'Lyle', 'Waterloo', 'one-fifth', 'Biedermann', 'Allendale', 'port', 'Ports', 'Lakes', 'Coast', 'relieve', 'hauling', 'compressed', 'delayed', 'refinery', 'tightening', 'Heating', '58.64', '19.94', 'sell-off', 'PRECIOUS', 'METALS', '3.20', '377.60', 'silver', '6.50', '5.2180', '5.70', '494.50', 'Precious', 'influenced', \"O'Neill\", 'Elders', 'equities', 'warehouses', '170,262', '226,570,380', 'miners', '1.20', '1.14', 'Bronces', 'Soldado', 'Exxon-owned', 'Minera', 'Disputada', 'Reasons', 'walkout', 'procedural', 'upbeat', 'mood', 'precedes', '51.6', 'readings', '47.1', 'NCR', 'midrange', 'networking', 'hub', 'Novell', 'NetWare', 'riding', 'industrywide', 'chunks', 'Marcus', 'appliance', 'Lorain', '50-50', 'Kobe', 'earning', 'Bethlehem', '54', 'Inland', 'plummeted', '34.625', 'exceeded', 'projections', 'Bradford', 'richer', 'pipe', 'galvanized', 'coated', 'lower-priced', 'structural', 'Marathon', '198', 'soliciting', 'TXO', 'Proceeds', '15.5', '257', '13.1', '721', '2.62', '598', '2.07', 'Barrett', 'Leon', 'Level', 'McFarlan', 'DiLoreto', 'container', 'Delmont', 'constitutional-law', 'Professors', 'Laurence', 'scuttle', 'implicitly', 'spectrum', 'authorizes', 'partial', 'Civil', 'shared', 'lawmaking', 'supports', 'reckless', 'railcar', 'platforms', 'Trailer', 'Train']\n",
            "6463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze_hz2nDb3ae"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Table 3.1 in Chapter 1 of the NLTK book lists a number of different methods that can be applied to a `FreqDist` frequency distribution. Pick one of these, explain what it does in your own words, and show how to apply it in text (show your code).\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> The method I chose is the .tabulate() method which returns the frequency distribution of a given text (in this case Macbeth) through a table. This makes it easier to read than a long list I think. Code shown below.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjsDu1jVmuBH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3da9681b-79f9-4034-9567-a1c498b34771"
      },
      "source": [
        "print(freq_m.most_common(10))\n",
        "print(freq_m.tabulate(30))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(',', 1962), ('.', 1235), (\"'\", 637), ('the', 531), (':', 477), ('and', 376), ('I', 333), ('of', 315), ('to', 311), ('?', 241)]\n",
            "   ,    .    '  the    :  and    I   of   to    ?    d    a  you   in   my  And   is that  not   it Macb with    s  his   be  The haue   me your  our \n",
            "1962 1235  637  531  477  376  333  315  311  241  224  214  184  173  170  170  166  158  155  138  137  134  131  129  124  118  117  111  110  103 \n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_IktbuuZ35-"
      },
      "source": [
        "We can combine frequency counts with other tests to find interesting words in a text. For example, we can use a list comprehension to get a list of long words in a text - let's define *long* as having at least 10 characters. Then we can filter that list of long words by frequency (according to our frequency distribution), keeping only words that occur in the text/corpus at least 7 times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aoVtN-3aObX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b154f5f-c4f3-40fe-b558-96cd19fb82e1"
      },
      "source": [
        "### first, a list comprehension for creating a list of long words\n",
        "wsj_long = [w for w in set(text7) if len(w) >= 10]\n",
        "print(len(wsj_long))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2354\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPvaGio9aX4H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54130265-e5fb-492a-ffb8-8c4506e37179"
      },
      "source": [
        "### next, we'll add a second condition to the list comprehension that uses \n",
        "### our frequency distribution\n",
        "wsj_long_freq = [w for w in set(text7) if len(w) >= 10 and fdist7[w] >= 7]\n",
        "print(len(wsj_long_freq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34r0ACKIaoNK"
      },
      "source": [
        "Notice how we combine the two conditions inside the list comprehension:\n",
        "\n",
        "* `if len(w) >= 10` checks for long words\n",
        "* `and fdist7[w] >= 7` uses `and` to conjoin the two conditions - both have to be true for w to be selected\n",
        "\n",
        "Now we can sort the list and take a look at it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXdrrhxJa_Wf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c18061ca-f361-4250-a9bf-cf10eeb104c7"
      },
      "source": [
        "print(sorted(wsj_long_freq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Associates', 'Association', 'California', 'Commission', 'Commonwealth', 'Connecticut', 'Constitution', 'Containers', 'Department', 'Georgia-Pacific', 'Greenville', 'Industrial', 'Industries', 'International', 'Investment', 'Management', 'Massachusetts', 'Mitsubishi', 'Pennsylvania', 'Philadelphia', 'Securities', 'Transportation', 'University', 'Waertsilae', 'Washington', 'acquisition', 'acquisitions', 'activities', 'additional', 'administration', 'advertisers', 'advertising', 'announcement', 'apparently', 'appropriations', 'assistance', 'association', 'bankruptcy', 'businesses', 'candidates', 'circulation', 'commercial', 'commission', 'commitments', 'competition', 'compromise', 'conditions', 'conference', 'confidence', 'congressional', 'considered', 'construction', 'continuing', 'contributed', 'convertible', 'current-carrying', 'debentures', 'department', 'development', 'difference', 'differences', 'discussions', 'electronics', 'eliminated', 'employment', 'enforcement', 'engineering', 'environment', 'especially', 'essentially', 'eventually', 'executives', 'experience', 'fetal-tissue', 'fourth-quarter', 'government', 'headquarters', 'homelessness', 'increasing', 'individual', 'individuals', 'industrial', 'information', 'institutional', 'institutions', 'interested', 'international', 'introduced', 'introduction', 'investment', 'investments', 'legislation', 'management', 'manufacturing', 'officially', 'operations', 'opportunity', 'organization', 'outstanding', 'particular', 'particularly', 'percentage', 'performance', 'previously', 'production', 'profitable', 'program-trading', 'publishing', 'purchasing', 'regulators', 'regulatory', 'relatively', 'reorganization', 'requirements', 'researchers', 'restrictions', 'restructuring', 'retirement', 'scientists', 'securities', 'shareholder', 'shareholders', 'short-term', 'significant', 'specialist', 'specialists', 'speculation', 'spokeswoman', 'standardized', 'stock-index', 'stock-market', 'subsidiary', 'substantially', 'superconductors', 'suspension', 'technology', 'television', 'temporarily', 'themselves', 'third-quarter', 'threatened', 'traditional', 'transaction', 'transactions', 'transplants', 'unconstitutional', 'underlying', 'university', 'violations', 'volatility', 'wrongdoing', 'year-earlier']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPhStaMXbF6U"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Why do you think we're more interested in high-frequency long words than in high-frequency short words?\n",
        "\n",
        "> From this list of high-frequency long words, could you make any guesses about what kind of data the words come from, or what kinds of texts?\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> I think high-frequency long words are more interesting because they are less common than short words, which can help indicate what sort of content is within the text you are examining as well as the genre or category of the text as well.\n",
        "\n",
        "> Based on the list of words, I would guess this is coming from the inaugural text or some other form of government documentation?\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP-JRmNUceXk"
      },
      "source": [
        "## **Printing results to file**\n",
        "\n",
        "There are a number of different ways to store results from your Python code in files. Today we will introduce one of the simplest methods - using `print()` to write results to a file.\n",
        "\n",
        "In order to write to a file, we first need to create the file and ensure that it's ready to be written to.\n",
        "\n",
        "Create the file by using `open()` - the first argument to `open` is the name you want your output file to have. The second argument (`'w'`) indicates that this file is being opened for writing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iollM42c7xE"
      },
      "source": [
        "outfile = open('myresults.txt','w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HnA1hgQdNLM"
      },
      "source": [
        "Now we can use a `print` statement with an extra argument specifying which file object we want to print to. Basically, we are redirecting the `print` statement - everything that would normally be printed to the screen will now be printed to a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VSqVJpcdcue"
      },
      "source": [
        "### I'm going to print my list of long, high-frequency words\n",
        "### found in text7 to my output file\n",
        "for w in wsj_long_freq:\n",
        "    print(w, file=outfile)\n",
        "\n",
        "### close the file when we're done printing to it\n",
        "outfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J-PwJ3zduQx"
      },
      "source": [
        "You'll see that the file appears in your list of files in Colab (over to the left, click the folder icon). You can now double-click the name of the file and it'll be opened up in another window for inspection. You can also download the file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vDb14N0beN9"
      },
      "source": [
        "# 5. Putting it all together\n",
        "\n",
        "Now it's time to use frequency analysis to investigate a text. This time, select one of the texts you uploaded from the Project Gutenberg website. Apply at least three different methods you learned from this lab to your text. At least one of those should use a frequency distribution. Show all of your code, and include a short written discussion of your results. Print some of your results to a file, and submit that file along with the link to your notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoZYWyOpoGJR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "cb8efa8d-bfd3-455b-ff1d-06acd8cd545c"
      },
      "source": [
        "#choosing dracula text\n",
        "freq_d = FreqDist(dracula_words)\n",
        "print(freq_d.most_common(10))\n",
        "print(freq_d.tabulate(40))\n",
        "mcb_long_freq = [w for w in set(macbeth_words) if len(w) >= 8 and fdist7[w] >= 5]\n",
        "print(mcb_long_freq)\n",
        "\n",
        "draculadict = {}\n",
        "for w in dracula_words:\n",
        "  if w in draculadict:\n",
        "    draculadict[w] = draculadict[w] + 1\n",
        "  else:\n",
        "    draculadict[w] = 1\n",
        "\n",
        "print(\"The word 'blood' appears\", draculadict['blood'], \"times in Dracula.\")\n",
        "print(\"The word 'death' appears\", draculadict['death'], \"times in Dracula.\")\n",
        "print(\"The word 'monster' appears\", draculadict['monster'], \"times in Dracula.\")\n",
        "\n",
        "frankiedict = {}\n",
        "\n",
        "for w in frankie_words:\n",
        "  if w in frankiedict:\n",
        "    frankiedict[w] = frankiedict[w] + 1\n",
        "  else:\n",
        "    frankiedict[w] = 1\n",
        "\n",
        "print(\"The word 'blood' appears\", frankiedict['blood'], \"times in Frankenstein.\")\n",
        "print(\"The word 'death' appears\", frankiedict['death'], \"times in Frankenstein.\")\n",
        "print(\"The word 'monster' appears\", frankiedict['monster'], \"times in Frankenstein.\")\n",
        "\n",
        "submitfile = open('submitfile.txt', 'w')\n",
        "\n",
        "for w in mcb_long_freq:\n",
        "  print(w, file=submitfile)\n",
        "\n",
        "submitfile.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(',', 11099), ('.', 7549), ('the', 7474), ('and', 5803), ('I', 4846), ('to', 4662), ('of', 3707), ('a', 2955), ('in', 2466), ('that', 2436)]\n",
            "    ,     .   the   and     I    to    of     a    in  that    he   was    it     ;     \"     '    is   for    as    me   not   his  with   you    we    my    be   all    on  have    so   her   had    at     -   him   but    --     s which \n",
            "11099  7549  7474  5803  4846  4662  3707  2955  2466  2436  1996  1870  1808  1671  1561  1530  1498  1480  1476  1452  1393  1384  1283  1265  1250  1157  1124  1119  1055  1053  1036  1032  1030  1017   990   942   890   843   704   668 \n",
            "None\n",
            "['direction', 'According', 'construction', 'becoming', 'Pictures', 'question', 'Commission', 'presence', 'strength', 'familiar', 'troubled', 'consider', 'interest', 'assistance', 'starting', 'speculation', 'addition', 'continue', 'performance', 'reported', 'approach', 'especially', 'together', 'yesterday', 'concluded', 'continued', 'straight', 'something', 'decision', 'conference', 'exposure']\n",
            "The word 'blood' appears 112 times in Dracula.\n",
            "The word 'death' appears 88 times in Dracula.\n",
            "The word 'monster' appears 22 times in Dracula.\n",
            "The word 'blood' appears 19 times in Frankenstein.\n",
            "The word 'death' appears 78 times in Frankenstein.\n",
            "The word 'monster' appears 31 times in Frankenstein.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auA6PPdbo8wj"
      },
      "source": [
        "> The three functions are shown up above in my code. First I wanted to find the 40 most common words and produce a table to represent the find. Next, I wanted to compare the word counts for the same chosen keywords as I did in Macbeth - 'honor' produced an error (assuming it's because there is no appearance of it in the text, so I added a new keyword to supplement that loss. It's interesting to see how often the same keywords will appear through similar genres so that was my final method. Comparing the keywords in dracula, macbeth and frankenstein."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7ej0sNaYhxX"
      },
      "source": [
        "# 6. REMINDER: Wrapping up and submitting\n",
        "\n",
        "Create a revision by going to **File > Save and Pin Revision**.\n",
        "\n",
        "View your revision history at **File > Revision History**.\n",
        "\n",
        "To submit: go to **Share** in the upper right corner, click **Get Shareable Link**, change the dropdown menu option to **Anyone with the link can edit**, and then **Copy Link**! This is what you'll submit on Canvas."
      ]
    }
  ]
}