{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Py4Text_Lab6&7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkj5P4Ja415w"
      },
      "source": [
        "# Combined Labs 6 & 7, Python for Text, Fall 2020\n",
        "## Lab 6 topics: Structuring programs, error handling (sections 1-2)\n",
        "## Lab 7 topics: Preprocessing with spaCy (sections 3-5)\n",
        "\n",
        "---\n",
        "To be completed and submitted by end of the day on Monday, October 19. \n",
        "\n",
        "The questions you need to answer are marked with **QUESTION**. For each one, there's a space (under **ANSWER**) for you to add your answer, which might be text, might be code, or might be a mix of the two. \n",
        "\n",
        "\n",
        "\n",
        "### Have questions?\n",
        "\n",
        "1. Use the Canvas discussion boards.\n",
        "\n",
        "## Reminder\n",
        "\n",
        "You will need to submit your own notebook (or rather, a link to your own notebook). There are (at least) two ways to do this:\n",
        "\n",
        "1. Make a copy of this notebook, rename it, and add new code cells when you want to write your own code.\n",
        "1. Create a new Python 3 notebook using the `File` menu and type in all cells yourself. \n",
        "\n",
        "Either way, your file should be named using this format: `LastName_Py4Text_Lab67.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRphYoQ7jHw_"
      },
      "source": [
        "#Overview: Working with scholarly/scientific data\n",
        "\n",
        "---\n",
        "\n",
        "{this is a long lab - don't wait to get started!}\n",
        "\n",
        "This week's lab covers several important topics:\n",
        "\n",
        "1. Working with scholarly/scientific data,\n",
        "2. Some methods for structuring and testing the Python code you write, and \n",
        "3. Preprocessing of textual data.\n",
        "\n",
        "The coding and debugging strategies you'll learn will be useful whenever you are programming. Preprocessing is an essential part of working with textual data. In essence, preprocessing helps us deal with some of the messiness that is part of text (and language). This week you'll learn about preprocessing with the example of scholarly data, but the preprocessing steps you'll learn about are *not* specific to scholarly data - they apply for just about any kind of written text.\n",
        "\n",
        "\n",
        "Things you should be able to do at the end of this lab:\n",
        "\n",
        "* Understand what a **scientific abstract** is\n",
        "* Get familiar with one small corpus of scientific abstracts\n",
        "* Write **pseudocode**\n",
        "* Structure programs by chaining functions together\n",
        "* Use print statements for **debugging**\n",
        "* Use **`assert`** for debugging\n",
        "* Use **`try/except`** for error handling\n",
        "* Define a **class** and create **objects** from that class\n",
        "* Understand the role and function of these **preprocessing** steps for textual data: **tokenization**, **sentence splitting**, **lemmatization**, and **part-of-speech (POS) tagging**\n",
        "* Use **spaCy** to perform preprocessing \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuVv7uLcWzfr"
      },
      "source": [
        "# 1. Scientific abstracts as textual data\n",
        "\n",
        "In the previous labs, we worked with news data and then with the texts of novels (or other fictional works). This week, we turn to scholarly data, and specifically to research articles - scientific publications. Research articles are one of the main sources of new knowledge in science. This is where researchers present their experimental work and their new results, across many different disciplines. Research papers are also extremely important in the humanities and social sciences. Such papers can be published in journals, proceedings of conferences, scholarly archives, and even on people's websites. \n",
        "\n",
        "Though the structure of the typical scientific publication changes somewhat from one academic discipline to another, nearly all disciplines include the **abstract** as part of that structure. An abstract is essentially a short summary of the article. In a very small amount of text - usually one or two paragraphs - an abstract:\n",
        "\n",
        "* explains why the research is important,\n",
        "* mentions the main methods used in the work, and\n",
        "* outlines the most important conclusions and results.\n",
        "\n",
        "Some people say that the abstract should convince a potential reader that it's worthwhile to read the rest of the paper. So, abstracts can give us a pretty good idea what a paper is about. In many fields, the number of articles published every year is *way* more than one individual can read and keep up with, and abstracts can be very helpful in deciding which papers to read, and then which to read first. We can also imagine using automatic methods for scanning abstracts in order to stay on top of the flood of scientific literature in a field.\n",
        "\n",
        "**READING:** To learn more about some of the potential uses of abstracts, read the first two and a half pages (through the end of section 3) of this paper: http://clg.wlv.ac.uk/papers/orasan-01b.pdf\n",
        "\n",
        "Title: Patterns in Scientific Abstracts <br>\n",
        "Author: Constantin Orason, University of Wolverhampton <br>\n",
        "In: *Proceedings of the 2001 Corpus Linguistics Conference* <br>\n",
        "\n",
        "The rest of the paper provides a nice example of a corpus analysis, as Orason describes the corpus he has collected and analyzed in great detail. If you think you might like to do a **corpus analysis** for your final project, this paper offers an excellent example.\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> How many abstracts are included in the corpus Orason analyzes? Which field has the most abstracts? Which has the fewest? \n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> 917 total abstracts with the field of Artificial Intelligence having the most files (512). Both Anthropology and Linguistics have the fewest amount with 50 files each. \n",
        "\n",
        "-----\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpJkLCpp65oO"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "In addition to being sources of new knowledge, academic articles are themselves widely used as the subject of other scientific studies, with the result that many different corpora of academic articles have been assembled.\n",
        "\n",
        "This webpage gives a nice overview of different corpora and their properties (including their availability): https://www.clarin.eu/resource-families/corpora-academic-texts\n",
        "\n",
        "For this week's lab, we'll be using mini-corpora that I have created by taking subsets of two existing corpora. One is a corpus of abstracts, and the other is a corpus of full-length scientific articles. In both cases, the texts have been transformed into plain text format, making it much easier to read into a computer program - imagine trying to deal with a bunch of pdfs in Python!\n",
        "\n",
        "Our first mini-corpus is a subset of the **SciDTB** corpus. This is a collection of abstracts which have been annotated with discourse structure and further analyzed for the function of each sentence. We're not using the annotated version, though, just the plain text. If you're interesting in reading more about the corpus and how it has been used in research, see this paper (*not* a required reading): https://www.aclweb.org/anthology/P18-2071/\n",
        "\n",
        "The second mini-corpus is a subset of the **ACL-ARC** corpus. This is a collection of computational linguistics papers. I've randomly selected 100 papers from the original corpus, which consists of more than 10,000 papers. We're using a version in which the original pdfs have been converted to plain text, and figures and tables have either been removed or transformed into a machine-friendly format. In addition, all special characters have been removed, so that we only have ASCII characters (we'll talk about encoding near the end of the semester - if you're interested in learning more in the meantime, here's Wikipedia on ASCII: https://en.wikipedia.org/wiki/ASCII). Here's the paper describing the corpus in more detail (optional reading): http://www.lrec-conf.org/proceedings/lrec2008/pdf/445_paper.pdf\n",
        "\n",
        "Take a look at the webpage for the ACL-ARC corpus, where you can find a description of the different ways the data has been cleaned up: https://web.eecs.umich.edu/~lahiri/acl_arc.html\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> Describe two kinds of data cleaning done by the corpus creators.\n",
        "\n",
        "### **ANSWER**\n",
        "\n",
        "> The corpus creators have removed headers from papers (this does not include the abstracts) as well as the footers (which does include the references).\n",
        "\n",
        "-----\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wwr2k4UCsql"
      },
      "source": [
        "\n",
        "## Accessing the data\n",
        "\n",
        "For this week's lab, I have created two mini-corpora for you to explore. Both are subsets of larger corpora (described in Section 1), and in both cases I have created a compressed zip file containing the texts: \n",
        "\n",
        "* the `sciDTB` mini-corpus contains 153 plain-text abstracts from research papers in computational linguistics\n",
        "* the `arcSmall` mini-corpus contains 100 plain-text versions of full-length research papers in computational linguistics\n",
        "\n",
        "You can fetch these using the following commands - after running this code block, the `.zip` files should show up in your list of Colab files (click the folder icon to the left)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkLCmMRkmULP"
      },
      "source": [
        "### download sciDTB zip file\n",
        "!wget https://github.com/alexispalmer/py4text/raw/main/data/SciDTB-mini.zip\n",
        "\n",
        "### download arcSmall zip file\n",
        "!wget https://github.com/alexispalmer/py4text/raw/main/data/aclArc-mini.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8Kpnt1PmlLy"
      },
      "source": [
        "Once you have downloaded the zip files, you need to unzip them to have access to the files. \n",
        "\n",
        "SIDE NOTE: You'll notice that both of the lines below, and also the `wget` commands we used to download the zip files, start with `!`. We can use this to run commands that we would normally run on the command line (specifically, in a `bash` shell)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRygeNHKmyrN"
      },
      "source": [
        "!unzip SciDTB-mini.zip\n",
        "!unzip aclArc-mini.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn_VT8xyBNeu"
      },
      "source": [
        "Now refresh your folders listing (click on the folder with the round arrow, above the list of file and folder names), and you can see two new folders, one for each mini-corpus. Click on one of these to see the list of filenames, and then double-click on one of those to inspect the contents of the file.\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> Look at (at least) one file from each mini-corpus. You'll see that each mini-corpus has a different format for its files. What differences do you see between the two?\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> The first difference I see between the two files is the structure of the content itself. For the txt files within the SciDTB folder, the entire text file is outputed in one line while the text files for the aclArc folder breaks up the files into sections (Abstract, Introduction, etc.) throughout multiple lines.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_wF951UW3lH"
      },
      "source": [
        "# 2a. Planning and structuring code\n",
        "\n",
        "Next, we want to access some of the individual texts from our mini-corpora. Let's focus on the SciDTB corpus for now. The first thing we need to do is make a list of all of the filenames in the corpus. Since we're no longer in the safety of the NLTK, we can't count on our trusty `fileids()` method. Instead, we're going to make use of a module in Python called `os` - this stands for **operating system**. The `os` module gives us a way of running commands that we would normal run outside of Python (very much like the `!` prefix mentioned above). \n",
        "\n",
        "One of the really useful things we can do with the `os` module is to get a list of all filenames in a directory. We'll break this process into two steps:\n",
        "\n",
        "1. Create a variable for the directory name (stored as a string).\n",
        "1. Use an `os` command to get a list of all files in the directory (each of these will also be a string, just like what we get from the `fileids()` method.\n",
        "\n",
        "Once we have that list, we can iterate through it using a `for`-loop, to process each file, one at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQo-Ywvn_J4C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c048e835-0c77-4d57-ec62-9cb64e165b2b"
      },
      "source": [
        "### First we need to import the os module\n",
        "import os\n",
        "\n",
        "### Create a variable with the directory name\n",
        "sciDir = 'SciDTB-test'\n",
        "\n",
        "### Get a list of all the filenames in that directory\n",
        "sciFiles = os.listdir(sciDir)\n",
        "print(sciFiles[:10])   ## it's a long list, so we'll just print the first few"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['D14-1020.edu.out', 'P16-1131.edu.out', 'D14-1189.edu.out', 'P14-1116.edu.out', 'D14-1175.edu.out', 'D14-1018.edu.out', 'D14-1005.edu.out', 'D14-1003.edu.out', 'D14-1023.edu.out', 'P16-1145.edu.out']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfQwG7J-RQhD"
      },
      "source": [
        "### **REVIEW QUESTION:**\n",
        "\n",
        "> Use Python code to figure out how many files are in our SciDTB mini-corpus. Show your code.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> 152 files, code shown below\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> Now create a list named `arcFiles` with all of the filenames for the aclArc mini-corpus. Show your code.\n",
        "\n",
        "> 100 files, code shown below\n",
        "\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrKH9uCmVGP8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fbded354-da07-4993-8352-7c996bcad248"
      },
      "source": [
        "print(len(sciFiles))\n",
        "arcDir = 'aclArc-mini'\n",
        "arcFiles = os.listdir(arcDir)\n",
        "print(len(arcFiles))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC1ypOVh-Oen"
      },
      "source": [
        "## Pseudocode\n",
        "\n",
        "Now I want to introduce the concept of **pseudocode**. Pseudocode is often a very helpful step to be performed prior to writing your actual code. To write pseudocode, we first *think through* the steps that need to be performed in order to solve our problem, and then *write those steps out* in a kind of language that is somewhere between natural language and programming language. Usually, pseudocode is quite general and is not specific to any particular programming language. So the same pseudocode could be used to guide programmers using Python or Java or C# or JavaScript or or or ... and result in programs that do the same things, in all those different programming languages. \n",
        "\n",
        "We're going to try out the process of writing pseudocode to do some things with the files in our mini-corpus. First, though, watch this video explaining pseudocode in detail. In this video, the pseudocode is converted into a programming language, but using JavaScript rather than Python. Don't worry about the fact that the code looks entirely different - focus on the pseudocode.\n",
        "\n",
        "* Codeacademy on pseudocode (10 minutes): https://www.youtube.com/watch?v=PwGA4Lm8zuE\n",
        "\n",
        "If you'd still like to hear more, here are two more videos about pseudocode\n",
        "\n",
        "* 5 minutes to code (5 minutes):\n",
        "https://www.youtube.com/watch?v=HhBrkpTqzqg\n",
        "\n",
        "* British guy with cartoons (5 minutes):\n",
        "https://www.youtube.com/watch?v=XDWw4Ltfy5w\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> As an exercise, write Python code for the FizzBuzz problem (described in the Codeacademy video). The pseudocode instructions are already copied into a code block below. HINT: to get a `for`-loop that will iterate exactly 20 times, use the `range()` function.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> code shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYLp9WIbTbgf"
      },
      "source": [
        "#### The Problem:\n",
        "## Write a program that prints the numbers from 1 to 20.\n",
        "## For multiples of three print \"Fizz\" instead of the number\n",
        "## For multiples of five print \"Buzz\" instead of the number\n",
        "## For numbers which are multiples of both 3 and 5 print \"FizzBuzz\"\n",
        "## For numbers not divisible by 3, or 5, or both, print the number as is\n",
        "\n",
        "#### The pseudocode:\n",
        "\n",
        "## FOR LOOP:\n",
        "## For numbers from 1 to 20\n",
        "   ## IF number MOD 15 == 0\n",
        "       ## print 'FizzBuzz'\n",
        "   ## IF number MOD 3 == 0\n",
        "       ## print 'Fizz'\n",
        "   ## IF number MOD 5 == 0\n",
        "       ## print 'Buzz'\n",
        "   ## ELSE\n",
        "       ## print number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQtyTi9TWyqG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "dcee5eb0-b389-415d-f967-21b7c253a125"
      },
      "source": [
        "for n in range(1,21):\n",
        "  if n % 15 == 0:\n",
        "    print('FizzBuzz')\n",
        "  elif n % 3 == 0:\n",
        "    print('Fizz')\n",
        "  elif n % 5 == 0:\n",
        "    print('Buzz')\n",
        "  else:\n",
        "    print(n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "Fizz\n",
            "4\n",
            "Buzz\n",
            "Fizz\n",
            "7\n",
            "8\n",
            "Fizz\n",
            "Buzz\n",
            "11\n",
            "Fizz\n",
            "13\n",
            "14\n",
            "FizzBuzz\n",
            "16\n",
            "17\n",
            "Fizz\n",
            "19\n",
            "Buzz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2_MEIfvUUGK"
      },
      "source": [
        "Now let's look at working with the texts in our mini-corpus -- one useful thing we could do is to build a frequency distribution for those texts. Before we just start typing, let's write some pseudocode.\n",
        "\n",
        "### Step one: what are the high-level tasks? What is the problem?\n",
        "\n",
        "The problem is to produce a frequency distribution from a set of plain texts. The high level steps:\n",
        "\n",
        "* Read the texts in and convert each to a list of words\n",
        "* Combine the texts into one list\n",
        "* Create a frequency distribution from that list\n",
        "\n",
        "### Step two: Now let's turn this into pseudocode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb1aFZZJVFye"
      },
      "source": [
        "## VARIABLES AND DATA STRUCTURES NEEDED:\n",
        "## 1. list of the filenames of texts\n",
        "## 2. list of words for the entire corpus\n",
        "\n",
        "## FOR LOOP:\n",
        "## For each text in the mini-corpus\n",
        "    ## Read the text in as a string\n",
        "    ## SPLIT the text into a list of words\n",
        "    ## Add those words to the corpus word list\n",
        "## Create FREQDIST from list of words\n",
        "## Print 20 most common words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_XIJ2SrVpH-"
      },
      "source": [
        "### Step three: Finally, turn this into Python code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nFx9D8qVthw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "91a8f319-b437-46cd-b424-2213ad093367"
      },
      "source": [
        "import nltk\n",
        "from nltk import FreqDist\n",
        "\n",
        "## VARIABLES AND DATA STRUCTURES NEEDED:\n",
        "## 1. list of the filenames of texts\n",
        "    ### this we already have: sciFiles\n",
        "## 2. list of words for the entire corpus\n",
        "\n",
        "sciWords = []\n",
        "\n",
        "## FOR LOOP:\n",
        "## For each text in the mini-corpus\n",
        "for text in sciFiles:\n",
        "    ## Read the text in as a string\n",
        "    ## We need to give the whole path: the directory name\n",
        "    ##    plus the file name, separated by a slash\n",
        "    thisText = sciDir+\"/\"+text\n",
        "    print(\"now processing\", thisText)\n",
        "    with open(thisText, 'r') as f:\n",
        "        thisTextString = f.read() ## read in as string\n",
        "    \n",
        "    ## SPLIT the text into a list of words\n",
        "    thisTextWords = thisTextString.split()\n",
        "    print(\"Current text word count:\", len(thisTextWords))\n",
        "    \n",
        "    ## add a debugging step - we will comment these two lines out after\n",
        "    ## we know we're happy with what the code is doing\n",
        "    \n",
        "    # print(thisTextWords)\n",
        "    # input(\"okay to proceed? \")\n",
        "\n",
        "    ## Add those words to the corpus word list\n",
        "    ## Use extend instead of append, because what we're adding is a list\n",
        "    ###   of strings, not just a single element\n",
        "    sciWords.extend(thisTextWords)\n",
        "\n",
        "### Check the number of files and number of words in the corpus\n",
        "print()\n",
        "print(\"The corpus has\", len(sciFiles), \"files.\")\n",
        "print(\"The corpus has\", len(sciWords), \"words.\")\n",
        "\n",
        "## Create FREQDIST from list of words\n",
        "sciDist = FreqDist(sciWords)\n",
        "\n",
        "## Print 20 most common words\n",
        "print()\n",
        "print(\"The 20 most common words are:\")\n",
        "sciDist.most_common(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "now processing SciDTB-test/D14-1020.edu.out\n",
            "Current text word count: 134\n",
            "now processing SciDTB-test/P16-1131.edu.out\n",
            "Current text word count: 91\n",
            "now processing SciDTB-test/D14-1189.edu.out\n",
            "Current text word count: 65\n",
            "now processing SciDTB-test/P14-1116.edu.out\n",
            "Current text word count: 157\n",
            "now processing SciDTB-test/D14-1175.edu.out\n",
            "Current text word count: 117\n",
            "now processing SciDTB-test/D14-1018.edu.out\n",
            "Current text word count: 147\n",
            "now processing SciDTB-test/D14-1005.edu.out\n",
            "Current text word count: 87\n",
            "now processing SciDTB-test/D14-1003.edu.out\n",
            "Current text word count: 126\n",
            "now processing SciDTB-test/D14-1023.edu.out\n",
            "Current text word count: 113\n",
            "now processing SciDTB-test/P16-1145.edu.out\n",
            "Current text word count: 132\n",
            "now processing SciDTB-test/D14-1050.edu.out\n",
            "Current text word count: 105\n",
            "now processing SciDTB-test/P16-1102.edu.out\n",
            "Current text word count: 211\n",
            "now processing SciDTB-test/D14-1034.edu.out\n",
            "Current text word count: 146\n",
            "now processing SciDTB-test/P16-1139.edu.out\n",
            "Current text word count: 145\n",
            "now processing SciDTB-test/D14-1188.edu.out\n",
            "Current text word count: 103\n",
            "now processing SciDTB-test/P14-1119.edu.out\n",
            "Current text word count: 59\n",
            "now processing SciDTB-test/D14-1021.edu.out\n",
            "Current text word count: 90\n",
            "now processing SciDTB-test/P16-1143.edu.out\n",
            "Current text word count: 136\n",
            "now processing SciDTB-test/P16-1108.edu.out\n",
            "Current text word count: 108\n",
            "now processing SciDTB-test/P16-1109.edu.out\n",
            "Current text word count: 203\n",
            "now processing SciDTB-test/D14-1040.edu.out\n",
            "Current text word count: 150\n",
            "now processing SciDTB-test/D14-1028.edu.out\n",
            "Current text word count: 102\n",
            "now processing SciDTB-test/D14-1015.edu.out\n",
            "Current text word count: 114\n",
            "now processing SciDTB-test/P16-1103.edu.out\n",
            "Current text word count: 160\n",
            "now processing SciDTB-test/P16-1142.edu.out\n",
            "Current text word count: 70\n",
            "now processing SciDTB-test/P16-1114.edu.out\n",
            "Current text word count: 100\n",
            "now processing SciDTB-test/D14-1172.edu.out\n",
            "Current text word count: 126\n",
            "now processing SciDTB-test/D14-1017.edu.out\n",
            "Current text word count: 144\n",
            "now processing SciDTB-test/P14-1103.edu.out\n",
            "Current text word count: 130\n",
            "now processing SciDTB-test/P16-1140.edu.out\n",
            "Current text word count: 89\n",
            "now processing SciDTB-test/P16-1135.edu.out\n",
            "Current text word count: 145\n",
            "now processing SciDTB-test/P16-1115.edu.out\n",
            "Current text word count: 170\n",
            "now processing SciDTB-test/D14-1183.edu.out\n",
            "Current text word count: 96\n",
            "now processing SciDTB-test/P16-1136.edu.out\n",
            "Current text word count: 137\n",
            "now processing SciDTB-test/P16-1105.edu.out\n",
            "Current text word count: 157\n",
            "now processing SciDTB-test/P16-1104.edu.out\n",
            "Current text word count: 178\n",
            "now processing SciDTB-test/P14-1108.edu.out\n",
            "Current text word count: 168\n",
            "now processing SciDTB-test/P16-1118.edu.out\n",
            "Current text word count: 157\n",
            "now processing SciDTB-test/P14-1106.edu.out\n",
            "Current text word count: 128\n",
            "now processing SciDTB-test/D14-1177.edu.out\n",
            "Current text word count: 180\n",
            "now processing SciDTB-test/D14-1038.edu.out\n",
            "Current text word count: 186\n",
            "now processing SciDTB-test/D14-1013.edu.out\n",
            "Current text word count: 219\n",
            "now processing SciDTB-test/P16-1129.edu.out\n",
            "Current text word count: 126\n",
            "now processing SciDTB-test/P16-1125.edu.out\n",
            "Current text word count: 183\n",
            "now processing SciDTB-test/D14-1192.edu.out\n",
            "Current text word count: 77\n",
            "now processing SciDTB-test/D14-1169.edu.out\n",
            "Current text word count: 126\n",
            "now processing SciDTB-test/P16-1101.edu.out\n",
            "Current text word count: 137\n",
            "now processing SciDTB-test/D14-1035.edu.out\n",
            "Current text word count: 118\n",
            "now processing SciDTB-test/P16-1107.edu.out\n",
            "Current text word count: 64\n",
            "now processing SciDTB-test/D14-1029.edu.out\n",
            "Current text word count: 100\n",
            "now processing SciDTB-test/D14-1164.edu.out\n",
            "Current text word count: 115\n",
            "now processing SciDTB-test/P16-1111.edu.out\n",
            "Current text word count: 158\n",
            "now processing SciDTB-test/P16-1138.edu.out\n",
            "Current text word count: 117\n",
            "now processing SciDTB-test/D14-1044.edu.out\n",
            "Current text word count: 205\n",
            "now processing SciDTB-test/P14-1110.edu.out\n",
            "Current text word count: 83\n",
            "now processing SciDTB-test/D14-1043.edu.out\n",
            "Current text word count: 110\n",
            "now processing SciDTB-test/P14-1105.edu.out\n",
            "Current text word count: 103\n",
            "now processing SciDTB-test/P14-1121.edu.out\n",
            "Current text word count: 119\n",
            "now processing SciDTB-test/P16-1119.edu.out\n",
            "Current text word count: 107\n",
            "now processing SciDTB-test/P16-1126.edu.out\n",
            "Current text word count: 210\n",
            "now processing SciDTB-test/D14-1030.edu.out\n",
            "Current text word count: 432\n",
            "now processing SciDTB-test/D14-1019.edu.out\n",
            "Current text word count: 139\n",
            "now processing SciDTB-test/P16-1113.edu.out\n",
            "Current text word count: 92\n",
            "now processing SciDTB-test/D14-1165.edu.out\n",
            "Current text word count: 152\n",
            "now processing SciDTB-test/D14-1027.edu.out\n",
            "Current text word count: 148\n",
            "now processing SciDTB-test/P16-1127.edu.out\n",
            "Current text word count: 162\n",
            "now processing SciDTB-test/D14-1007.edu.out\n",
            "Current text word count: 116\n",
            "now processing SciDTB-test/P16-1134.edu.out\n",
            "Current text word count: 127\n",
            "now processing SciDTB-test/D14-1001.edu.out\n",
            "Current text word count: 125\n",
            "now processing SciDTB-test/P16-1132.edu.out\n",
            "Current text word count: 83\n",
            "now processing SciDTB-test/D14-1025.edu.out\n",
            "Current text word count: 127\n",
            "now processing SciDTB-test/D14-1046.edu.out\n",
            "Current text word count: 119\n",
            "now processing SciDTB-test/P14-1115.edu.out\n",
            "Current text word count: 135\n",
            "now processing SciDTB-test/P14-1118.edu.out\n",
            "Current text word count: 93\n",
            "now processing SciDTB-test/D14-1010.edu.out\n",
            "Current text word count: 120\n",
            "now processing SciDTB-test/D14-1171.edu.out\n",
            "Current text word count: 86\n",
            "now processing SciDTB-test/D14-1162.edu.out\n",
            "Current text word count: 156\n",
            "now processing SciDTB-test/D14-1037.edu.out\n",
            "Current text word count: 210\n",
            "now processing SciDTB-test/D14-1002.edu.out\n",
            "Current text word count: 189\n",
            "now processing SciDTB-test/D14-1170.edu.out\n",
            "Current text word count: 172\n",
            "now processing SciDTB-test/D14-1004.edu.out\n",
            "Current text word count: 110\n",
            "now processing SciDTB-test/P16-1130.edu.out\n",
            "Current text word count: 92\n",
            "now processing SciDTB-test/D14-1009.edu.out\n",
            "Current text word count: 98\n",
            "now processing SciDTB-test/D14-1174.edu.out\n",
            "Current text word count: 98\n",
            "now processing SciDTB-test/D14-1161.edu.out\n",
            "Current text word count: 120\n",
            "now processing SciDTB-test/D14-1048.edu.out\n",
            "Current text word count: 63\n",
            "now processing SciDTB-test/P14-1109.edu.out\n",
            "Current text word count: 187\n",
            "now processing SciDTB-test/P16-1116.edu.out\n",
            "Current text word count: 170\n",
            "now processing SciDTB-test/D14-1176.edu.out\n",
            "Current text word count: 138\n",
            "now processing SciDTB-test/D14-1168.edu.out\n",
            "Current text word count: 138\n",
            "now processing SciDTB-test/P14-1102.edu.out\n",
            "Current text word count: 155\n",
            "now processing SciDTB-test/D14-1014.edu.out\n",
            "Current text word count: 130\n",
            "now processing SciDTB-test/P14-1112.edu.out\n",
            "Current text word count: 123\n",
            "now processing SciDTB-test/D14-1008.edu.out\n",
            "Current text word count: 157\n",
            "now processing SciDTB-test/D14-1036.edu.out\n",
            "Current text word count: 125\n",
            "now processing SciDTB-test/D14-1033.edu.out\n",
            "Current text word count: 128\n",
            "now processing SciDTB-test/P14-1111.edu.out\n",
            "Current text word count: 113\n",
            "now processing SciDTB-test/P14-1122.edu.out\n",
            "Current text word count: 101\n",
            "now processing SciDTB-test/D14-1167.edu.out\n",
            "Current text word count: 177\n",
            "now processing SciDTB-test/D14-1011.edu.out\n",
            "Current text word count: 96\n",
            "now processing SciDTB-test/D14-1049.edu.out\n",
            "Current text word count: 115\n",
            "now processing SciDTB-test/P16-1110.edu.out\n",
            "Current text word count: 143\n",
            "now processing SciDTB-test/D14-1039.edu.out\n",
            "Current text word count: 100\n",
            "now processing SciDTB-test/P16-1122.edu.out\n",
            "Current text word count: 124\n",
            "now processing SciDTB-test/P16-1123.edu.out\n",
            "Current text word count: 90\n",
            "now processing SciDTB-test/P16-1106.edu.out\n",
            "Current text word count: 84\n",
            "now processing SciDTB-test/D14-1045.edu.out\n",
            "Current text word count: 104\n",
            "now processing SciDTB-test/P16-1120.edu.out\n",
            "Current text word count: 149\n",
            "now processing SciDTB-test/P14-1101.edu.out\n",
            "Current text word count: 134\n",
            "now processing SciDTB-test/P14-1114.edu.out\n",
            "Current text word count: 92\n",
            "now processing SciDTB-test/P16-1133.edu.out\n",
            "Current text word count: 133\n",
            "now processing SciDTB-test/P14-1117.edu.out\n",
            "Current text word count: 123\n",
            "now processing SciDTB-test/P16-1117.edu.out\n",
            "Current text word count: 97\n",
            "now processing SciDTB-test/D14-1187.edu.out\n",
            "Current text word count: 62\n",
            "now processing SciDTB-test/D14-1190.edu.out\n",
            "Current text word count: 68\n",
            "now processing SciDTB-test/D14-1182.edu.out\n",
            "Current text word count: 72\n",
            "now processing SciDTB-test/P16-1146.edu.out\n",
            "Current text word count: 129\n",
            "now processing SciDTB-test/D14-1186.edu.out\n",
            "Current text word count: 110\n",
            "now processing SciDTB-test/D14-1026.edu.out\n",
            "Current text word count: 100\n",
            "now processing SciDTB-test/D14-1024.edu.out\n",
            "Current text word count: 101\n",
            "now processing SciDTB-test/P14-1107.edu.out\n",
            "Current text word count: 106\n",
            "now processing SciDTB-test/D14-1006.edu.out\n",
            "Current text word count: 126\n",
            "now processing SciDTB-test/D14-1042.edu.out\n",
            "Current text word count: 167\n",
            "now processing SciDTB-test/D14-1041.edu.out\n",
            "Current text word count: 173\n",
            "now processing SciDTB-test/D14-1166.edu.out\n",
            "Current text word count: 141\n",
            "now processing SciDTB-test/P16-1137.edu.out\n",
            "Current text word count: 140\n",
            "now processing SciDTB-test/D14-1022.edu.out\n",
            "Current text word count: 104\n",
            "now processing SciDTB-test/D14-1016.edu.out\n",
            "Current text word count: 157\n",
            "now processing SciDTB-test/D14-1193.edu.out\n",
            "Current text word count: 103\n",
            "now processing SciDTB-test/P14-1104.edu.out\n",
            "Current text word count: 110\n",
            "now processing SciDTB-test/P16-1121.edu.out\n",
            "Current text word count: 102\n",
            "now processing SciDTB-test/D14-1185.edu.out\n",
            "Current text word count: 139\n",
            "now processing SciDTB-test/D14-1012.edu.out\n",
            "Current text word count: 152\n",
            "now processing SciDTB-test/D14-1181.edu.out\n",
            "Current text word count: 105\n",
            "now processing SciDTB-test/D14-1163.edu.out\n",
            "Current text word count: 143\n",
            "now processing SciDTB-test/D14-1179.edu.out\n",
            "Current text word count: 131\n",
            "now processing SciDTB-test/D14-1031.edu.out\n",
            "Current text word count: 92\n",
            "now processing SciDTB-test/P16-1128.edu.out\n",
            "Current text word count: 107\n",
            "now processing SciDTB-test/D14-1173.edu.out\n",
            "Current text word count: 179\n",
            "now processing SciDTB-test/D14-1191.edu.out\n",
            "Current text word count: 110\n",
            "now processing SciDTB-test/P16-1144.edu.out\n",
            "Current text word count: 155\n",
            "now processing SciDTB-test/P14-1113.edu.out\n",
            "Current text word count: 122\n",
            "now processing SciDTB-test/P14-1120.edu.out\n",
            "Current text word count: 165\n",
            "now processing SciDTB-test/P16-1112.edu.out\n",
            "Current text word count: 88\n",
            "now processing SciDTB-test/P14-1123.edu.out\n",
            "Current text word count: 108\n",
            "now processing SciDTB-test/D14-1047.edu.out\n",
            "Current text word count: 67\n",
            "now processing SciDTB-test/P16-1124.edu.out\n",
            "Current text word count: 201\n",
            "now processing SciDTB-test/D14-1184.edu.out\n",
            "Current text word count: 179\n",
            "now processing SciDTB-test/D14-1032.edu.out\n",
            "Current text word count: 142\n",
            "now processing SciDTB-test/P16-1141.edu.out\n",
            "Current text word count: 154\n",
            "now processing SciDTB-test/D14-1180.edu.out\n",
            "Current text word count: 114\n",
            "now processing SciDTB-test/D14-1178.edu.out\n",
            "Current text word count: 190\n",
            "\n",
            "The corpus has 152 files.\n",
            "The corpus has 19743 words.\n",
            "\n",
            "The 20 most common words are:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 816),\n",
              " ('the', 784),\n",
              " (',', 689),\n",
              " ('of', 575),\n",
              " ('a', 503),\n",
              " ('and', 491),\n",
              " ('to', 416),\n",
              " ('in', 274),\n",
              " ('that', 240),\n",
              " ('for', 223),\n",
              " ('on', 214),\n",
              " ('We', 200),\n",
              " ('is', 194),\n",
              " (')', 165),\n",
              " ('(', 163),\n",
              " ('with', 147),\n",
              " ('we', 137),\n",
              " ('model', 133),\n",
              " ('are', 114),\n",
              " ('this', 112)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9_z_SzybzH1"
      },
      "source": [
        "Stopwords strike again! We can see just one word which seems specific to the corpus: *model*. We'll remove the stopwords in the next section of the lab.\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> Your turn to write pseudocode, but for a simpler problem this time. Write pseudocode for calculating the average text length (in number of words) in this corpus. You can assume that you already have a list of filesnames, but start from there. You do not have to write the Python code.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> (your answers here)\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di6m9ya2DVU2"
      },
      "source": [
        "##VARIABLES NEEDED:\n",
        "## 1. List of filenames \n",
        "    ## (already done, going to use arcFiles)\n",
        "## 2. List of words for the corpus\n",
        "\n",
        "##Create empty list for total words in the corpus\n",
        "\n",
        "##FOR LOOP:\n",
        "##For each text in corpus:\n",
        "  ##Read in the text as a string\n",
        "  ##Split text into list of words\n",
        "  ##Extend list to previous defined variable\n",
        "\n",
        "##Create variable for each word in the totalwords list\n",
        "  ## w for w in totalwords\n",
        "##Create average calculation variable:\n",
        "  ##average word length = sum(length(word) for word in totalwords) / length(words)\n",
        "  ##print statement of the average length of the words in the corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5ypl21SS4dm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5956269b-d20e-432d-f31d-44589ef72251"
      },
      "source": [
        "totalwords = []\n",
        "\n",
        "for text in arcFiles:\n",
        "  text2str = arcDir+\"/\"+text\n",
        "  with open(text2str, 'rb') as f:\n",
        "    newtext = f.read()\n",
        "\n",
        "  words = newtext.split()\n",
        "  totalwords.extend(words)\n",
        "\n",
        "avgwords = [word for word in totalwords]\n",
        "avg = sum(len(word) for word in avgwords) / len(avgwords)\n",
        "print(\"The average word length in the corpus is:\", avg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The average word length in the corpus is: 5.198972054741825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjkcS0iWdurc"
      },
      "source": [
        "### **BONUS:**\n",
        "\n",
        "> (worth 5 extra points) Write the Python code for your average-text-length pseudocode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7xxpmvp-QTF"
      },
      "source": [
        "## Chaining functions together\n",
        "\n",
        "Next, let's look at how to combine functions into a bigger program. For both of the problems above, we used a very similar process of reading in a text and splitting it into words. Since this is something we may want to do often, let's define a function to do this. The function should take as input the path to the file (this could be just the filename, or the directory plus filename, as we have here), and as output it should return a list of words. The steps in the function are the same as what we did above, including print statements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6QSyRJ4eZe_"
      },
      "source": [
        "### function to split text into list of words\n",
        "### input: filename (or path to file), as string\n",
        "### return: list of words in the file\n",
        "\n",
        "### optionally: uncomment the print statements for more output from the function\n",
        "\n",
        "def wordList(textPath):\n",
        "    #print()\n",
        "    #print(\"Extracting words from:\", textPath)\n",
        "    with open(textPath) as f: \n",
        "        textString = f.read()\n",
        "    \n",
        "    textWords = textString.split()\n",
        "    #print(\"Word count:\", len(textWords))\n",
        "\n",
        "    return textWords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLCLxeFCfAqd"
      },
      "source": [
        "Instead of writing this code inside our `for`-loop, we can simply call our new function, named `wordList`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlku_XTufIwM"
      },
      "source": [
        "corpusWords = []\n",
        "\n",
        "for filename in sciFiles:\n",
        "    path = sciDir+\"/\"+filename\n",
        "    print(\"Now processing:\", path)\n",
        "    fileWords = wordList(path)   ### calling function to get word list\n",
        "    corpusWords.extend(fileWords)\n",
        "\n",
        "print(\"Corpus word count:\", len(corpusWords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsrZnVamftWg"
      },
      "source": [
        "As we start to write code of greater complexity, we'll want to think in terms of combining functions. A common practice is that the output of one function might serve as input to another function. Let's see what this looks like, by writing another function to remove stopwords and punctuation from the word list produced by `wordList()`. The code for removing stopwords and punctuation should be familiar by now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJtRFlCCgKrI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bd1c0597-8ffb-46e2-a058-131a5abfe0e8"
      },
      "source": [
        "### necessary downloads and imports\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "import string\n",
        "enStops = sw.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxvlv3NygBpv"
      },
      "source": [
        "### function to remove stopwords and punctuation\n",
        "### input: list of words\n",
        "### returns: list of words with no stopwords and no punctuation\n",
        "\n",
        "### optionally: uncomment print statement for more output from function\n",
        "\n",
        "def cleanWordList(words):\n",
        "    noStops = [w for w in words if w not in enStops]\n",
        "    noPunct = [w for w in noStops if w not in string.punctuation]\n",
        "    \n",
        "    #print(\"New word count:\", len(noPunct))\n",
        "\n",
        "    return noPunct\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kXtDXYUgxTp"
      },
      "source": [
        "We can add this cleaning step into the `for`-loop. Notice that the output of `wordList()` is the input to `cleanWordList()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caPhZLOsg8VH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39ddee31-8411-4710-a4a6-450b65e398c5"
      },
      "source": [
        "corpusCleanWords = []\n",
        "\n",
        "for filename in sciFiles:\n",
        "    path = sciDir+\"/\"+filename\n",
        "    fileWords = wordList(path)\n",
        "    cleanWords = cleanWordList(fileWords)\n",
        "    corpusCleanWords.extend(cleanWords)\n",
        "\n",
        "print(\"Cleaned corpus word count:\", len(corpusCleanWords))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaned corpus word count: 11812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6jdtHiUhokL"
      },
      "source": [
        "Remember that we can call functions that have been defined anywhere in our program, as long as the program has been defined before we call it. You will often see that data may be sent to many different functions before returning back to the main flow of the program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ_3xEIUh7te"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> With the new, cleaned word list, what are the 20 most common words in the corpus? (Show your code.)\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> code shown below:\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIizdhOcU3z8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9c13db35-3394-4dfc-fc8c-59f24bb20089"
      },
      "source": [
        "len(corpusCleanWords)\n",
        "CleanDist = FreqDist(corpusCleanWords)\n",
        "print(\"The 20 most common words in the cleaned corpus are:\")\n",
        "print(CleanDist.most_common(20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 20 most common words in the cleaned corpus are:\n",
            "[('We', 200), ('model', 133), ('models', 94), ('approach', 93), ('show', 86), ('translation', 83), ('word', 78), ('In', 73), ('language', 72), ('The', 71), ('data', 69), ('semantic', 64), ('using', 63), ('results', 63), ('features', 61), ('task', 59), ('paper', 57), ('Our', 54), ('learning', 54), ('method', 51)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XprPW88FXJNK"
      },
      "source": [
        "# 2b. Error handling and debugging\n",
        "\n",
        "**NOTE:** This section of Lab 6 is primarily reading, with just a couple of exercises.\n",
        "\n",
        "Debugging is the process of modifying your code until it works properly, and it is a *constant companion* to writing code. Some programmers (e.g. my partner) estimate that programming is something like 20% writing new code and 80% testing and debugging. It's something you'll get better and better at over time, as you start to recognize common errors and error patterns. \n",
        "\n",
        "*Think Python* has a short section about debugging at the end of each chapter with tips and strategies for debugging. There's a *lot* of useful information in these sections! In particular, I suggest reading the debugging sections for chapters 2 (sec. 2.8), 3 (sec. 3.12), 5 (sec. 5.12), 6 (sec. 6.9), 7 (sec. 7.7), and 8 (sec. 8.11). It sounds like a lot of reading, but in fact each section shouldn't take more than a few minutes.\n",
        "\n",
        "Two highly-recommended strategies for debugging:\n",
        "\n",
        "* `print` statements\n",
        "* `assert` statements\n",
        "\n",
        "`print` statements should be used to check your variables and data structures at different points in the process, so that you can see whether the code has actually done what you expected it to do (very often the answer is no, and these discrepancies help us pinpoint errors in our code). \n",
        "\n",
        "Using lots of `print` statements can also help to find exactly where something is going wrong. At an extreme, you can `print` a variable before and after each statement that does something to the variable. For example:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1u3eH_ikJj1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "da9983ee-4400-4b3b-aefa-f416fc68503a"
      },
      "source": [
        "mytext = '''If there is a semantic error in your program, it will run without generating error messages, but it will not do the right thing. It will do something else.'''\n",
        "\n",
        "### first we'll see whether split() is doing what's expected\n",
        "print(mytext)\n",
        "mylist = mytext.split()\n",
        "print(mylist)\n",
        "\n",
        "### it looks okay, except for things like the comma after 'program'\n",
        "### let's try removing word-final punctuation - remember rstrip()?\n",
        "### we'll then print the new version of the list\n",
        "\n",
        "newList = [w.rstrip(string.punctuation) for w in mylist]\n",
        "print(newList)\n",
        "\n",
        "### good, that got rid of punctuation after words\n",
        "### now how about stopwords?\n",
        "\n",
        "thirdList = [w for w in newList if w not in enStops]\n",
        "print(thirdList)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "If there is a semantic error in your program, it will run without generating error messages, but it will not do the right thing. It will do something else.\n",
            "['If', 'there', 'is', 'a', 'semantic', 'error', 'in', 'your', 'program,', 'it', 'will', 'run', 'without', 'generating', 'error', 'messages,', 'but', 'it', 'will', 'not', 'do', 'the', 'right', 'thing.', 'It', 'will', 'do', 'something', 'else.']\n",
            "['If', 'there', 'is', 'a', 'semantic', 'error', 'in', 'your', 'program', 'it', 'will', 'run', 'without', 'generating', 'error', 'messages', 'but', 'it', 'will', 'not', 'do', 'the', 'right', 'thing', 'It', 'will', 'do', 'something', 'else']\n",
            "['If', 'semantic', 'error', 'program', 'run', 'without', 'generating', 'error', 'messages', 'right', 'thing', 'It', 'something', 'else']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRrIMw7ClSHO"
      },
      "source": [
        "Another useful strategy is to write an `assert` statement. `assert` is something like a short-hand way of writing an `if`-condition. If the statement following `assert` is true, nothing happens. If the statement following `assert` is false, we'll get an exception/error message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZf-qU4VllFE"
      },
      "source": [
        "### in this example, we want to check whether our list is shorter than 20 items\n",
        "assert len(thirdList) < 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZfIZpXVlx4h"
      },
      "source": [
        "Because the condition (`len(thirdList) < 20`) is true, nothing happens. Let's see what happens with a false condition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nMkll7Zl3du"
      },
      "source": [
        "### in this example, we want to check whether our list is longer than 20 items\n",
        "assert len(thirdList) > 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hv_rbB0l9dK"
      },
      "source": [
        "And we get an error message. `assert` statements can be helpful if we need to make sure some condition is met before proceeding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5TeVkflkGaF"
      },
      "source": [
        "### Exceptions\n",
        "\n",
        "Exceptions are error messages - they are the responses that Python gives us whene there's a problem with our code. Exceptions are very useful in communicating that there are syntax errors (or other runtime errors), but they can be frustrating too. When we encounter an exception, the code halts execution - the program stops running. \n",
        "\n",
        "**READING:** Python Crash Course, chapter 10, pages 200-207.\n",
        "\n",
        "The `try`/`except` structure allows us to handle exceptions without the code crashing to a halt. \n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> a. Exercise 10-6 from *Python Crash Course*: One common problem when prompting for numerical input occurs when people provide text instead of numbers. When you try to convert the input to an `int`, you'll get a `ValueError`. Write a program that prompts the user to input two numbers. Add them together and print the result. Catch the `ValueError` if either input value is not a number, and print a friendly error message. Test your program by entering two numbers and then by entering some text instead of a number.\n",
        "\n",
        "> b. Exercise 10-8 from *Python Crash Course*: Make two files, *cats.txt* and *dogs.txt*. Store at least three names of cats in the first file and three names of dogs in the second file. Write code that tries to read these files and print the contents of the file to the screen. Wrap your code in a `try`-`except` block to catch the `FileNotFound` error, and print a friendly message if a file is missing. Change the name of one of the files, and make sure the code in the `except` block executes correctly.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "code shown below:\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qdDnogYWRs5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cdaefee0-ce2a-45b1-dcac-52c81fb0026d"
      },
      "source": [
        "try:\n",
        "  first = input(\"Give me a number:\")\n",
        "  second = int(first)\n",
        "  third = input(\"And another number:\")\n",
        "  fourth = int(third)\n",
        "\n",
        "except ValueError:\n",
        "  print(\"Sorry, that's not what I'm looking for. Please enter a number.\")\n",
        "\n",
        "else:\n",
        "  adding = first + third\n",
        "  print(\"The sum of \" + str(first) + \" and \" + str(third) + \" equals to \" + adding)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Give me a number:1\n",
            "And another number:2\n",
            "The sum of 1 and 2 equals to 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck53FBbeX0-Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "eae8668b-3c1f-4591-8547-43ef7955dab0"
      },
      "source": [
        "new_file = open('cats.txt', 'w')\n",
        "\n",
        "new_file.writelines(['\\nbella', '\\ntom', '\\nhoney'])\n",
        "new_file.close()\n",
        "\n",
        "second_file = open('dogs.txt', 'w')\n",
        "\n",
        "second_file.writelines(['\\nrocky', '\\nmilo', '\\nzenki'])\n",
        "second_file.close()\n",
        "\n",
        "filenames = ['birds.txt', 'dogs.txt']\n",
        "\n",
        "for filename in filenames:\n",
        "  print(\"\\nOpening file: \" + filename)\n",
        "  try:\n",
        "    with open(filename, 'r') as f:\n",
        "      contents = f.read()\n",
        "      print(contents.upper())\n",
        "  except FileNotFoundError:\n",
        "    print(\"Sorry that file doesn't exist.\")\n",
        "     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Opening file: birds.txt\n",
            "Sorry that file doesn't exist.\n",
            "\n",
            "Opening file: dogs.txt\n",
            "\n",
            "ROCKY\n",
            "MILO\n",
            "ZENKI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKxEwVZzoRNZ"
      },
      "source": [
        "### HOORAY! \n",
        "Congratulations, you've finished Lab 6! Take a breather now before you move on to Lab 7. There are fun and exciting things ahead!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmN0X2IeXLxx"
      },
      "source": [
        "# 3. Classes and objects in Python\n",
        "\n",
        "So far we have been writing code mostly in a way that is usually described as **procedural programming**. In this style of programming, the code consists of a sequence of actions (statements, procedures) to be performed in a specified manner. We accomplish things largely by building data structures, like lists or dictionaries, and then doing things with or to those data structures.\n",
        "\n",
        "In this lab, we'll take a first look at another style of programming called **object-oriented programming (OOP)**. Python allows for both procedural and object-oriented programming, and very often the most effective code is some combination of the two. For today, we will simply learn about two fundamental concepts of OOP, **classes** and **objects**. \n",
        "\n",
        "A **class** is similar to a function in that it must be defined before it can be used. One way to think of a class is as a blueprint - it defines a category that is relevant for the program at hand. A class definition can then be used to create **objects** - each object is an **instance** of the class. The class definition specifies properties of the objects which belong to that class, as well as functions or methods that are specific to the class.\n",
        "\n",
        "It might be helpful to think about an analogy from the non-digital world. If we applied this approach to objects in the real world, we could think about (for example) the class of *birds*. There are many subclasses of birds, like cardinals, herons, penguins, or egrets, and then many individual bird objects which are instances of the class. The bird class definition might specify properties like having classes, laying eggs, or flying, and we'd expect the individual bird instances to have these properties. There are also properties like size or diet that need to be specified for individual bird objects.\n",
        "\n",
        "With this analogy in mind, turn now to chapter 9 of *Python Crash Course* for a very clear, detailed, and specific discussion of classes and objects.\n",
        "\n",
        "**READING:** Python Crash Course, Chapter 9, pages 161-171\n",
        "\n",
        "Try out the code examples as you read the chapter, and the exercises below (from the text). In the next section, we'll apply this knowledge to text.\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> a. Exercise 9-1 from Python Crash Course (modified). Make a class called `Book`. The `__init__()` method for `Book` should store two attributes: a `title` and an `author`. Make a method called `book_info()` that prints these two pieces of information, and a method called `book_available()` that prints a message indicating that the book is available in the library.\n",
        "\n",
        "> b. Create an instance called `book1` from your class. Print the two attributes individually, and then call both methods.\n",
        "\n",
        "> c. Exercise 9-2 from Python Crash Course (modified). Create three different instances from the class, and call `book_info()` for each instance.\n",
        "\n",
        "> d. Exercise 9-4 from Python Crash Course (modified). Add an attribute called `number_reads` with a default value of 0. Create a new instance called `book5` from this class. Print the number of times the book has been read, and then change this value and print it again.\n",
        "\n",
        "> e. Add a method called `set_number_reads()` that lets you set the number of times the book has been read. Call this method with a new number and print the value (`number_reads`) again.\n",
        "\n",
        "> f. Add a method called `increment_number_reads()` that lets you increment the number of customers who've been served. Call this method with any number you like.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "code shown below:\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPNCe-dfe5H8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d37d22d4-476f-496b-9df2-b32b30012ee3"
      },
      "source": [
        "class Book():\n",
        "  def __init__(self, title, author):\n",
        "    self.title = title.title()\n",
        "    self.author = author.title()\n",
        "    self.number_reads = 0\n",
        "\n",
        "  def book_info(self):\n",
        "    msg = self.title + \" written by \" + self.author\n",
        "    print(\"\\n\" + msg)\n",
        "\n",
        "  def book_available(self):\n",
        "    msg = self.title + \" is available at the library.\"\n",
        "    print(\"\\n\" + msg)\n",
        "  \n",
        "  def set_number_reads(self, number_reads):\n",
        "    self.number_reads = number_reads\n",
        "\n",
        "  def increment_number_reads(self, additional_reads):\n",
        "    self.number_reads += additional_reads\n",
        "\n",
        "book1 = Book('in cold blood', 'truman capote')\n",
        "print(book1.title)\n",
        "print(book1.author)\n",
        "\n",
        "book1.book_info()\n",
        "book1.book_available()\n",
        "\n",
        "book5 = Book('in our time', 'ernest hemingway')\n",
        "book5.book_info()\n",
        "\n",
        "print(\"\\nNumber Read: \" + str(book5.number_reads))\n",
        "\n",
        "book5.number_reads=9\n",
        "print(\"Number Read: \" + str(book5.number_reads))\n",
        "\n",
        "book5.set_number_reads(12)\n",
        "print(\"Number Read:\" + str(book5.number_reads))\n",
        "\n",
        "book5.increment_number_reads(3)\n",
        "print(\"Number Read:\" + str(book5.number_reads))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In Cold Blood\n",
            "Truman Capote\n",
            "\n",
            "In Cold Blood written by Truman Capote\n",
            "\n",
            "In Cold Blood is available at the library.\n",
            "\n",
            "In Our Time written by Ernest Hemingway\n",
            "\n",
            "Number Read: 0\n",
            "Number Read: 9\n",
            "Number Read:12\n",
            "Number Read:15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnERV5xarQpy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c0eb3c65-172c-46e1-f729-324f9747e076"
      },
      "source": [
        "truman = Book('in cold blood', 'truman capote')\n",
        "king = Book('carrie', 'stephen king')\n",
        "plath = Book('the bell jar', 'sylvia plath')\n",
        "\n",
        "truman.book_info()\n",
        "king.book_info()\n",
        "plath.book_info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "In Cold Blood written by Truman Capote\n",
            "\n",
            "Carrie written by Stephen King\n",
            "\n",
            "The Bell Jar written by Sylvia Plath\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04NdCGjyXP4W"
      },
      "source": [
        "# 4. Text preprocessing with spaCy\n",
        "\n",
        "And now we get to the big payoff, the really exciting stuff: preprocessing and using the spaCy toolkit (https://spacy.io/). spaCy is a Python library for natural language processing (NLP) that is easy to install, easy to use, and super fast and powerful to boot. It's widely used, both in research and in industry. There are many, many useful functions available through spaCy, and our focus for today is a set of methods for preprocessing text.\n",
        "\n",
        "As the name suggests, preprocessing is the stuff we do *before* processing - these are steps we take to get raw text ready for more complicated processing steps. Today we will look at four different preprocessing steps: tokenization, sentence splitting, lemmatization, and part-of-speech (POS) tagging.\n",
        "\n",
        "Though each of these steps seems quite simple to perform, that's because we are looking at them through the eyes of a human speaker of language. The tasks performed in preprocessing are not at all trivial for a machine, and doing them automatically will certainly lead to some errors and some noise in the data. At the same time, the amount of mistakes is usually small enough that it doesn't negatively effect the outcome of the later processing steps (at least for English). \n",
        "\n",
        "### Why preprocessing matters\n",
        "\n",
        "The tasks that we perform in preprocessing involve a number of decisions. For example, if we come across the word *didn't*, do we want to treat that as one token or two? If we treat it as just one token, we're saying that *did* and *didn't* are two different word types, and we are ignoring the fact that the *-n't* at the end of *didn't* has the same meaning as that same suffix on other words like *can't* or *wouldn't*. If we then treat it as two tokens, how do we represent the second token - is it *n't* or *-n't* or *not*? Some of these decisions don't have a huge impact on our processing of text, but it is **essential* that we make the same decision every time we see a word ending in *-n't*. \n",
        "\n",
        "Handling preprocessing carefully leads to better, more consistent analyses with more reliable results. \n",
        "\n",
        "### Preprocessing is largely language-specific\n",
        "\n",
        "Many preprocessing steps rely on language-specific knowledge. For example, we can (mostly) use white space to separate words in English, but there are (usually) no spaces between words in Mandarin Chinese. Libraries for doing preprocessing need to keep in mind the properties of the language that the text is written in. (And that's assuming our texts are written in only one language!)\n",
        "\n",
        "Now let's look at each of these steps in turn. For each one, we'll read a little bit about the motivation for that step, we'll see some examples, and we'll see how to use spaCy to get it done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPe6k2g_xA5N"
      },
      "source": [
        "##Preliminaries: import spaCy and an English-language model (from spaCy)\n",
        "\n",
        "Just like NLTK, spaCy needs to be imported as a Python library. After importing `spacy`, our next step is to load a model. Here we're using the smaller version of the core English language model, trained on web data. \n",
        "\n",
        "To learn about other English models available through spaCy: https://spacy.io/models/en -- there are three sizes: small, medium, and large. There are models for some other languages too: https://spacy.io/usage/models\n",
        "\n",
        "Loading the model creates a `Language` object (that's right, an instance of the spacy-defined class `Language`) the contains all of the data and the components needed to process text in that language. By convention, usually we assign this object to the variable `nlp`, because this is the object that we'll use to process texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq5SW01EEEge"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLpFVzMj051W"
      },
      "source": [
        "**READING:** Speech and Language Processing:\n",
        "\n",
        "Chapter 2: https://web.stanford.edu/~jurafsky/slp3/2.pdf\n",
        "\n",
        "* sections 2.4-2.4.2 (tokenization)\n",
        "* section 2.4.5 (sentence splitting)\n",
        "* section 2.4.4 (lemmatization)\n",
        "\n",
        "Chapter 8: https://web.stanford.edu/~jurafsky/slp3/8.pdf\n",
        "\n",
        "* section 8.1 (word classes in English)\n",
        "* section 8.2 (the Penn Treebank tagset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naxD-ez4t624"
      },
      "source": [
        "## Tokenization & sentence splitting\n",
        "\n",
        "Two of the most essential preprocessing steps are **tokenization** and **sentence splitting**. Tokenization involves splitting a long string of text into its individual tokens (including splitting off punctuation), and sentence splitting is just what it sounds like - dividing that string of text into sentences. To better understand how important these are, imagine that we didn't have these kinds of divisions. Any text - whether it's a haiku, Moby Dick, or the entire Bible - would just be one long string, and viewed by the computer as just one long sequence of characters, with a bunch of spaces and other white space. Breaking the text into sentences, and then into tokens, adds structure to otherwise unstructured text. \n",
        "\n",
        "For both tokenization and sentence splitting, the way that English text is written provides some pretty strong cues, but they're not perfect.\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> a. What is the main cue for splitting English documents into sentences? When and how might it go wrong? (No coding required.)\n",
        "\n",
        "> b. What is the main cue in English for splitting sentences into tokens? When and how might it go wrong? (No coding required.)\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> a. I think the main cue for sentence segmentation is punctuation (periods, question marks, exclamation points, etc.) which usually signify the end of a sentence. A pretty common problem here are using periods as cues, since they can be pretty ambiguous when identifying a sentence boundary or marking an abbreviation.\n",
        "\n",
        "> b. Splitting sentence into tokens is similarily cued by punctuations, specifically periods. Identifying whether a period is marking an abbreviation or the end of the sentence would be the first step for tokenizing.\n",
        "\n",
        "-----\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWw0zU4g08qW"
      },
      "source": [
        "To process new text, we give the text (as a string) as an argument to the `nlp` object. It will return a processed version of the document. The processed document is an object of spacy's `Doc` class and offers many useful attributes and functionalities. As part of this processing, the text is split into tokens (we would say 'the text gets tokenized'), which we can see by iterating through the tokens of the document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwAuEi8y1vxW"
      },
      "source": [
        "mystring = \"Denton, Texas has been my home for four years. I'm a professor at the Univ. of North Texas, in the Linguistics Department. My cat's name is Bella, and you have all seen her on zoom.\"\n",
        "\n",
        "mydoc = nlp(mystring)\n",
        "for token in mydoc:\n",
        "    print(token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIBA2xvs2WXM"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> What do you notice about how spacy tokenizes punctuation? Anything that surprises you? (No coding required.)\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> I've worked with spacy tokenization before so no surprises here.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCPPZwgq2jez"
      },
      "source": [
        "Similarly, the document has already been split into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf4DfDFi2nEk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ab4a35e0-a4cb-40cd-da8e-63682e6de313"
      },
      "source": [
        "sentences = list(mydoc.sents)\n",
        "for s in sentences:\n",
        "    print(s.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Denton, Texas has been my home for four years.\n",
            "I'm a professor at the Univ.\n",
            "of North Texas, in the Linguistics Department.\n",
            "My cat's name is Bella, and you have all seen her on zoom.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZRShVd121yu"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Any problems with the sentence splitting?\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> Yes, again identifying a period as an abbreviation or end-of-sentence marker is the typical problem. So the abbreviation Univ. is split into two sentences instead of one.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlLK8d0zt9e8"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "For each token that spaCy identifies, it also determines the lemma, or base form of the word. Just like `text`, `lemma` is also an attribute of the token. For many of the token attributes (lemma, pos, tag, etc.) we need to add an underscore after the name of the attribute. Adding the underscore tells Python that we want to see the human-readable form of the lemma rather than spacy's internal representation. (Try without the _ and see what happens!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYvSJqaH3THX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "fee3af11-6f96-4c6c-afb9-d206067a7730"
      },
      "source": [
        "for token in mydoc:\n",
        "    print(token.text, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Denton Denton\n",
            ", ,\n",
            "Texas Texas\n",
            "has have\n",
            "been be\n",
            "my -PRON-\n",
            "home home\n",
            "for for\n",
            "four four\n",
            "years year\n",
            ". .\n",
            "I -PRON-\n",
            "'m be\n",
            "a a\n",
            "professor professor\n",
            "at at\n",
            "the the\n",
            "Univ Univ\n",
            ". .\n",
            "of of\n",
            "North North\n",
            "Texas Texas\n",
            ", ,\n",
            "in in\n",
            "the the\n",
            "Linguistics Linguistics\n",
            "Department Department\n",
            ". .\n",
            "My -PRON-\n",
            "cat cat\n",
            "'s 's\n",
            "name name\n",
            "is be\n",
            "Bella Bella\n",
            ", ,\n",
            "and and\n",
            "you -PRON-\n",
            "have have\n",
            "all all\n",
            "seen see\n",
            "her -PRON-\n",
            "on on\n",
            "zoom zoom\n",
            ". .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0graGOL-3woj"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> What do you see that's interesting in the lemmas associated with the tokens? Describe at least two different cases where the text and the lemma are different (for many words, the two are identical).\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> For most of these the text and lemma are identical - however, with pronouns the lemma is only labeled by the type PRON since there is no clear base form for pronouns in English. Another difference is with plurals (years, year) which spacy seems to handle fine.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDioalYat-yD"
      },
      "source": [
        "## Part-of-speech (POS) tagging\n",
        "\n",
        "The part-of-speech of a word captures a lot of information about how the word behaves. Sometimes it can be very useful to be able to distinguish nouns from verbs, or adjectives from nouns. Other times, we may want to count the relative proportion of certain POS categories. For example, authors may differ in how descriptive their texts are, and we might try to measure that by looking at the relative proportion of adjectives in a text.\n",
        "\n",
        "For each token, spaCy provides two different POS tags. `pos` is a coarse-grained, very general part of speech. `tag` is a fine-grained, more specific part of speech. For example, `VERB` is a coarse-grained part of speech, and the fine-grained categories associated with verb specify things like tense (past, present, future) and person (first person, second person, third person)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yapIITNI52lh"
      },
      "source": [
        "for token in mydoc:\n",
        "    print(token.text, token.pos_, token.tag_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br499Gne5-kh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e7cfb550-dca2-4f0a-f9b5-ca1b71c46569"
      },
      "source": [
        "### spacy offers a method for getting explanations of any POS tag\n",
        "print(spacy.explain('PART'))\n",
        "print(spacy.explain('POS'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "particle\n",
            "possessive ending\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie316mOSuLO0"
      },
      "source": [
        "## Other useful functions in spaCy\n",
        "\n",
        "### Noun chunks:\n",
        "\n",
        "spaCy can produce a list of noun chunks in a text:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU8nVJqj6VRZ"
      },
      "source": [
        "noun_chunks = list(mydoc.noun_chunks)\n",
        "for chunk in noun_chunks:\n",
        "    print(chunk.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh8X_eOF6pFJ"
      },
      "source": [
        "### Named entities\n",
        "\n",
        "The document produced by spaCy includes automatic recognition and classification of named entities in the text. There are many different types of named entities, from language names to works of art, to people, organizations, and geo-political entities (like countries, cities, states). You can see the named entities by iterating through the relevant attribute (`ents`) of the document, or by using spaCy's visualizer (which is called, cutely, *displacy*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73vSiq3C7euJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "53e6786b-9b93-4056-d786-b2374f63a42e"
      },
      "source": [
        "for ent in mydoc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Denton GPE\n",
            "Texas GPE\n",
            "four years DATE\n",
            "North Texas LOC\n",
            "the Linguistics Department ORG\n",
            "Bella PERSON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6A1HHXT729O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "5aaf4b69-1291-4a5c-cb16-85eae8673bd5"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(mydoc, style=\"ent\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Denton\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Texas\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " has been my home for \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    four years\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". I'm a professor at the Univ. of \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    North Texas\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ", in \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Linguistics Department\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". My cat's name is \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bella\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", and you have all seen her on zoom.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnHdUWnp8DNl"
      },
      "source": [
        "Finally, each token in the processed document is realized as an instance of the `Token` class, and there are a large number of attributes available through that class. Some of them are illustrated below, and more of them are described here: https://spacy.io/api/token#attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN8BxABoEsNJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "426faff6-170d-416f-c049-75e8dcba7a07"
      },
      "source": [
        "### first we'll pick one token to look at\n",
        "denton = mydoc[0]\n",
        "\n",
        "print(\"Text:\", denton.text)\n",
        "print(\"Lemma:\", denton.lemma_)\n",
        "print(\"Coarse-grained POS tag:\", denton.pos_)\n",
        "print(\"Fine-grained POS tag:\", denton.tag_)\n",
        "print(\"Word shape:\", denton.shape_)\n",
        "\n",
        "### some attributes are Boolean\n",
        "print(\"Alphabetic characters?\", denton.is_alpha)\n",
        "print(\"Punctuation mark?\", denton.is_punct)\n",
        "print(\"Digit?\", denton.is_digit)\n",
        "print(\"Like a number?\", denton.like_num)\n",
        "print(\"Is it a stopword?\", denton.is_stop)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: Denton\n",
            "Lemma: Denton\n",
            "Coarse-grained POS tag: PROPN\n",
            "Fine-grained POS tag: NNP\n",
            "Word shape: Xxxxx\n",
            "Alphabetic characters? True\n",
            "Punctuation mark? False\n",
            "Digit? False\n",
            "Like a number? False\n",
            "Is it a stopword? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSfcIsGk7O-S"
      },
      "source": [
        "This is just the start of what we'll do with spaCy. There's a *huge* amount of documentation on the spaCy website. If you're eager to learn more right away, I'd suggest these two links as good places to start:\n",
        "\n",
        " * spaCy 101 (free interactive course): https://spacy.io/usage/spacy-101\n",
        " * more on linguistic features: https://spacy.io/usage/linguistic-features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKcpxhjcXUhU"
      },
      "source": [
        "# 5. Putting it together\n",
        "\n",
        "TIP: You may want to first write pseudocode for section of the lab.\n",
        "\n",
        "For this final section of the lab, you will pick out 10 documents from the mini-corpus and run them through spaCy. Using spaCy functions (most of these are already built in to spaCy), do the following tasks:\n",
        "\n",
        "* Count the number of sentences in the subcorpus (your 10 documents)\n",
        "* Print all of the noun chunks in your subcorpus\n",
        "* Print all of the named entities in your subcorpus\n",
        "\n",
        "Do you see anything unexpected in the output?\n",
        "\n",
        "Next, make a list of all of the lemmas in your corpus. Build a frequency distribution from this list. What are the top 10 most-frequent lemmas? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG7lEk3fArFN"
      },
      "source": [
        "TIP: once you have created a list of filenames (I've called mine `mytexts`), you can use this code to convert the ten texts to one big string. This string will be the input to the `nlp()` function. \n",
        "\n",
        "Click 'SHOW CODE' to see the code (I've hidden the block, in case people want to work this out on their own first). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7oCKwjm_eov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "ff8f0d88-cb86-4049-8391-bdc734f9376e"
      },
      "source": [
        "#@title\n",
        "# first create a list of texts  - I've selected the first 10 files\n",
        "# from sciDTB-test \n",
        "mytexts = sciFiles[:10]\n",
        "\n",
        "# now we'll read each text in, one at a time\n",
        "# we are creating a string (corpusString) to hold the text of \n",
        "# the selected files - each file gets read in as one big string\n",
        "# and added to corpusString\n",
        "# \n",
        "# we use the variable sum to collect a count of the characters\n",
        "\n",
        "corpusString = ''\n",
        "sum = 0\n",
        "for text in mytexts:\n",
        "    this = sciDir+\"/\"+text\n",
        "    with open(this) as f:\n",
        "        thisString = f.read()\n",
        "    corpusString = corpusString + thisString\n",
        "    sum = sum + len(thisString)\n",
        "\n",
        "print(corpusString)\n",
        "print(len(corpusString))\n",
        "print(sum)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatic metrics are widely used in machine translation as a substitute for human assessment . With the introduction of any new metric comes the question of just how well that metric mimics human assessment of translation quality . This is often measured by correlation with human judgment . Significance tests are generally not used to establish whether improvements over existing methods such as BLEU are statistically significant or have occurred simply by chance , however . In this paper , we introduce a significance test for comparing correlations of two metrics , along with an open-source implementation of the test . When applied to a range of metrics across seven language pairs , tests show that for a high proportion of metrics , there is insufficient evidence to conclude significant improvement over BLEU . \n",
            "This paper presents neural probabilistic parsing models which explore up to thirdorder graph-based parsing with maximum likelihood training criteria . Two neural network extensions are exploited for performance improvement . Firstly , a convolutional layer that absorbs the influences of all words in a sentence is used so that sentence-level information can be effectively captured . Secondly , a linear layer is added to integrate different order neural models and trained with perceptron method . The proposed parsers are evaluated on English and Chinese Penn Treebanks and obtain competitive accuracies . \n",
            "We propose a simple unsupervised approach to detecting non-compositional components in multiword expressions based on Wiktionary . The approach makes use of the definitions , synonyms and translations in Wiktionary , and is applicable to any type of MWE in any language , assuming the MWE is contained in Wiktionary . Our experiments show that the proposed approach achieves higher F-score than state-of-the-art methods . \n",
            "We present a novel approach for automatic report generation from time-series data , in the context of student feedback generation . Our proposed methodology treats content selection as a multi-label ( ML ) classification problem , which takes as input time-series data and outputs a set of templates , while capturing the dependencies between selected templates . We show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher F-score than multiple simple classifiers that keep a history of selected templates . Furthermore , we compare a ML classifier with a Reinforcement Learning ( RL ) approach in simulation and using ratings from real student users . We show that the different methods have different benefits , with ML being more accurate for predicting what was seen in the training data , whereas RL is more exploratory and slightly preferred by the students . \n",
            "Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language , often only poorly captured by existing word translation models . We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy . Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering . We report significant improvements in word translation prediction accuracy for three morphologically rich target languages . In addition , preliminary results for integrating our approach into a large-scale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality . \n",
            "Distinct properties of translated text have been the subject of research in linguistics for many year ( Baker , 1993 ) . In recent years computational methods have been developed to empirically verify the linguistic theories about translated text ( Baroni and Bernardini , 2006 ) . While many characteristics of translated text are more apparent in comparison to the original text , most of the prior research has focused on monolingual features of translated and original text . The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level , rather than using monolingual statistics at the document level . We show that these bilingual features out-perform the monolingual features used in prior work ( Kurokawa et al. , 2009 ) for the task of classifying translation direction . \n",
            "We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( CNN ) trained on a large labeled object recognition dataset . This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach . Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks . We use visual features computed using either ImageNet or ESP Game images . \n",
            "This work presents two different translation models using recurrent neural networks . The first one is a word-based approach using word alignments . Second , we present phrase-based translation models that are more consistent with phrase-based decoding . Moreover , we introduce bidirectional recurrent neural models to the problem of machine translation , allowing us to use the full source sentence in our models , which is also of theoretical interest . We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks : IWSLT 2013 German to English , BOLT Arabic to English and Chinese to English . We obtain gains up to 1.6 % BLEU and 1.7 % TER by rescoring 1000-best lists . \n",
            "Since larger n-gram Language Model ( LM ) usually performs better in Statistical Machine Translation ( SMT ) , how to construct efficient large LM is an important topic in SMT . However , most of the existing LM growing methods need an extra monolingual corpus , where additional LM adaption technology is necessary . In this paper , we propose a novel neural network based bilingual LM growing method , only using the bilingual parallel corpus in SMT . The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT , and significantly outperforms the existing LM growing methods without extra corpus . \n",
            "We present WIKIREADING , a large-scale natural language understanding task and publicly-available dataset with 18 million instances . The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles . The task contains a rich variety of challenging classification and extraction sub-tasks , making it well-suited for end-to-end models such as deep neural networks ( DNNs ) . We compare various state-of-the-art DNN based architectures for document classification , information extraction , and question answering . We find that models supporting a rich answer space , such as word or character sequences , perform best . Our best-performing model , a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words , obtains an accuracy of 71.8 % . \n",
            "\n",
            "7469\n",
            "7469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmDLnsOD6mGN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e63bef54-971c-49cd-f95b-f4da61fa4eb3"
      },
      "source": [
        "new_texts = sciFiles[20:30]\n",
        "corpus_string = ''\n",
        "sum = 0\n",
        "for text in new_texts:\n",
        "  path = sciDir+\"/\"+text\n",
        "  with open(path) as f:\n",
        "    new_string = f.read()\n",
        "  corpus_string = corpus_string + new_string\n",
        "  sum = sum + len(new_string)\n",
        "\n",
        "# print(len(new_texts))\n",
        "print(corpus_string)\n",
        "print(\"Number of sentences:\", len(corpus_string))\n",
        "print(\"Number of characters:\", sum)\n",
        "\n",
        "newdoc = nlp(corpus_string)\n",
        "nc = list(newdoc.noun_chunks)\n",
        "print(\"Noun Chunks in the subcorpus:\")\n",
        "print(nc)\n",
        "\n",
        "print()\n",
        "for ent in newdoc.ents:\n",
        "  print(ent.text, ent.label_)\n",
        "\n",
        "print()\n",
        "lemmas = []\n",
        "for token in newdoc:\n",
        "  lemmas.append(token.lemma_)\n",
        "# print(len(lemmas))\n",
        "lemmadist = FreqDist(lemmas)\n",
        "print(\"The top 10 most frequent lemmas are:\")\n",
        "lemmadist.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We propose the first probabilistic approach to modeling cross-lingual semantic similarity ( CLSS ) in context which requires only comparable data . The approach relies on an idea of projecting words and sets of words into a shared latent semantic space spanned by language-pair independent latent semantic concepts ( e.g. , cross-lingual topics obtained by a multilingual topic model ) . These latent cross-lingual concepts are induced from a comparable corpus without any additional lexical resources . Word meaning is represented as a probability distribution over the latent concepts , and a change in meaning is represented as a change in the distribution over these latent concepts . We present new models that modulate the isolated out-of-context word representations with contextual knowledge . Results on the task of suggesting word translations in context for 3 language pairs reveal the utility of the proposed contextualized models of cross-lingual semantic similarity . \n",
            "Left-to-right ( LR ) decoding ( Watanabe et al. , 2006 ) is promising decoding algorithm for hierarchical phrase-based translation ( Hiero ) that visits input spans in arbitrary order producing the output translation in left to right order . This leads to far fewer language model calls , but while LR decoding is more efficient than CKY decoding , it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result . This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero . \n",
            "We investigate how to improve bilingual embedding which has been successfully used as a feature in phrase-based statistical machine translation ( SMT ) . Despite bilingual embedding's success , the contextual information , which is of critical importance to translation quality , was ignored in previous work . To employ the contextual information , we propose a simple and memory-efficient model for learning bilingual embedding , taking both the source phrase and context around the phrase into account . Bilingual translation scores generated from our proposed bilingual embedding model are used as features in our SMT system . Experimental results show that the proposed method achieves significant improvements on large-scale Chinese-English translation task . \n",
            "Most machine translation systems construct translations from a closed vocabulary of target word forms , posing problems for translating into languages that have productive compounding processes . We present a simple and effective approach that deals with this problem in two phases . First , we build a classifier that identifies spans of the input text that can be translated into a single compound word in the target language . Then , for each identified span , we generate a pool of possible compounds which are added to the translation model as `` synthetic '' phrase translations . Experiments reveal that ( i ) we can effectively predict what spans can be compounded ; ( ii ) our compound generation model produces good compounds ; and ( iii ) modest improvements are possible in end-to-end English-German and English-Finnish translation tasks . We additionally introduce KomposEval , a new multi-reference dataset of English phrases and their translations into German compounds . \n",
            "This paper complements semantic role representations with spatial knowledge beyond indicating plain locations . Namely , we extract where entities are ( and are not ) located , and for how long ( seconds , hours , days , etc. ) . Crowdsourced annotations show that this additional knowledge is intuitive to humans and can be annotated by non-experts . Experimental results show that the task can be automated . \n",
            "Intelligent assistants on mobile devices , such as Siri , have recently gained considerable attention as novel applications of dialogue technologies . A tremendous amount of real users of intelligent assistants provide us with an opportunity to explore a novel task of predicting whether users will continually use their intelligent assistants in the future . We developed prediction models of prospective user engagement by using large-scale user logs obtained from a commercial intelligent assistant . Experiments demonstrated that our models can predict prospective user engagement reasonably well , and outperforms a strong baseline that makes prediction based past utterance frequency \n",
            "Learning from errors is a crucial aspect of improving expertise . Based on this notion , we discuss a robust statistical framework for analysing the impact of different error types on machine translation ( MT ) output quality . Our approach is based on linear mixed-effects models , which allow the analysis of error-annotated MT output taking into account the variability inherent to the specific experimental setting from which the empirical observations are drawn . Our experiments are carried out on different language pairs involving Chinese , Arabic and Russian as target languages . Interesting findings are reported , concerning the impact of different error types both at the level of human perception of quality and with respect to performance results measured with automatic metrics . \n",
            "Generative word alignment models , such as IBM Models , are restricted to one-to-many alignment , and cannot explicitly represent many-to-many relationships in a bilingual text . The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other . In this paper , we focus on the posterior regularization frame-work ( Ganchev et al. , 2010 ) that can force two directional word alignment models to agree with each other during training , and propose new constraints that can take into account the difference between function words and content words . Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline . We also observed gains in Japanese-to-English translation tasks , which prove the effectiveness of our methods under grammatically different language pairs . \n",
            "We present a method to jointly learn features and weights directly from distributional data in a log-linear framework . Specifically , we propose a non-parametric Bayesian model for learning phonological markedness constraints directly from the distribution of input-output mappings in an Optimality Theory ( OT ) setting . The model uses an Indian Buffet Process prior to learn the feature values used in the log-linear method , and is the first algorithm for learning phonological constraints without presupposing constraint structure . The model learns a system of constraints that explains observed data as well as the phonologically-grounded constraints of a standard analysis , with a violation structure corresponding to the standard constraints . These results suggest an alternative data-driven source for constraints instead of a fully innate constraint set . \n",
            "Recently , many NLP tasks have benefited from distributed word representation . However , it remains unknown whether embedding models are really immune to the typological diversity of languages , despite the language-independent architecture . Here we investigate three representative models on a large set of language samples by mapping dense embedding to sparse linguistic property space . Experiment results reveal the language universal and specific properties encoded in various word representation . Additionally , strong evidence supports the utility of word form , especially for inflectional languages . \n",
            "\n",
            "Number of sentences: 7714\n",
            "Number of characters: 7714\n",
            "Noun Chunks in the subcorpus:\n",
            "[We, the first probabilistic approach, cross-lingual semantic similarity, CLSS, context, only comparable data, The approach, an idea, words, sets, words, a shared latent semantic space, language-pair independent latent semantic concepts, ( e.g. , cross-lingual topics, a multilingual topic model, These latent cross-lingual concepts, a comparable corpus, any additional lexical resources, Word meaning, a probability distribution, the latent concepts, a change, meaning, a change, the distribution, these latent concepts, We, new models, context, contextual knowledge, Results, the task, word translations, context, 3 language pairs, the utility, the proposed contextualized models, cross-lingual semantic similarity, decoding algorithm, hierarchical phrase-based translation, Hiero, input spans, arbitrary order, the output translation, right order, far fewer language model calls, LR decoding, CKY decoding, it, some hierarchical phrase alignments, lower translation quality, a result, This paper, two improvements, it, translation quality, CKY-based Hiero, We, bilingual embedding, a feature, phrase-based statistical machine translation, SMT, bilingual embedding's success, the contextual information, critical importance, translation quality, previous work, the contextual information, we, a simple and memory-efficient model, bilingual embedding, both the source phrase, context, the phrase, account, Bilingual translation scores, our proposed bilingual embedding model, features, our SMT system, Experimental results, the proposed method, significant improvements, large-scale Chinese-English translation task, Most machine translation systems, translations, a closed vocabulary, target word forms, problems, languages, productive compounding processes, We, a simple and effective approach, this problem, two phases, we, a classifier, spans, the input text, a single compound word, the target language, each identified span, we, a pool, possible compounds, the translation model, `` synthetic '' phrase translations, Experiments, we, what, spans, our compound generation model, good compounds, ( iii ) modest improvements, end, We, KomposEval, a new multi-reference dataset, English phrases, their translations, German compounds, This paper, semantic role representations, spatial knowledge, plain locations, we, entities, how long ( seconds, hours, days, Crowdsourced annotations, this additional knowledge, humans, non-experts, Experimental results, the task, Intelligent assistants, mobile devices, Siri, considerable attention, novel applications, dialogue technologies, A tremendous amount, real users, intelligent assistants, us, an opportunity, a novel task, users, their intelligent assistants, the future, We, prediction models, prospective user engagement, large-scale user logs, a commercial intelligent assistant, Experiments, our models, prospective user engagement, a strong baseline, prediction, utterance frequency, errors, a crucial aspect, expertise, this notion, we, a robust statistical framework, the impact, different error types, machine translation, ( MT, output quality, Our approach, linear mixed-effects models, the analysis, MT output, account, the variability, the specific experimental setting, the empirical observations, Our experiments, different language pairs, Russian, target languages, Interesting findings, the impact, different error types, the level, human perception, quality, respect, performance results, automatic metrics, Generative word alignment models, IBM Models, one-to-many alignment, many-to-many relationships, a bilingual text, The problem, heuristics, agreement constraints, two directional word alignments, this paper, we, the posterior regularization frame-work, ( Ganchev et al., two directional word alignment models, training, new constraints, account, the difference, function words, content words, Experimental results, French-to-English and Japanese-to-English alignment tasks, statistically significant gains, the previous posterior regularization baseline, We, gains, Japanese-to-English translation tasks, the effectiveness, our methods, grammatically different language pairs, We, a method, features, weights, distributional data, a log-linear framework, we, a non-parametric Bayesian model, phonological markedness constraints, the distribution, input-output mappings, ( OT, The model, an Indian Buffet Process, the feature values, the log-linear method, the first algorithm, phonological constraints, constraint structure, The model, a system, constraints, observed data, the phonologically-grounded constraints, a standard analysis, a violation structure, the standard constraints, These results, an alternative data-driven source, constraints, a fully innate constraint set, many NLP tasks, distributed word representation, it, embedding models, the typological diversity, languages, the language-independent architecture, we, three representative models, a large set, language samples, linguistic property space, Experiment results, the language, universal and specific properties, various word representation, strong evidence, the utility, word form, inflectional languages]\n",
            "\n",
            "first ORDINAL\n",
            "CLSS ORG\n",
            "3 CARDINAL\n",
            "Watanabe et al. PERSON\n",
            "2006 DATE\n",
            "algorithm ORG\n",
            "Hiero PERSON\n",
            "LR PERSON\n",
            "CKY ORG\n",
            "CKY ORG\n",
            "two CARDINAL\n",
            "LR PERSON\n",
            "CKY ORG\n",
            "Hiero EVENT\n",
            "Chinese-English NORP\n",
            "two CARDINAL\n",
            "First ORDINAL\n",
            "English-German NORP\n",
            "English LANGUAGE\n",
            "KomposEval ORG\n",
            "English LANGUAGE\n",
            "German NORP\n",
            "seconds TIME\n",
            "hours TIME\n",
            "Siri PERSON\n",
            "Chinese NORP\n",
            "Arabic LANGUAGE\n",
            "Russian LANGUAGE\n",
            "IBM Models ORG\n",
            "one CARDINAL\n",
            "two CARDINAL\n",
            "Ganchev et al. PERSON\n",
            "2010 DATE\n",
            "two CARDINAL\n",
            "French NORP\n",
            "English LANGUAGE\n",
            "Japanese NORP\n",
            "to-English NORP\n",
            "Japanese NORP\n",
            "English LANGUAGE\n",
            "Indian NORP\n",
            "first ORDINAL\n",
            "NLP ORG\n",
            "three CARDINAL\n",
            "\n",
            "The top 10 most frequent lemmas are:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 50),\n",
              " ('the', 43),\n",
              " ('-', 42),\n",
              " (',', 42),\n",
              " ('-PRON-', 30),\n",
              " ('of', 30),\n",
              " ('a', 30),\n",
              " ('to', 28),\n",
              " ('be', 28),\n",
              " ('and', 22)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7ej0sNaYhxX"
      },
      "source": [
        "6. REMINDER: Wrapping up and submitting\n",
        "\n",
        "Create a revision by going to **File > Save and Pin Revision**.\n",
        "\n",
        "View your revision history at **File > Revision History**.\n",
        "\n",
        "To submit: go to **Share** in the upper right corner, click **Get Shareable Link**, change the dropdown menu option to **Anyone with the link can edit**, and then **Copy Link**! This is what you'll submit on Canvas."
      ]
    }
  ]
}