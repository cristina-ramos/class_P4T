{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Py4Text_Lab5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qkj5P4Ja415w"
      },
      "source": [
        "# Functions and Files - Lab 5, Python for Text, Fall 2020\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "To be completed and submitted by end of the day on Monday, October 5.\n",
        "\n",
        "The questions you need to answer are marked with **QUESTION**. For each one, there's a space (under **ANSWER**) for you to add your answer, which might be text, might be code, or might be a mix of the two. \n",
        "\n",
        "\n",
        "\n",
        "### Have questions?\n",
        "\n",
        "1. Use the Canvas discussion boards.\n",
        "\n",
        "## Reminder\n",
        "\n",
        "You will need to submit your own notebook (or rather, a link to your own notebook). There are (at least) two ways to do this:\n",
        "\n",
        "1. Make a copy of this notebook, rename it, and add new code cells when you want to write your own code.\n",
        "1. Create a new Python 3 notebook using the `File` menu and type in all cells yourself. \n",
        "\n",
        "Either way, your file should be named using this format: `LastName_Py4Text_Lab5.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRphYoQ7jHw_"
      },
      "source": [
        "#Overview: Beginning to build up a system\n",
        "{this lab is a bit shorter than the usual lab, so we can stay on schedule}\n",
        "\n",
        "This week we'll move from writing one command at a time to building up larger blocks of code. We'll focus on **functions**, which are reusable blocks of code. Functions give a number of benefits:\n",
        "\n",
        "1. Very often we want to do some things with code multiple times - functions can save us from writing this same code over and over again. Instead, we can write it just once and **call the function** multiple times.\n",
        "1. Functions help us to organize our thinking (and our code). Each function should perform one task. The task can have multiple steps, but these steps should all be related to the main goal of the function.\n",
        "1. Functions can be reused across programs (and across notebooks).\n",
        "\n",
        "I like to thank of functions like magic spells - once you've learned a spell to turn someone into a hedgehog, you can do that over and over again, simply by saying the name of the spell. (look, hedgehogs everywhere!) Once we have defined a function, we can call it over and over again, with different **arguments**.\n",
        "\n",
        "This week you'll also learn how to get more informative frequency counts by removing punctuation and highly-frequent (but not highly-informative) words like and or of - these are known as **stopwords**.\n",
        "\n",
        "Along the way, you'll get practice working with files in Python. You'll get to know **file input operations** (methods for reading in data from files) and **file output operations** (methods for writing data to external files). Learning these operations gives you the ability to work on data of your own choosing, and also to store your results for further analysis. For now, we're working with plain text files. Later in October we'll learn about working with other file formats, like .xml and .json.\n",
        "\n",
        "Things you should be able to do at the end of this lab:\n",
        "\n",
        "* Define and call **functions**\n",
        "* Understand the role of **arguments** in functions\n",
        "* Understand what it means for a function to **return a value**, and also how we can store the returned value and use it later in our code\n",
        "* Use several different methods to **read in data from a file**\n",
        "* **Write data to a file**\n",
        "* Use the string methods **split** and **join**\n",
        "* Get interactive **input** from a user\n",
        "* Get familiar with **stopwords** and stopword lists from NLTK\n",
        "* Build frequency distributions without punctuation or stopwords\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wwr2k4UCsql"
      },
      "source": [
        "\n",
        "# 0. Preliminaries\n",
        "\n",
        "To get started, import the NLTK and download the book data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db7CwgVwC28x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53b63faf-28ba-41f7-f64f-61e2dedf284d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('book')  ## the 'book' download includes some texts from Project Gutenberg \n",
        "from nltk.book import *\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKuduFps5K-D"
      },
      "source": [
        "# 1. Functions\n",
        "\n",
        "Functions are essential build blocks of programs in nearly every programming language. A function is a reusable block of code - once we have **defined** a function, we can then **call** it as many times as we need to.\n",
        "\n",
        "Here's a first example - a function to convert a temperature in fahrenheit to its equivalent in celsius.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAk9VrWV02_b"
      },
      "source": [
        "### defining a function to convert fahrenheit to celsius\n",
        "def f2c(temp):\n",
        "    cels = ((temp - 32) * 5) / 9  ## this is the formula for conversion\n",
        "    print(temp, \"fahrenheit is \", cels, \"celsius\")  ## print the results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqffOhZt1iSC"
      },
      "source": [
        "Every function definition has three essential parts:\n",
        "\n",
        "1. The **header** - this line starts with the keyword `def` and continues with the name of the function (`f2c`), then parentheses, and the line ends with a colon.\n",
        "1. Any **parameters** for the function go between the parentheses on the header line. (Our `f2c` function has one parameter: `temp`.)\n",
        "1. The **body** of the function is the code that comes under the header line, and it needs to be indented. This is the code that will run when we call the function.\n",
        "\n",
        "To call the function, we type the name of the function, then parantheses, and put any **arguments** to the function between the parentheses. Let's do a couple of conversions from fahrenheit to celsius.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4lDcAkv2Qu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2cdba499-2fdf-4538-af9c-e3c706f9d2db"
      },
      "source": [
        "### one function call\n",
        "f2c(212)\n",
        "\n",
        "### second function call\n",
        "f2c(32)\n",
        "\n",
        "### third function call\n",
        "f2c(86)\n",
        "\n",
        "### you get the idea"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "212 fahrenheit is  100.0 celsius\n",
            "32 fahrenheit is  0.0 celsius\n",
            "86 fahrenheit is  30.0 celsius\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbeC4Xnh3MPg"
      },
      "source": [
        "When defining functions, it's important to use meaningful names for the functions, and also for the parameters. Notice that when we call a function, the argument (for example, `212`) gets passed up to the function. The argument value then gets stored to a variable with the parameter name (`temp`). The parameter value is available inside of the function, but only inside of the function. Each time we call the function, `temp` takes on whatever value we pass it when calling the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsQkAM4Y2coZ"
      },
      "source": [
        "**READINGS:**\n",
        "Please read the following sections for a thorough introduction to functions:\n",
        "* Think Python, Chapter 3: http://greenteapress.com/thinkpython2/html/thinkpython2004.html\n",
        "* Python Crash Course, Chapter 8, pages 133-140\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suH8x3Wo0xH6"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Write a simple function that takes an integer as its argument, prints the integer, and then prints the integer divided by 6.\n",
        "\n",
        "> Test your function with a number of different inputs.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> def six(int):\n",
        "  input = int\n",
        "  print(\"Original integer:\", input), print(\"Divided by six equals:\", input/6)\n",
        "input = 12\n",
        "six(12)\n",
        "input = -25\n",
        "six(input)\n",
        "\n",
        "-----------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk4rK9Bm7Obp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "daa53192-90a2-44a2-d4dc-e139f29af71c"
      },
      "source": [
        "def six(int):\n",
        "  input = int\n",
        "  print(\"Original integer:\", input), print(\"Divided by six equals:\", input/6)\n",
        "input = 12\n",
        "six(12)\n",
        "input = -25\n",
        "six(input)\n",
        "#couldn't think of another name for this function "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original integer: 12\n",
            "Divided by six equals: 2.0\n",
            "Original integer: -25\n",
            "Divided by six equals: -4.166666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HSsZtQLzOrO"
      },
      "source": [
        "### Arguments\n",
        "\n",
        "A function can take multiple arguments (and many do). For example, we might be interested in comparing two lists and finding the items that are on both lists. We could use such a function to find words that are used in two different novels by Jane Austen, for example.\n",
        "\n",
        "* Our function takes two arguments, each one a list of words\n",
        "* We convert each list to a **set** - each set represents the vocabulary (the set of word types) used in the book\n",
        "* We then use the operator `&` which finds the intersection of the two sets - the intersection means all of the items that are in *both* sets.\n",
        "* Now we'll turn the set of shared items back into a list, so we can do list-like things with it.\n",
        "\n",
        "Finding the intersection (or difference) between two sets is much, much faster than doing the same thing using lists. For more on sets and set operations: https://snakify.org/en/lessons/sets/\n",
        "\n",
        "Notice that this function takes lists as arguments, where our previous functions took integers as arguments. Functions in Python can take arguments of pretty much any data type, and the arguments can also be of different data types!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BO7_wN-INQf"
      },
      "source": [
        "### define function named commonItems\n",
        "def commonItems(list1, list2):\n",
        "    set1 = set(list1)\n",
        "    set2 = set(list2)\n",
        "    common = list(set1 & set2)  ### get set intersection and convert to a list\n",
        "    print(\"Your two lists have\", len(common), \"items in common.\")\n",
        "    print(\"Here are 20 of them:\")\n",
        "    print(common[:20])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaBymzrKIXgW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "3f10eabf-4eaa-4232-faf5-4ecb74353e08"
      },
      "source": [
        "### we'll create word lists from two Jane Austen novels\n",
        "from nltk.corpus import gutenberg\n",
        "emmaWords = gutenberg.words('austen-emma.txt')\n",
        "senseWords = gutenberg.words('austen-sense.txt')\n",
        "\n",
        "### and call our function with these two word lists as arguments\n",
        "commonItems(emmaWords, senseWords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your two lists have 4579 items in common.\n",
            "Here are 20 of them:\n",
            "['nominal', 'conversed', 'touch', 'formerly', 'impossible', 'timed', 'somewhat', 'anything', 'admire', 'accidental', 'wept', 'beat', 'considering', 'begin', 'grateful', 'lengths', 'feet', 'regretted', 'disposed', 'roused']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toY2OWoaGXMu"
      },
      "source": [
        "This function is quite fast and efficient! An alternate version would be to use a list comprehension, but this version is much, much slower. The reason it's so slow is the double iteration - for each item in list1 (there are 192K words in list1), we need to check whether it's in list2. Checking whether an item is in a list means inspecting the entire list - list2 has 141K words. This means we're doing 192K * 141K list operations!\n",
        "\n",
        "**TIP:** To see how long a code block has been running, as well as the list time it was executed, and whether or not it's been executed since it was last changed, hover over the arrow to the right of the first line of the code block.\n",
        "\n",
        "Try running the code below to see how much slower it is than the `set` version. If you're interested, play around with the length of the lists - notice how much slower things get as you increase the length. (If you get tired of waiting, click on the arrow/square to stop execution.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j4EQbAGLRfk"
      },
      "source": [
        "### here's the super pokey version of the same function\n",
        "def commonItemsSlow(list1, list2):\n",
        "    common = [w for w in list1 if w in list2] ## for each word (w) in list1, we check whether it's in list2\n",
        "    \n",
        "    ## the rest of the function is the same\n",
        "    print(\"Your two lists have\", len(common), \"items in common.\")\n",
        "    print(\"Here are 20 of them:\")\n",
        "    print(common[:20])\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQk38_jjLnIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "103e0995-92e0-45e6-e19c-d25a210b3414"
      },
      "source": [
        "### now we'll call the function, but on shorter versions of the texts (10K words each)\n",
        "emmaWordsShort = emmaWords[:10000]\n",
        "senseWordsShort = senseWords[:10000]\n",
        "\n",
        "commonItemsSlow(emmaWordsShort, senseWordsShort)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1424f8c977c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msenseWordsShort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msenseWords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcommonItemsSlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memmaWordsShort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msenseWordsShort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-5a234905510e>\u001b[0m in \u001b[0;36mcommonItemsSlow\u001b[0;34m(list1, list2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### here's the super pokey version of the same function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcommonItemsSlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcommon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m## for each word (w) in list1, we check whether it's in list2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m## the rest of the function is the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-5a234905510e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### here's the super pokey version of the same function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcommonItemsSlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcommon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m## for each word (w) in list1, we check whether it's in list2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m## the rest of the function is the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/collections.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;34m\"\"\"Return true if this list contains ``value``.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/collections.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;34m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/collections.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;34m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0melt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    298\u001b[0m                 \u001b[0;34m'block reader %s() should return list or tuple.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/plaintext.py\u001b[0m in \u001b[0;36m_read_word_block\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Read 20 lines at a time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m             \u001b[0mstartpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbytebuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m             \u001b[0mnew_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5FICifQOypu"
      },
      "source": [
        "Now let's compare our list comprehension version of the function to the set version of the function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYKZa8BGMs4n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5c400254-2196-4f65-c411-e3f7a1ed64f1"
      },
      "source": [
        "commonItems(emmaWordsShort, senseWordsShort)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your two lists have 837 items in common.\n",
            "Here are 20 of them:\n",
            "['determined', 'impossible', 'up', 'Upon', 'miss', 'quiet', 'begin', 'horses', 'society', 'disposed', 'seen', 'pleasant', 'miles', 'alone', 'something', 'beloved', 'take', 'worthy', 'want', 'what']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rPjPBIXO4EP"
      },
      "source": [
        "The set version runs much faster, but it also finds a different, much smaller set of words.\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> Why does the list comprehension version of the function (`commonItemsSlow`) find so many more shared words than the set version of the function (`commonItems`)?  \n",
        "\n",
        "HINT: There's something fundamentally different in what they're doing.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> The list comprehension version returns more shared words because it is iterating through each list separately then finding commanlities while the fast version finds the intersections between the two lists.\n",
        "-------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JeJZ-_1E_ZS"
      },
      "source": [
        "### Functions that return values (fruitful functions)\n",
        "\n",
        "As discussed in the readings, some functions return values (fruitful functions), and some don't (void functions). Currently, our `commonItems` function is a void function - it creates a list of items shared between the two lists, and it prints some of those items, but that list is only accessible within the function. Once the code inside the function has finished running, we don't have access to the list anymore. \n",
        "\n",
        "We can easily convert `commonItems` into a fruitful function by adding a `return` statement. The `return` statement does two things:\n",
        "\n",
        "1. It tells the function what value (or values) to return.\n",
        "1. It triggers python to exit the function and return to the code from where the function was called.\n",
        "\n",
        "Here's the new version, named `commonItemsReturn`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3C9T0nqQg4P"
      },
      "source": [
        "### new version of commonItems, now returning the list of common items\n",
        "def commonItemsReturn(list1, list2):\n",
        "    set1 = set(list1)\n",
        "    set2 = set(list2)\n",
        "    common = list(set1 & set2)\n",
        "\n",
        "    print(\"Your two lists have\", len(common), \"items in common.\")\n",
        "    return common    ### exit the function and return the list common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjziayhHQ1zH"
      },
      "source": [
        "Any time we call this function, it will return a list - this essentially means that it'll make the list available to the program. In order to use the list, though, we need to tell python where to store the returned list. To do this, we choose a variable name and assign the function's output (the returned value) to that variable. We do this using the assignment operator (`=`). Like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Xs4hy-RVMF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "108867ab-ac96-4919-c8e4-6387c2b5bfe4"
      },
      "source": [
        "austenCommon = commonItemsReturn(emmaWords, senseWords)\n",
        "\n",
        "print(\"To verify that we got back the same list:\")\n",
        "print(\"Our returned list has\", len(austenCommon), \"words.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your two lists have 4579 items in common.\n",
            "To verify that we got back the same list:\n",
            "Our returned list has 4579 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g35oiOARjdS"
      },
      "source": [
        "Now that we have stored the list to a variable, we can do things with it.\n",
        "\n",
        "### **REVIEW QUESTION:**\n",
        "\n",
        "> Using a list comprehension, create and print a list of all the long words that appear in both *Emma* (`emmaWords`) and *Sense and Sensibility* (`senseWords`). You can decide what counts as long. Show your code.\n",
        "\n",
        "### **ANSWER:**\n",
        "> code shown below:\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVJxwvKHsVFn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "48e24ee4-a02c-4f17-953b-889cbf4222cf"
      },
      "source": [
        "def commonItemsLong(list1, list2):\n",
        "  set1 = set(list1)\n",
        "  set2 = set(list2)\n",
        "  common = list(set1 & set2)\n",
        "\n",
        "  print(\"Your two lists have\", len(common), \"items in common.\")\n",
        "  return common \n",
        "\n",
        "longsense = [word for word in senseWords if len(word) > 12]\n",
        "longemma = [word for word in emmaWords if len(word) > 12]\n",
        "\n",
        "longausten = commonItemsLong(longsense, longemma)\n",
        "print(longausten)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your two lists have 83 items in common.\n",
            "['representations', 'circumspection', 'probabilities', 'neighbourhood', 'encouragement', 'incomprehensible', 'congratulating', 'discrimination', 'insignificance', 'companionableness', 'impertinently', 'establishment', 'inconveniences', 'circumstances', 'unwillingness', 'insensibility', 'acknowledgments', 'respectability', 'disapprobation', 'unintelligible', 'indispensable', 'inconvenience', 'recommendation', 'instantaneous', 'consternation', 'conversations', 'communicative', 'intentionally', 'uncomfortable', 'significantly', 'communicating', 'misunderstood', 'inconsiderable', 'comprehension', 'acknowledging', 'indisposition', 'considerations', 'entertainment', 'comparatively', 'mortification', 'commiseration', 'prepossessing', 'disinclination', 'understanding', 'encouragements', 'opportunities', 'gentlemanlike', 'correspondence', 'impossibility', 'affectionately', 'inconsiderately', 'particularity', 'disappointments', 'disinterested', 'accommodation', 'determination', 'gratification', 'communication', 'unaccountable', 'disagreements', 'disappointment', 'compassionate', 'dissatisfaction', 'indefatigable', 'distinguishing', 'embarrassment', 'prepossession', 'congratulated', 'involuntarily', 'acknowledgment', 'reconciliation', 'extraordinary', 'accomplishment', 'representation', 'consideration', 'disappointing', 'consciousness', 'congratulations', 'distinguished', 'unfortunately', 'inconsistency', 'disinterestedness', 'accommodations']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfZSKA0DSgH4"
      },
      "source": [
        "It's important to practice writing functions. We often start by writing functions to do simple things, so that we can get to know the anatomy of functions and how the pieces fit together. \n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> Choose **THREE** of the tasks below and write functions to perform each one. TIP: Don't overthink these - they are simple tasks and should be doable with just a line or two of code in the function body.\n",
        "\n",
        "1. A function that takes one argument - an integer - and returns that number plus 7.\n",
        "1. A function that takes one argument - a string - and returns the upper-case version of that string.\n",
        "1. A function that takes one argument - a string - and returns the length of that string.\n",
        "1. A function that takes one argument - a string - and prints the length of that string.\n",
        "1. A function that takes one argument - a list - and returns a new list with just the last 5 items from the original list.\n",
        "1. A function that takes one argument - a list - and prints the length of the list.\n",
        "\n",
        "> For each function that you write, call the function. If the function returns a value, store the returned value to a variable and then print it out.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> code shown below:\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-_sTzo08ur0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a64b6c44-9355-4e51-e6a9-072a01a8732a"
      },
      "source": [
        "#1\n",
        "def plus_seven(int):\n",
        "  return(int + 7)\n",
        "\n",
        "#plus_seven(10)\n",
        "seven_fun = plus_seven\n",
        "seven_fun(2)\n",
        "\n",
        "#2\n",
        "def upper(str):\n",
        "  return(str.upper())\n",
        "\n",
        "uppercase = upper\n",
        "uppercase('hello')\n",
        "\n",
        "#5\n",
        "def lastfive(list):\n",
        "  print(\"The last five items of your list are:\", list[-5:])  \n",
        "fruit = ['apple', 'banana', 'grape', 'mango', 'dragonfruit', 'blueberries']\n",
        "lastfive(fruit)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The last five items of your list are: ['banana', 'grape', 'mango', 'dragonfruit', 'blueberries']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ikDji9pYR-"
      },
      "source": [
        "# 2. Input and output\n",
        "\n",
        "Now we're going to switch gears for a bit and talk about **input** and **output** operations. Mostly, we'll focus on working with files - reading input from files and writing output to files. \n",
        "\n",
        "**NOTE:** the first thing you need to do is upload a plaintext file or two to work with. You do not need to create a new folder, instead you can simply upload the \n",
        "file(s) on Colab. (Click the folder icon to the left of this window, then right-click in the white space and choose 'Upload'.)\n",
        "\n",
        "**IMPORTANT:** I'm using `alice.txt` (Alice in Wonderland) - in all of the code below, you should use the filename for *your file* wherever I have used `alice.txt`.\n",
        "\n",
        "## **Reading from files**\n",
        "\n",
        "Once you have uploaded the file, it's easy to access it, using the function `open()`. The file name needs to be given as a string (in quotation marks). The `r` before the name of the file tells Python to treat the string as a **raw string** and ignore any special characters. For example, a backslash `\\` in a string can have a special meaning for Python - but sometimes the path to a file name might also include `\\`. The `r` prefix tells Python to treat the backslash as a backslash and not as a special character.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r84RDN1mYN0K"
      },
      "source": [
        "### open the file and store it as one big string\n",
        "### (the file is assigned to the variable alice)\n",
        "with open(r'alice.txt', 'r') as infile:\n",
        "    alice = infile.read()\n",
        "\n",
        "### print the length of the file (number of characters)\n",
        "### and the first 100 characters\n",
        "print(len(alice))\n",
        "print(alice[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfgLmINjY1Mq"
      },
      "source": [
        "More typically, when working with text, we may want to access our file one line at a time, instead of as one long string. One way to do this is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh_fNcQnY9X5"
      },
      "source": [
        "# “open” takes as its argument a file name, which may include directory information.\n",
        "# Do not forget to start with the “open” command!\n",
        "# “open” returns a file object. This, then, can be used to access the file contents.\n",
        "# We can iterate through the lines of a file as if it were a list.\n",
        "# Note that \"line\" is just a variable name. I could have named it anything else.\n",
        "# “line” is a variable that will be filled by each line of the file in turn.\n",
        "# Don't forget to change the file name shown below to your own text file to see the results!\n",
        "\n",
        "with open(r\"alice.txt\", 'r') as file_object:\n",
        "  for line in file_object:\n",
        "    print( line )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T_yzPVLZOJ3"
      },
      "source": [
        "Notice that `open()` has two arguments: the filename and the mode. We have used the string `'r'` to indicate that we want to open the file for reading. `'w'` opens a file for writing, and `'a'` opens the file to append data to the end of the file. More on these later. If no mode is specified, `open()` will default to reading. \n",
        "\n",
        "\n",
        "We\thave\tfour ways\tof\treading\tfrom\ta\tfile:\t\n",
        "\n",
        "1. line-by-line,\t\n",
        "1. the\twhole\tfile\tas\ta\tsingle\tstring,\t\n",
        "1. just\tone\tline\tat\ta\ttime,\tor\t\n",
        "1. reading\tthe\tfile\tin\tas\ta\tlist of\tstrings,\tone\tfor\teach\tline.\t\n",
        "\n",
        "Each\tof\tthese\tis\ta\tseparate\tsmall\tprogram. Try\tthese\tfour different\tmethods\tand\tlook\tat\tthe\tdifferences\tin\thow the\tfile\tgets\tread\tinto\t your\tPython\tsession. Notice that nothing gets printed until we use the `print()` function.\n",
        "\n",
        "\n",
        "**IMPORTANT\tNOTE:\tAs\tlong\tas\tyou\tuse\tthe\t`with`\tsyntax,\tPython\tautomatically\tcloses\tthe\tfile\tonce\tit\tleaves\tthe\tindented\tcode\tblock.\tOtherwise,\teach\ttime\tyou\topen\ta\tfile\tand\tread\tits\tcontents,\tPython\tends\tthe\treading\tprocess\twherever\tit\tstops\t(usually\tat\tthe\tend\tof\tthe\tfile). You\tcan\tthink\tof\tthis\tas\tbeing\tlike\thaving\tyour\tcursor\tat\tthe\tend\tof\ta\ttext\tdocument or\tweb\tpage.\tIf\tyou now\task\tPython\tto\tread\tthe\tfile\tagain,\tit\twill\ttell\tyou\tthat\tthere's\tnothing\tleft\tto\tread. So,\tif\tyou\twere\ttrying\tto\tread\ta\tfile\tand\tgot\tthis\tunexpected\tresult,\tyou\twould\tneed\tto\tclose\tand\tre-open\tthe\tfile\tin\torder\tto\tread\tit\tagain.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JmrTBaxaoWl"
      },
      "source": [
        "# Method 1: read a file line by line\n",
        "\n",
        "with open(\"alice.txt\") as f:\n",
        "  for line in f:\n",
        "    print( line )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aVMfUrCa41d"
      },
      "source": [
        "# Method 2: read the whole file in one go, as a string\n",
        "\n",
        "with open(\"alice.txt\") as f:\n",
        "  myfilecontents = f.read()\n",
        "\n",
        "print(len(myfilecontents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWkGmDala_yO"
      },
      "source": [
        "# Method 3: read the next line of the file\n",
        "\n",
        "with open(\"alice.txt\") as f:\n",
        "  line1 = f.readline() # reads the first line\n",
        "   # at this point the “file reading pointer”\n",
        "   # points to the second line\n",
        "  line2 = f.readline() # reads the second line\n",
        "  line3 = f.readline() # reads the third line\n",
        "\n",
        "print(line1)\n",
        "print(line2)\n",
        "print(line3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VhGoLrmbEQR"
      },
      "source": [
        "# Method 4: read in the entire file as a list of strings, one for each line\n",
        "\n",
        "with open(\"alice.txt\") as f:\n",
        "  mylines = f.readlines()\n",
        "  print(mylines[:3]) ## prints the first three lines of the file\n",
        "  print(len(mylines)) ### tells you how many lines are in the file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAvyjynAbnOS"
      },
      "source": [
        "The `readlines()` function can be very useful if you want to iterate through your file one line at a time. This method is often used in conjunction with `split()`, which uses spaces to split a string into a list of words. In the code block below, we're looking through the first 5 lines of your file, splitting each line, and then printing the list that we get after splitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue4NLo5TcULe"
      },
      "source": [
        "for line in mylines[:5]:\n",
        "    line = line.split()\n",
        "    print(line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cC9quaTnO1d"
      },
      "source": [
        "**GOOD TO KNOW:** The method `join()` is (sort of) the opposite of `split()`. Where `split()` turns a string into a list of words, `join()` turns a list of words back into a string by concatenating the words together, in the same order that they appear in the list. The syntax of `join()` is a little quirky - when you use this method, you start with the string that you want to use as the separator/delimiter. The list that you want to join appears as an argument, between the parentheses. See below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLeqyzdun2H6"
      },
      "source": [
        "testList = ['here', 'is', 'a', 'list', 'of', 'words']\n",
        "\n",
        "joined1 = ' '.join(testList)  ### here we use a space as delimiter\n",
        "joined2 = '***'.join(testList)  ### now we use *** as delimiter\n",
        "\n",
        "print(joined1)\n",
        "print(joined2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG06dX2pdCtR"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Write code to read in the lines of your file (using `readlines()`). How many lines are in your file? (BONUS: how do you think Python knows where to split the file into lines?)\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> There are 15973 lines in my text file, code shown below. I think Python uses the '\\n' newline marker to split the line where it's shown.\n",
        "\n",
        "-----\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xvaumCeD3Sz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "60cf7385-8aff-4927-cae0-20aa8ea70d36"
      },
      "source": [
        "with open('dracula.txt') as f:\n",
        "  mylines = f.readlines()\n",
        "  print(mylines[:4])\n",
        "  print(len(mylines))\n",
        "\n",
        "for line in mylines[:4]:\n",
        "  line = line.split()\n",
        "  print(line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeffThe Project Gutenberg EBook of Dracula, by Bram Stoker\\n', '\\n', 'This eBook is for the use of anyone anywhere at no cost and with\\n', 'almost no restrictions whatsoever.  You may copy it, give it away or\\n']\n",
            "15973\n",
            "['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Dracula,', 'by', 'Bram', 'Stoker']\n",
            "[]\n",
            "['This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with']\n",
            "['almost', 'no', 'restrictions', 'whatsoever.', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYd4BG8SDI1n"
      },
      "source": [
        "with open('dracula.txt') as f:\n",
        "  for line in f:\n",
        "    print(line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aby_9-EFDVVl"
      },
      "source": [
        "with open('dracula.txt') as f:\n",
        "  filecontents = f.read()\n",
        "  print(len(filecontents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUvrs3zhDjOP"
      },
      "source": [
        "with open('dracula.txt') as f:\n",
        "  line1 = f.readline()\n",
        "  line2 = f.readline()\n",
        "  line3 = f.readline()\n",
        "print(line1)\n",
        "print(line2)\n",
        "print(line3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T69IxYzOd4Ey"
      },
      "source": [
        "## **Writing to files**\n",
        "\n",
        "Writing\tto\tfiles\tin\tPython\talso\tuses\tthe\t`open()`\tcommand,\tbut\tnow\twe specify\tthat\twe\twant\tto\twrite\tinstead\tof\treading\tthe\tfile. The\tcode\tbelow\twill\tcreate\ta\tnew\tfile\t(called `myoutfile.txt`)\tin\tthe\tcurrent\tworking\tdirectory. Again, we make a file object with `open`. Only this time we give two arguments.\n",
        "The first is the name of the file. The second one is `w` for “write”.\n",
        "So we have to decide at the time when we open a file whether we want to read it or write to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGyrIqvwe4Hh"
      },
      "source": [
        "with open('myoutfile.txt', 'w') as f:\n",
        "    f.write(\"We made a file!!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdxrIcQRj5Ho"
      },
      "source": [
        "In this example, we used the `write` method to write some text to the file we created. Check it out! Double click on the file (to the left). \n",
        "\n",
        "`write()` will write to the file *exactly* what you ask it to. It doesn't add new line characters, and it won't convert other data types into strings. It will *only* write strings, and it also doesn't accept multiple arguments.\n",
        "\n",
        "To get around these limitations, another way of writing data to a file is using `print()`. Now that the file is open, we use the `print` command to write to the file, with an extra argument that specifies what file we want to send the output to (`file=f`, where `f` is the variable name we specified in the `with` statement):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap6t3FhwkaMq"
      },
      "source": [
        "# We use the \"print\" command to write to a file, but with the additional\n",
        "# parameter file=f.\n",
        "# Note that f is the variable in which we put the file object.\n",
        "# If I had named the file object \"bob\", it would have been \"print(..., file = bob)\"\n",
        "\n",
        "with open(\"myoutfile.txt\", \"w\") as f:\n",
        "  print(\"Hello\", file=f)\n",
        "  print(\"Writing another line to the file.\", file = f)\n",
        "  print(\"Here’s a number:\", 5, file= f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpZGTP-pkfP7"
      },
      "source": [
        "**IMPORTANT:** Open up `myoutfile.txt` again (by double-clicking on the file name) and notice that the contents have changed, and the first line of text we wrote to the file (\"We made a file!!!\") isn't there anymore. The `\"w\"` mode of `open()` opens a brand new file with the name we have provided. If another file of the same name already exists, it will be deleted, created a new, empty file with the same name.\n",
        "\n",
        "If we want to add data to an existing file, we instead use the append mode `\"a\"`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duFtlimik7o1"
      },
      "source": [
        "with open(\"myoutfile.txt\", \"a\") as f:\n",
        "    print(\"\", file=f)\n",
        "    print(\"Now we've added new text without erasing the previous text.\", file=f)\n",
        "    print(\"Yay!\", file=f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGlE661iljC5"
      },
      "source": [
        "Now let's look at how we can combine these commands to transform a data file and create a new file with the transformed data. In this case, imagine that we need a version of our text file with just one word per line. \n",
        "\n",
        "1. First, we open the file and read it in as a string.\n",
        "1. Next, we split that string into a list of words.\n",
        "1. Third, we'll open a new output file.\n",
        "1. Finally, we print one word at a time into the output file.\n",
        "\n",
        "Notice that we can also use `print()` to write to a file outside of a `with` code block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3Dj81Ogl8Ul"
      },
      "source": [
        "### First, open the file\n",
        "with open(r'alice.txt') as infile:\n",
        "    myfile = infile.read()  ### read in as a string\n",
        "\n",
        "### split the string into a list of words\n",
        "myfileWords = myfile.split()\n",
        "\n",
        "### check - how many words? Does this sound about right?\n",
        "### print the first 10 or so words\n",
        "print(len(myfileWords))\n",
        "print(myfileWords[:10])\n",
        "\n",
        "### open a new output file and assign\n",
        "### file object to the variable outfile\n",
        "outfile = open('alice.1perline.txt', 'w')\n",
        "\n",
        "### print the new file\n",
        "for word in myfileWords:\n",
        "    print(word, file=outfile)\n",
        "\n",
        "### remember to close the output file!\n",
        "outfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYUSaAz7m47X"
      },
      "source": [
        "If you're not using the with open() syntax demonstrated here, you have to also close your file (e.g. with f.close())! Data is written to files in larger chunks. This means that writing to your file might not happen until you have issued a large number of print commands. When you explicitly close the file, though, the operating system makes sure that the remaining data is written. This can cause nasty errors if you write a file, don’t close it, and then try to read the contents – they simply might not be there yet.\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> Transform your input text in some way, create a new file, and store the transformed text in the new file. For example, you could create a new version of the text that is only 750 words long. For more of a challenge, create a new, all lower-case version of the text, and write it back as a normal text file. As usual, show your code.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> code shown below:\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgwskrCjFrtP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bea4113-66bd-4fde-d933-994d37080a3b"
      },
      "source": [
        "with open(r'dracula.txt') as f:\n",
        "  myfile = f.read()\n",
        "  myfilewords = myfile.lower()\n",
        "\n",
        "print(myfilewords[:50])\n",
        "newfile = open('draculaLower.txt', 'w')\n",
        "for word in myfilewords:\n",
        "  print(word, file=newfile)\n",
        "\n",
        "newfile.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿the project gutenberg ebook of dracula, by bram s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IYZ6lzAVL_m"
      },
      "source": [
        "## **Getting user input**\n",
        "\n",
        "The `input()` function is a simple way to get input interactively from a user. `input()` can be used with no argument, or you can supply a prompt that will be displayed to the user to ask for input. Whatever the user types in (before pressing enter) will be stored into the variable you have provided.  For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD_TY7FvpcXT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fdcf126c-9b50-49bf-81cb-0fe81c39a983"
      },
      "source": [
        "name = input(\"What is your name, friend? \")\n",
        "print()\n",
        "print(\"Well, howdy doo,\", name+\"!!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is your name, friend? Cristina\n",
            "\n",
            "Well, howdy doo, Cristina!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxA7A_Ips2b"
      },
      "source": [
        "## **READINGS:** \n",
        "For more on input and output operations and working with files, read the following:\n",
        "\n",
        "* Python Crash Course, chapter 7, pages 117-128 (about `input()` and ways to use it)\n",
        "* Python Crash Course, chapter 10, pages 189-199 (on files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABUMi_l0pift"
      },
      "source": [
        "# 3. Punctuation and stopwords\n",
        "\n",
        "In this short unit, we'll look at how to create more informative frequency distributions by removing punctuation and stopwords. Stopwords are high-frequency words that appear in nearly all texts. In fact, certain stopwords are likely to be the most-frequent words in just about any text. We can test this out by looking at the 10 most frequent words for `text1` - `text9` in the NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmKJ5p7MqgLw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "842fee92-0b86-4226-e0b1-e0a076428cfb"
      },
      "source": [
        "### first make a list of the texts\n",
        "texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9]\n",
        "\n",
        "### now define a function to create a frequency distribution and\n",
        "### print the 10 most-frequent words\n",
        "def highFreq(text):\n",
        "    fdist = FreqDist(text)\n",
        "    print(\"Highest frequency words are:\")\n",
        "    print([word for (word, count) in fdist.most_common(10)]) ## fdist.most_common() returns list of tuples\n",
        "                                                             ## (word, count) stores the first item of each tuple to word\n",
        "                                                             ## and the second item to count\n",
        "\n",
        "### now we'll iterate through the list of texts,\n",
        "### calling our function for each one in turn\n",
        "for t in texts:\n",
        "    print()\n",
        "    print(\"Now analyzing\", t)\n",
        "    highFreq(t)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Now analyzing <Text: Moby Dick by Herman Melville 1851>\n",
            "Highest frequency words are:\n",
            "[',', 'the', '.', 'of', 'and', 'a', 'to', ';', 'in', 'that']\n",
            "\n",
            "Now analyzing <Text: Sense and Sensibility by Jane Austen 1811>\n",
            "Highest frequency words are:\n",
            "[',', 'to', '.', 'the', 'of', 'and', 'her', 'a', 'I', 'in']\n",
            "\n",
            "Now analyzing <Text: The Book of Genesis>\n",
            "Highest frequency words are:\n",
            "[',', 'and', 'the', 'of', '.', 'And', 'his', 'he', 'to', ';']\n",
            "\n",
            "Now analyzing <Text: Inaugural Address Corpus>\n",
            "Highest frequency words are:\n",
            "['the', 'of', ',', 'and', '.', 'to', 'in', 'a', 'our', 'that']\n",
            "\n",
            "Now analyzing <Text: Chat Corpus>\n",
            "Highest frequency words are:\n",
            "['.', 'JOIN', 'PART', '?', 'lol', 'to', 'i', 'the', 'you', ',']\n",
            "\n",
            "Now analyzing <Text: Monty Python and the Holy Grail>\n",
            "Highest frequency words are:\n",
            "[':', '.', '!', ',', \"'\", '[', ']', 'the', 'I', 'ARTHUR']\n",
            "\n",
            "Now analyzing <Text: Wall Street Journal>\n",
            "Highest frequency words are:\n",
            "[',', 'the', '.', 'of', 'to', 'a', 'in', 'and', '*-1', '0']\n",
            "\n",
            "Now analyzing <Text: Personals Corpus>\n",
            "Highest frequency words are:\n",
            "[',', '.', '/', 'for', 'and', 'to', 'lady', '-', 'seeks', 'a']\n",
            "\n",
            "Now analyzing <Text: The Man Who Was Thursday by G . K . Chesterton 1908>\n",
            "Highest frequency words are:\n",
            "[',', 'the', '.', 'a', 'of', 'and', '\"', 'to', 'in', 'I']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu-xr6SWrqGk"
      },
      "source": [
        "There are differences in the most-frequent words across the texts, sure, but they're not very interesting differences! Mostly, it looks like we took the same list of punctuation marks and stopwords and shuffled it from text to text. \n",
        "\n",
        "Before proceeding, read (just a bit) more about stopwords in the NLTK book, Chapter 2, section 4.1: http://www.nltk.org/book/ch02.html\n",
        "\n",
        "The NLTK includes lists of stopwords in various languages that we'll use in this next section.\n",
        "\n",
        "### **QUESTION:**\n",
        "\n",
        "> Look at the list of English stopwords from the NLTK. How many are there? Do any of them surprise you? (Show your code.)\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> There are 179 stopwords in the list of English words, no surprises here.\n",
        "\n",
        "-----\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tc0BQcZH9Lv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73ef9766-2117-4b20-86e6-c49d7e10fef4"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "len(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy1fPpbrs_py"
      },
      "source": [
        "The way to solve this problem is to build a new frequency distribution from a new version of the text. We will do this in several steps:\n",
        "\n",
        "1. Create a new version of the text with stopwords removed.\n",
        "1. Make a new frequency distribution from this version.\n",
        "1. Create yet another version of the text with both punctuation and stopwords removed.\n",
        "1. Make another new frequency distribution from this version.\n",
        "\n",
        "First, let's look at how we can remove stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRSmhj7Duqii",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8a582534-8b23-4de4-9189-f0af5d10d1bd"
      },
      "source": [
        "### first we import the nltk stopwords list for English\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.fileids())   ### look how many languages!\n",
        "enStop = stopwords.words(\"english\")    ### notice we're using the words() corpus reader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adE_5aXuu5Dk"
      },
      "source": [
        "### next, use a list comprehension to remove stopwords from the text\n",
        "senseNoStops = [word for word in text2 if word not in enStop]   ### we keep only the words that aren't in enStop\n",
        "\n",
        "### this list comprehension is equivalent to the following code:\n",
        "# senseNoStops = []\n",
        "# for word in text2:\n",
        "#    if word not in enStop:\n",
        "#        senseNoStops.append(word)\n",
        "#    else: pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8wp35Y7vyD1"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> Compare the length of the original `text2` to the length of the version with all stopwords removed (`senseNoStops`). How many tokens of stopwords were removed? (Show your code.)\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> Orginal length of text2: 141576\n",
        "Length of text2 with stopwords removed: 80251\n",
        "61325 tokens were removed\n",
        "code shown below:\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOZROl1ZIgwR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e0e2ee83-8ed1-4163-8172-b47402b4438e"
      },
      "source": [
        "print(\"Orginal length of text2:\", len(text2))\n",
        "print(\"Length of text2 with stopwords removed:\", len(senseNoStops))\n",
        "edited = len(text2) - len(senseNoStops)\n",
        "print(edited, \"tokens were removed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Orginal length of text2: 141576\n",
            "Length of text2 with stopwords removed: 80251\n",
            "61325 tokens were removed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLZXGSGEwFA2"
      },
      "source": [
        "## Punctuation\n",
        "\n",
        "For all its usefulness, punctuation often gets in the way when we want to process words. Python has a special string which consists of a list of typical punctuation characters. We access this by importing the `string` module and then printing the string stored to `string.punctuation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4oHDbvswoZG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "472f7c4d-069f-41f9-fa5b-e9b341c82478"
      },
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ8eLN9PxCuu"
      },
      "source": [
        "We can remove most of the punctuation from our text with another list comprehension. This time instead of filtering out based on a list of stopwords, we will filter based on this string of characters. This only removes single-character punctuation, or sequences of punctuation in the same order that the characters appear in `string.punctuation`. For example, !\" would be removed, but not !?, because !? is not a substring of the (sequence) `string.punctuation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPtcYqefx_az",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8535be00-8db9-468f-8d65-8826f8d04718"
      },
      "source": [
        "### we'll start from senseNoStops\n",
        "### and use a list comprehension to remove punctuation\n",
        "\n",
        "senseNoStopsNoPunct = [word for word in senseNoStops if word not in string.punctuation]\n",
        "print(len(senseNoStopsNoPunct))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4z7OD5Cylm9"
      },
      "source": [
        "### **QUESTION:**\n",
        "\n",
        "> This removes much of the punctation, but not all of it. What could you try to remove more of the punctuation? Suggest one thing you could try to get rid of some more punctuation. You don't need to write code for it (unless you want to), and you don't need to try to remove *all* punctuation.\n",
        "\n",
        "### **ANSWER:**\n",
        "\n",
        "> Maybe create another list of punctuation with common clusters like the \"!?\" example mentioned above. Or, assuming punctuation follows the end of a sentence, create a function to remove sentence-final tokens.\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l50aUGNA3q2K"
      },
      "source": [
        "#### Side trip: more methods for removing punctuation\n",
        "\n",
        "Now, how can we remove punctuation at the end (or beginning) of a word? We can use Python's functions for stripping characters from strings – take a look at the functions `strip()`, `lstrip()`, and `rstrip()`.\n",
        "\n",
        "By default, `strip()` removes spaces at the beginning or end of a string - these are usually referred to as **leading spaces** or **trailing spaces**. `lstrip()` removes only leading spaces, and `rstrip()` removes only trailing spaces. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDFlgkYw4c9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eb367e57-3f76-4d86-a536-d06e42f16dcd"
      },
      "source": [
        "s1 = \"    hello!    \"\n",
        "print(s1.strip(), len(s1.strip()))\n",
        "print(s1.lstrip(), len(s1.lstrip()))\n",
        "print(s1.rstrip(), len(s1.rstrip()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello! 6\n",
            "hello!     10\n",
            "    hello! 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEWLQI0u5BEi"
      },
      "source": [
        "Optionally, we can specify a set of characters (as a string) that we want to be stripped from the leading and/or trailing edges of a string. Every occurrence of each character in the set will be removed, no matter what order they are in.\n",
        "\n",
        "Notice that we included space as a character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3fqzjjR5Zf-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "851cd26f-6c3f-430e-eebe-bf650e2d6e1d"
      },
      "source": [
        "s2 = \"!???rrrr  !hello??!!r!?\"\n",
        "print(s2.strip('!?r '), len(s2.strip('!?r ')))\n",
        "print(s2.lstrip('!?r '), len(s2.lstrip('!?r ')))\n",
        "print(s2.rstrip('!?r '), len(s2.rstrip('!?r ')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello 5\n",
            "hello??!!r!? 12\n",
            "!???rrrr  !hello 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU1XuOJ_6GZU"
      },
      "source": [
        "We can combine this with `string.punctuation` to strip any leading or trailing punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgeREsAY6MuU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e402c4be-35d6-493a-e025-63dc2c67f8c3"
      },
      "source": [
        "s3 = \"***()wowie!!!???\"\n",
        "print(s3.strip(string.punctuation), len(s3.strip(string.punctuation)))\n",
        "print(s3.lstrip(string.punctuation), len(s3.lstrip(string.punctuation)))\n",
        "print(s3.rstrip(string.punctuation), len(s3.rstrip(string.punctuation)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wowie 5\n",
            "wowie!!!??? 11\n",
            "***()wowie 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Fl4UTLyYz8"
      },
      "source": [
        "### **Back to our frequency distributions...**\n",
        "\n",
        "We can put these together into a function that will take a text (in the form of a list of words) as input, remove English stopwords, remove punctuation, and return the new list.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjaWdeABy-z8"
      },
      "source": [
        "### define the function\n",
        "def cleanUp(text):\n",
        "    textNoStops = [w for w in text if w not in enStop]\n",
        "    textNSNP = [w for w in textNoStops if w not in string.punctuation]\n",
        "    return textNSNP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO12hKjGzUHY"
      },
      "source": [
        "Now let's combine this function with our earlier function - the one that created and displayed frequency distributions - and see how different the set of most common words looks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbjHLqlIzc4L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "f8743827-9ce0-4eda-850b-519f7ac61b57"
      },
      "source": [
        "### again, iterate through the list of texts,\n",
        "### calling both functions for each one in turn\n",
        "for t in texts:\n",
        "    print()\n",
        "    print(\"Now analyzing\", t)\n",
        "    cleanedText = cleanUp(t) ### first we get the cleaned-up version of the text\n",
        "    highFreq(cleanedText)    ### and use that list to create and display a frequency distribution"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Now analyzing <Text: Moby Dick by Herman Melville 1851>\n",
            "Highest frequency words are:\n",
            "['I', '--', 'whale', 'one', 'But', 'like', 'The', 'upon', 'man', 'ship']\n",
            "\n",
            "Now analyzing <Text: Sense and Sensibility by Jane Austen 1811>\n",
            "Highest frequency words are:\n",
            "['I', '.\"', 'Elinor', 'could', 'Marianne', 'Mrs', 'would', '--', 'said', ',\"']\n",
            "\n",
            "Now analyzing <Text: The Book of Genesis>\n",
            "Highest frequency words are:\n",
            "['And', 'unto', 'I', 'said', 'thou', 'thy', 'thee', 'shall', 'God', 'father']\n",
            "\n",
            "Now analyzing <Text: Inaugural Address Corpus>\n",
            "Highest frequency words are:\n",
            "['I', 'The', 'people', 'We', 'us', 'must', 'upon', '--', 'It', 'Government']\n",
            "\n",
            "Now analyzing <Text: Chat Corpus>\n",
            "Highest frequency words are:\n",
            "['JOIN', 'PART', 'lol', 'I', 'hi', '...', '..', 'ACTION', 'hey', 'u']\n",
            "\n",
            "Now analyzing <Text: Monty Python and the Holy Grail>\n",
            "Highest frequency words are:\n",
            "['I', 'ARTHUR', '--', '...', 'Oh', '1', 'No', 'LAUNCELOT', 'GALAHAD', 'KNIGHT']\n",
            "\n",
            "Now analyzing <Text: Wall Street Journal>\n",
            "Highest frequency words are:\n",
            "['*-1', '0', \"'s\", '*T*-1', '*U*', 'The', '``', \"''\", 'said', 'million']\n",
            "\n",
            "Now analyzing <Text: Personals Corpus>\n",
            "Highest frequency words are:\n",
            "['lady', 'seeks', 'S', 'ship', 'relationship', 'fun', 'slim', 'build', 'smoker', '50']\n",
            "\n",
            "Now analyzing <Text: The Man Who Was Thursday by G . K . Chesterton 1908>\n",
            "Highest frequency words are:\n",
            "['I', 'Syme', ',\"', 'said', '.\"', 'The', 'man', 'He', 'like', '?\"']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnXtTIkg1B6E"
      },
      "source": [
        "Aha! Now we have something more useful. I suspect that it would even be possible to match each title with its 10 most common words (given a list of the titles).\n",
        "\n",
        "Notice how we used the output of one function (`cleanUp`) as the input to the next function (`highFreq`). This is a very common practice and strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vDb14N0beN9"
      },
      "source": [
        "# 4. Putting it all together - short version :)\n",
        "\n",
        "To wrap up this week's lab, please write a single function that does the following:\n",
        "\n",
        "1. Takes a text as input (in the form of a list of words)\n",
        "1. Returns a list of the 20 most-frequent non-stopwords in the text, together with their frequencies\n",
        "\n",
        "You will find all of the pieces you need in the code above - your task is to combine the right pieces, in the right order, and with the necessary changes in variable names etc. Please do not reuse the functions from above, but instead write your own function. Give it a meaningful name.\n",
        "\n",
        "Finally, apply this function to your external text (e.g. I would apply mine to `alice.txt`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVmq2NksKrtE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "39f1b66c-205b-4a30-8cfa-76e759756568"
      },
      "source": [
        "def limpiar(text):\n",
        "  english_stops = stopwords.words('english')\n",
        "  text_nstops = [word for word in text if word not in english_stops]\n",
        "  text_npunct = [word for word in text_nstops if word not in string.punctuation]\n",
        "  return text_npunct\n",
        "\n",
        "def mostfreq(text):\n",
        "  freq_dist = FreqDist(text)\n",
        "  print(\"Highest frequency words are:\")\n",
        "  print([word for (word, count) in freq_dist.most_common(10)])\n",
        "\n",
        "\n",
        "dracula = open('dracula.txt')\n",
        "cleanedup = limpiar(dracula)\n",
        "mostfreq(cleanedup)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Highest frequency words are:\n",
            "['\\n', '       *       *       *       *       *\\n', \"_Dr. Seward's Diary._\\n\", \"_Mina Harker's Journal._\\n\", \"_Jonathan Harker's Journal._\\n\", 'said:--\\n', \"DR. SEWARD'S DIARY\\n\", \"JONATHAN HARKER'S JOURNAL\\n\", '\"My dearest Lucy,--\\n', '\"MINA HARKER.\"\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7ej0sNaYhxX"
      },
      "source": [
        "# 5. REMINDER: Wrapping up and submitting\n",
        "\n",
        "Create a revision by going to **File > Save and Pin Revision**.\n",
        "\n",
        "View your revision history at **File > Revision History**.\n",
        "\n",
        "To submit: go to **Share** in the upper right corner, click **Get Shareable Link**, change the dropdown menu option to **Anyone with the link can edit**, and then **Copy Link**! This is what you'll submit on Canvas."
      ]
    }
  ]
}